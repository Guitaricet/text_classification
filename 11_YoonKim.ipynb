{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Yoon Kim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from time import time\n",
    "from random import random, choice\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.nn import init\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "import torchtext\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "from tqdm import tqdm as tqdm\n",
    "\n",
    "np.random.seed(42)\n",
    "CUDA = torch.cuda.is_available()\n",
    "\n",
    "CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "VALID_SIZE = 0.1\n",
    "\n",
    "NOISE_LEVELS = [0, 0.005, 0.01, 0.025, 0.05, 0.075, 0.1, 0.125, 0.15, 0.175, 0.2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы использовать CNN на слова, нужно фиксировать длину слова."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_WORD_LEN = 8  # chars in word (try 32?)\n",
    "MAX_TEXT_LEN = 256  # words in text\n",
    "\n",
    "# alphabet from the paper\n",
    "# https://papers.nips.cc/paper/5782-character-level-convolutional-networks-for-text-classification.pdf\n",
    "ALPHABET = ['<UNK>'] + ['\\n'] + [s for s in \"\"\" abcdefghijklmnopqrstuvwxyz0123456789-,;.!?:'’’/\\|_@#$%ˆ&* ̃‘+-=<>()[]{}\"\"\"]\n",
    "char2int = {s: i for s, i in zip(ALPHABET, range(len(ALPHABET)))}\n",
    "\n",
    "\n",
    "class HieracialIMDB(torchtext.datasets.imdb.IMDB):\n",
    "    \"\"\"\n",
    "    Zero vector used for padding\n",
    "    \"\"\"\n",
    "    noise_level = 0\n",
    "    alphabet = ALPHABET\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = super(HieracialIMDB, self).__getitem__(idx)\n",
    "        _text_tensor = self.preprocess(item.text)\n",
    "\n",
    "        label = int(item.label == 'pos')\n",
    "        return _text_tensor, label\n",
    "    \n",
    "    def preprocess(self, text, with_noise=True):\n",
    "        _text_tensor = torch.zeros([MAX_WORD_LEN * MAX_TEXT_LEN, len(self.alphabet)])\n",
    "\n",
    "        for i, token in enumerate(text):\n",
    "            if i >= MAX_TEXT_LEN:\n",
    "                break\n",
    "            if with_noise:\n",
    "                token = self.noise_generator(token)\n",
    "            for j, char in enumerate(token):\n",
    "                if j >= MAX_WORD_LEN:\n",
    "                    break\n",
    "                _text_tensor[i*MAX_WORD_LEN + j, char2int.get(char, char2int['<UNK>'])] = 1.\n",
    "        return _text_tensor\n",
    "    \n",
    "#     def _encode_word(self, word):\n",
    "#         word_tensor = torch.zeros([MAX_WORD_LEN, len(ALPHABET)])\n",
    "        \n",
    "#         for i, char in enumerate(word):\n",
    "#             word_tensor[i,char2int[char]] = 1.\n",
    "        \n",
    "#         return word_tensor\n",
    "\n",
    "    def noise_generator(self, string):\n",
    "        # removed '' symbol from alphabet for safety on word vectors\n",
    "        noised = \"\"\n",
    "        for c in string:\n",
    "            if random() > self.noise_level:\n",
    "                noised += c\n",
    "            if random() < self.noise_level:\n",
    "                noised += choice(self.alphabet)\n",
    "        return noised\n",
    "\n",
    "def get_train_valid_loader(dataset, valid_size, batch_size, random_seed=42, shuffle=True, num_workers=4):\n",
    "\n",
    "    len_dataset = len(dataset)\n",
    "    indices = list(range(len_dataset))\n",
    "\n",
    "    if shuffle:\n",
    "        np.random.seed(random_seed)\n",
    "        np.random.shuffle(indices)\n",
    "\n",
    "    val_actual_size = int(len_dataset * valid_size)\n",
    "\n",
    "    train_idx, valid_idx = indices[:-val_actual_size], indices[-val_actual_size:]\n",
    "\n",
    "    train_sampler = SubsetRandomSampler(train_idx)\n",
    "    valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        dataset, batch_size=batch_size, sampler=train_sampler, num_workers=4\n",
    "    )\n",
    "    valid_loader = torch.utils.data.DataLoader(\n",
    "        dataset, batch_size=batch_size, sampler=valid_sampler, num_workers=4\n",
    "    )\n",
    "\n",
    "    return train_loader, valid_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mokoron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymystem3 import Mystem\n",
    "\n",
    "\n",
    "MAX_WORD_LEN = 8\n",
    "MAX_TEXT_LEN = 32\n",
    "\n",
    "ALPHABET = ['<UNK>'] + ['\\n'] + [s for s in \"\"\" 0123456789-,;.!?:'’’/\\|_@#$%ˆ&* ̃‘+-=<>()[]{}\"\"\"]\n",
    "ALPHABET += [s for s in 'абвгдеёжзийклмнопрстуфхцчщъыьэюя']\n",
    "ALPHABET += [s for s in 'abcdefghijklmnopqrstuvwxyz']\n",
    "\n",
    "ALPHABET = [s for s in ALPHABET if s not in ('(', ')')]\n",
    "\n",
    "ALPHABET_LEN = len(ALPHABET)\n",
    "char2int = {s: i for s, i in zip(ALPHABET, range(ALPHABET_LEN))}\n",
    "\n",
    "\n",
    "class HieracialMokoron(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    Zero vector for padding.\n",
    "    \"\"\"\n",
    "    noise_level = 0\n",
    "\n",
    "    def __init__(self, filepath, text_field, maxwordlen=MAX_WORD_LEN, maxtextlen=MAX_TEXT_LEN):\n",
    "        self.alphabet = ALPHABET\n",
    "\n",
    "        self.mystem = Mystem()\n",
    "        self.text_field = text_field\n",
    "        self.data = pd.read_csv(filepath)\n",
    "        self.maxwordlen = maxwordlen\n",
    "        self.maxtextlen = maxtextlen\n",
    "        self.char2int = {s: i for s, i in zip(self.alphabet, range(len(self.alphabet)))}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        line = self.data.iloc[idx]\n",
    "        text = line[self.text_field].lower()\n",
    "        label = int(line.sentiment == 1.)\n",
    "\n",
    "        if self.noise_level > 0:\n",
    "            text = self._noise_generator(text)\n",
    "\n",
    "        text = self._tokenize(text)\n",
    "        text = self._preprocess(text)\n",
    "        return text, label\n",
    "\n",
    "    def _tokenize(self, text):\n",
    "        return [res['text'] for res in self.mystem.analyze(text) if res['text'] != ' ']\n",
    "\n",
    "    def _noise_generator(self, string):\n",
    "        noised = \"\"\n",
    "        for c in string:\n",
    "            if random() > self.noise_level:\n",
    "                noised += c\n",
    "            if random() < self.noise_level:\n",
    "                noised += choice(self.alphabet)\n",
    "        return noised\n",
    "\n",
    "    def _one_hot(self, char):\n",
    "        zeros = np.zeros(len(self.alphabet))\n",
    "        if char in self.char2int:\n",
    "            zeros[self.char2int[char]] = 1.\n",
    "        else:\n",
    "            zeros[self.char2int['<UNK>']] = 1.\n",
    "\n",
    "    def _preprocess(self, text):\n",
    "        _text_tensor = torch.zeros([self.maxwordlen * self.maxtextlen, len(self.alphabet)])\n",
    "        \n",
    "        for i, token in enumerate(text):\n",
    "            if i >= self.maxtextlen:\n",
    "                break\n",
    "            for j, char in enumerate(token):\n",
    "                if j >= self.maxwordlen:\n",
    "                    break\n",
    "                _text_tensor[i*self.maxwordlen + j, char2int.get(char, char2int['<UNK>'])] = 1.\n",
    "\n",
    "        return _text_tensor\n",
    "\n",
    "    def onehot2text(one_hotted_text, batch_size=None, show_pad=False):\n",
    "        if batch_size is None:\n",
    "            text = ''\n",
    "            max_values, idx = torch.max(one_hotted_text, 1)\n",
    "            for c, i in enumerate(idx):\n",
    "                if max_values[c] == 0:\n",
    "                    if show_pad:\n",
    "                        symb = '<PAD>'\n",
    "                    else:\n",
    "                        symb = ''\n",
    "                else:\n",
    "                    symb = ALPHABET[i]\n",
    "                text += symb\n",
    "            return text\n",
    "        else:\n",
    "            texts = []\n",
    "            for text in one_hotted_text:\n",
    "                texts.append(onehot2text(one_hotted_text, batch_size=None))\n",
    "            return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(model, test_data, noise_level=None):\n",
    "    \"\"\"\n",
    "    :param test_data: dataset or dataloader\n",
    "\n",
    "    Moder will be in TRAIN mode after that\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    predictions = []\n",
    "    lables = []\n",
    "    \n",
    "    if isinstance(test_data, torch.utils.data.Dataset):\n",
    "        if noise_level is not None:\n",
    "            test_data.noise_level = noise_level\n",
    "\n",
    "        test_dataloader = torch.utils.data.DataLoader(\n",
    "            test_data, batch_size=BATCH_SIZE\n",
    "        )\n",
    "    else:\n",
    "        assert isinstance(test_data, torch.utils.data.DataLoader)\n",
    "        test_dataloader = test_data\n",
    "\n",
    "    for text, label in test_dataloader:\n",
    "        if CUDA:\n",
    "            text = Variable(text.cuda())\n",
    "        else:\n",
    "            text = Variable(text)\n",
    "\n",
    "        text = text.permute(1, 0, 2)  # (1, 0, 2) for RNN\n",
    "        prediction = model(text)\n",
    "\n",
    "        _, idx = torch.max(prediction, 1)\n",
    "        predictions += idx.data.tolist()\n",
    "        lables += label.tolist()\n",
    "\n",
    "    acc = accuracy_score(lables, predictions)\n",
    "    f1 = f1_score(lables, predictions)\n",
    "    model.train()\n",
    "    return {'accuracy': acc, 'f1': f1}\n",
    "\n",
    "\n",
    "def onehot2text(one_hotted_text, batch_size=None, show_pad=False):\n",
    "    if batch_size is None:\n",
    "        text = ''\n",
    "        max_values, idx = torch.max(one_hotted_text, 1)\n",
    "        for c, i in enumerate(idx):\n",
    "            if max_values[c] == 0:\n",
    "                if show_pad:\n",
    "                    symb = '<PAD>'\n",
    "                else:\n",
    "                    symb = ''\n",
    "            else:\n",
    "                symb = ALPHABET[i]\n",
    "            text += symb\n",
    "        return text\n",
    "    else:\n",
    "        texts = []\n",
    "        for text in one_hotted_text:\n",
    "            texts.append(onehot2text(one_hotted_text, batch_size=None))\n",
    "        return texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# without spacy tokenizer it's commas all after the words =(\n",
    "\n",
    "text_field = torchtext.data.Field(\n",
    "    lower=True, include_lengths=False, fix_length=MAX_TEXT_LEN, tensor_type=torch.FloatTensor, batch_first=True,\n",
    "    use_vocab=False, tokenize='spacy'\n",
    ")\n",
    "label_field = torchtext.data.Field(sequential=False, use_vocab=False)\n",
    "\n",
    "train, test = HieracialIMDB.splits(text_field, label_field)\n",
    "\n",
    "dataloader, val_dataloader = get_train_valid_loader(train, VALID_SIZE, BATCH_SIZE)\n",
    "\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "    test, batch_size=BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"thismusicalisdecidedlymixed,andnoneoftheelementsreallyfittogether,butitsomehowmanagestobemostlyenjoyable.theplotcontainssomeoftheelementsofwodehouse'snovel,butnoneofitsvirtues,thoughheco-wrotethescript.thesongs,thoughcharming,havenothingtodowiththisparticularfilm,andareunusuallycrudelysqueezedintotheplot,evenbypre-oklahomastandards.burnsandallendotheirusualshtickquitecompetently,butitmissesthetoneoftherestofthefilmbyaboutfortyiqpoints.<br/><br/>thereareafewhighpoints.reginaldgardinerdoesgoodworkwhenheremembersthatthisisatalkie,andstopsmugginglikeasilentactor.andthereareafewbitsofwritingwhichcouldonlyhavebeenwrittenbywodehouse,thoughmostofthefilmfeelsliketheproductionofoneofthehollywoodmeetingshelaterparodied.\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onehot2text(train[0][0])  # no spaces is onehot2text problem, not a data one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mokoron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "basepath = '/media/data/nlp/sentiment/ru-mokoron/splits/'\n",
    "\n",
    "train = HieracialMokoron(basepath + 'train.csv', 'text_spellchecked')\n",
    "valid = HieracialMokoron(basepath + 'validation.csv', 'text_spellchecked')\n",
    "test = HieracialMokoron(basepath + 'test.csv', 'text_spellchecked')\n",
    "\n",
    "test_original = HieracialMokoron(basepath + 'test.csv', 'text_original')\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(train, BATCH_SIZE, shuffle=True, num_workers=4)\n",
    "val_dataloader = torch.utils.data.DataLoader(valid, BATCH_SIZE, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Статья: https://arxiv.org/abs/1508.06615\n",
    "\n",
    "Модель принципиально работает так же, но есть некоторые сильные упрощения:\n",
    "  * нету highway-слоя\n",
    "  * тут используется фильтры только одного размера (а не трёх, как в оригинальной статье)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Для RNN: (word, batch, feature)\n",
    "    CNN: (batch, word, feature)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YoonKimModel(nn.Module):\n",
    "    def __init__(self, n_filters, cnn_kernel_size, hidden_dim_out,\n",
    "                 dropout=0.5, init_function=None, embedding_dim=len(ALPHABET), pool_kernel_size=MAX_WORD_LEN):\n",
    "        \"\"\"\n",
    "        Default pooling is MaxOverTime pooling\n",
    "        \"\"\"\n",
    "        assert cnn_kernel_size % 2  # for 'same' padding\n",
    "\n",
    "        super(YoonKimModel, self).__init__()\n",
    "        self.dropout_prob = dropout\n",
    "        self.init_function = init_function\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.n_filters = n_filters\n",
    "        self.cnn_kernel_size = cnn_kernel_size\n",
    "        self.hidden_dim_out = hidden_dim_out\n",
    "\n",
    "        self.embedding = nn.Linear(len(ALPHABET), embedding_dim)\n",
    "        self.chars_cnn = nn.Sequential(\n",
    "            nn.Conv1d(embedding_dim, n_filters, kernel_size=cnn_kernel_size, stride=1, padding=int(cnn_kernel_size - 1) // 2),  # 'same' padding\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=pool_kernel_size)\n",
    "        )\n",
    "        if init_function is not None:\n",
    "            self.chars_cnn[0].weight = init_function(self.chars_cnn[0].weight)\n",
    "\n",
    "        _conv_stride = 1  # by default\n",
    "        _pool_stride = pool_kernel_size  # by default\n",
    "        # I am not sure this formula is always correct:\n",
    "        self.conv_dim = n_filters * max(1, int(((MAX_WORD_LEN - cnn_kernel_size) / _conv_stride - pool_kernel_size) / _pool_stride + 1))\n",
    "        self.dropout = nn.Dropout(self.dropout_prob)\n",
    "        self.words_rnn = nn.GRU(self.conv_dim, hidden_dim_out)\n",
    "        self.projector = nn.Linear(hidden_dim_out, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(1)\n",
    "        # TODO: hadrcode! (for CUDA)\n",
    "        words_tensor = Variable(torch.zeros(MAX_TEXT_LEN, batch_size, self.conv_dim)).cuda()\n",
    "        \n",
    "        for i in range(MAX_TEXT_LEN):\n",
    "            word = x[i * MAX_WORD_LEN : (i + 1) * MAX_WORD_LEN, :]\n",
    "            word = self.embedding(word)\n",
    "            word = word.permute(1, 2, 0)\n",
    "            word = self.chars_cnn(word)\n",
    "            word = word.view(word.size(0), -1)\n",
    "            words_tensor[i, :] = word\n",
    "\n",
    "        x, _ = self.words_rnn(words_tensor)\n",
    "        x = self.dropout(x)\n",
    "        x = self.projector(x[-1])\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_params_num(model):\n",
    "    return sum(np.prod(list(p.size())) for p in model.parameters())\n",
    "\n",
    "def mk_dataline(model_type, epochs, lr, noise_level_train, noise_level_test, acc_train, acc_test,\n",
    "                f1_train, f1_test, dropout, model, run_name, task, init_function=None):\n",
    "    return {\n",
    "        'task': task,\n",
    "        'model_type': model_type,\n",
    "        'trainable_params': model_params_num(model), 'dropout': dropout, 'init_function': init_function,\n",
    "        'epochs': epochs, 'lr': lr,\n",
    "        'noise_level_train': noise_level_train, 'noise_level_test': noise_level_test,\n",
    "        'acc_train': acc_train, 'acc_test': acc_test,\n",
    "        'f1_train': f1_train, 'f1_test': f1_test,\n",
    "        'model_desc': str(model),\n",
    "        'run_name': run_name,\n",
    "        'data_desc': 'MaxWordLen %s, MaxTexLen %s' % (MAX_WORD_LEN, MAX_TEXT_LEN)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model_with(noise_level, n_filters, cnn_kernel_size, hidden_dim_out, dropout=0.5,\n",
    "                   lr=1e-4, epochs=30, comment='', log_every=10, init_function=None, _model=None):\n",
    "\n",
    "    print_every = log_every\n",
    "    start_time = time()\n",
    "#     HieracialIMDB.noise_level = noise_level\n",
    "#     task='IMDB binary classification'\n",
    "    HieracialMokoron.noise_level = noise_level\n",
    "    task='Mokoron binary classification'\n",
    "\n",
    "    if _model is None:\n",
    "        model = YoonKimModel(\n",
    "            n_filters=n_filters, cnn_kernel_size=cnn_kernel_size, hidden_dim_out=hidden_dim_out, dropout=dropout,\n",
    "            init_function=init_function\n",
    "        )\n",
    "        if CUDA:\n",
    "            model.cuda()\n",
    "        model.train()\n",
    "    \n",
    "    else:\n",
    "        model = _model\n",
    "\n",
    "    model_name = '_YoonKim_lr%s_noise_level%s_wordlen8' % (\n",
    "        int(-np.log10(lr)), noise_level\n",
    "    ) + comment\n",
    "    \n",
    "    if '(' not in ALPHABET:\n",
    "        model_name += '_no_emoji'\n",
    "\n",
    "    writer = SummaryWriter(comment=model_name)\n",
    "    if len(list(writer.all_writers.keys())) > 1:\n",
    "        print('More than one writer! 0_o')\n",
    "        print(list(writer.all_writers.keys()))\n",
    "\n",
    "    run_name = list(writer.all_writers.keys())[0]\n",
    "    print('Writer: %s' % run_name)\n",
    "\n",
    "    optimizer = optim.Adam(params=model.parameters(), lr=lr)\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    global_step = 0\n",
    "\n",
    "    loss_f = F.cross_entropy\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        for batch_idx, (text, label) in enumerate(dataloader):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            if CUDA:\n",
    "                text = Variable(text.cuda())\n",
    "                label = Variable(torch.LongTensor(label).cuda())\n",
    "            else:\n",
    "                text = Variable(text)\n",
    "                label = Variable(torch.LongTensor(label))\n",
    "\n",
    "            text = text.permute(1, 0, 2)\n",
    "            prediction = model(text)\n",
    "            loss = loss_f(prediction, label)\n",
    "\n",
    "            writer.add_scalar('loss', loss.data[0], global_step=global_step)\n",
    "\n",
    "            loss.backward()        \n",
    "            torch.nn.utils.clip_grad_norm(model.parameters(), 1e-1)\n",
    "            optimizer.step()\n",
    "\n",
    "            if CUDA:\n",
    "                torch.cuda.synchronize()\n",
    "            global_step += 1\n",
    "\n",
    "        # evaluation\n",
    "        if epoch % print_every == 0:\n",
    "            print('Epoch %s. Global step %s. T=%s min' % (epoch, global_step, (time() - start_time) / 60.))\n",
    "            print('Loss               : %s' % loss.data[0])\n",
    "\n",
    "        # in-batch\n",
    "        _, idx = torch.max(prediction, 1)\n",
    "        _labels = label.data.tolist()\n",
    "        _predictions = idx.data.tolist()\n",
    "        acc = accuracy_score(_labels, _predictions)\n",
    "        f1 = f1_score(_labels, _predictions)\n",
    "        writer.add_scalar('accuracy_train', acc, global_step=global_step)\n",
    "        writer.add_scalar('f1_train', f1, global_step=global_step)\n",
    "        if epoch % print_every == 0:\n",
    "            print('In-batch accuracy  :', acc)\n",
    "\n",
    "        # validation\n",
    "        metrics = get_metrics(model, val_dataloader)\n",
    "        if epoch % print_every == 0:\n",
    "            print('Validation accuracy: %s, f1: %s' % (metrics['accuracy'], metrics['f1']))\n",
    "            print()\n",
    "\n",
    "        writer.add_scalar('accuracy_val', metrics['accuracy'], global_step=global_step)\n",
    "        writer.add_scalar('f1_val', metrics['f1'], global_step=global_step)\n",
    "\n",
    "    with open('models/%s.torch' % run_name.split('/')[-1], 'wb') as f:\n",
    "        try:\n",
    "            torch.save(model, f)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print('Continuing (probably) without saving')\n",
    "\n",
    "    print('Calculating validation metrics... Time %s min' % ((time() - start_time) / 60.))\n",
    "    metrics_train = get_metrics(model, dataloader)\n",
    "    acc_train = metrics_train['accuracy']\n",
    "    f1_train = metrics_train['f1']\n",
    "\n",
    "    for test_noise in tqdm(NOISE_LEVELS):\n",
    "        metrics = get_metrics(model, test, test_noise)\n",
    "        if test_noise == noise_level:\n",
    "            metrics_test = metrics\n",
    "\n",
    "        acc_test = metrics['accuracy']\n",
    "        f1_test = metrics['f1']\n",
    "        results.append(mk_dataline(\n",
    "            model_type='charCNN', epochs=epochs, lr=lr,\n",
    "            noise_level_train=noise_level, acc_train=acc_train, f1_train=f1_train,\n",
    "            noise_level_test=test_noise, acc_test=acc_test, f1_test=f1_test,\n",
    "            dropout=dropout, model=model,\n",
    "            init_function=init_function,\n",
    "            run_name=run_name,\n",
    "            task=task\n",
    "        ))\n",
    "    \n",
    "    # test original\n",
    "    metrics = get_metrics(model, test_original)\n",
    "    results.append(mk_dataline(\n",
    "        model_type='charCNN', epochs=epochs, lr=lr,\n",
    "        noise_level_train=noise_level, acc_train=acc_train, f1_train=f1_train,\n",
    "        noise_level_test=-1, acc_test=metrics['accuracy'], f1_test=metrics['f1'],\n",
    "        dropout=dropout, model=model,\n",
    "        init_function=init_function,\n",
    "        run_name=run_name,\n",
    "        task=task\n",
    "    ))\n",
    "    \n",
    "    print('Original dataset: acc %s, f1 %s' % (metrics['accuracy'], metrics['f1']))\n",
    "    writer.add_scalar('accuracy_test_original', metrics['accuracy'], global_step=global_step)\n",
    "    writer.add_scalar('f1_test_original', metrics['f1'], global_step=global_step)\n",
    "\n",
    "    print('Final test metrics: %s, Time %s min' % (metrics_test, ((time() - start_time) / 60.)))\n",
    "    if metrics_test is not None:\n",
    "        writer.add_scalar('accuracy_test_final', metrics_test['accuracy'], global_step=global_step)\n",
    "        writer.add_scalar('f1_test_final', metrics_test['f1'], global_step=global_step)\n",
    "    print()\n",
    "    # model is in EVAL mode!\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/11 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writer: runs/May13_01-35-55_phobos-aijun_YoonKim_lr3_noise_level0_wordlen8_no_emoji\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for noise_level in tqdm(NOISE_LEVELS):\n",
    "    model = run_model_with(\n",
    "        noise_level=noise_level, n_filters=256, cnn_kernel_size=5, hidden_dim_out=128, dropout=0.5,\n",
    "        lr=1e-3, epochs=30, log_every=10\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writer: runs/May13_01-35-21_phobos-aijun_YoonKim_lr3_noise_level0_wordlen8_test_no_emoji\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-16:\n",
      "Process Process-15:\n",
      "Traceback (most recent call last):\n",
      "Process Process-13:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/phobos_aijun/.virtualenvs/pytorch-env/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 50, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/phobos_aijun/.virtualenvs/pytorch-env/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 50, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "Process Process-14:\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/home/phobos_aijun/.virtualenvs/pytorch-env/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 55, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/phobos_aijun/.virtualenvs/pytorch-env/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 55, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/phobos_aijun/.virtualenvs/pytorch-env/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 50, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 343, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "  File \"<ipython-input-11-a24a7dfaedde>\", line 44, in __getitem__\n",
      "    text = self._tokenize(text)\n",
      "  File \"<ipython-input-11-a24a7dfaedde>\", line 49, in _tokenize\n",
      "    return [res['text'] for res in self.mystem.analyze(text) if res['text'] != ' ']\n",
      "  File \"/home/phobos_aijun/.virtualenvs/pytorch-env/lib/python3.5/site-packages/pymystem3/mystem.py\", line 249, in analyze\n",
      "    result.extend(self._analyze_impl(line))\n",
      "  File \"/home/phobos_aijun/.virtualenvs/pytorch-env/lib/python3.5/site-packages/pymystem3/mystem.py\", line 287, in _analyze_impl\n",
      "    select.select([self._procout_no], [], [])\n",
      "KeyboardInterrupt\n",
      "Exception ignored in: <bound method DataLoaderIter.__del__ of <torch.utils.data.dataloader.DataLoaderIter object at 0x7f33fd488128>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/phobos_aijun/.virtualenvs/pytorch-env/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 333, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/phobos_aijun/.virtualenvs/pytorch-env/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 319, in _shutdown_workers\n",
      "    self.data_queue.get()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 345, in get\n",
      "    return ForkingPickler.loads(res)\n",
      "  File \"/home/phobos_aijun/.virtualenvs/pytorch-env/lib/python3.5/site-packages/torch/multiprocessing/reductions.py\", line 70, in rebuild_storage_fd\n",
      "    fd = df.detach()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/resource_sharer.py\", line 57, in detach\n",
      "    with _resource_sharer.get_connection(self._id) as conn:\n",
      "  File \"/usr/lib/python3.5/multiprocessing/resource_sharer.py\", line 87, in get_connection\n",
      "    c = Client(address, authkey=process.current_process().authkey)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 487, in Client\n",
      "    c = SocketClient(address)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 614, in SocketClient\n",
      "    s.connect(address)\n",
      "FileNotFoundError: [Errno 2] No such file or directory\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-28c708407b8f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m model = run_model_with(\n\u001b[1;32m      2\u001b[0m     \u001b[0mnoise_level\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_filters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcnn_kernel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_dim_out\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcomment\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'_test'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m )\n",
      "\u001b[0;32m<ipython-input-8-466a6762fc9f>\u001b[0m in \u001b[0;36mrun_model_with\u001b[0;34m(noise_level, n_filters, cnn_kernel_size, hidden_dim_out, dropout, lr, epochs, comment, print_every, init_function, _model)\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1e-1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/pytorch-env/lib/python3.5/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m    165\u001b[0m                 \u001b[0mVariable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \"\"\"\n\u001b[0;32m--> 167\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/pytorch-env/lib/python3.5/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(variables, grad_variables, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m---> 99\u001b[0;31m         variables, grad_variables, retain_graph)\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = run_model_with(\n",
    "    noise_level=0, n_filters=16, cnn_kernel_size=5, hidden_dim_out=8, dropout=0.5,\n",
    "    lr=1e-3, epochs=1, comment='_test'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1. Global step 1408\n",
      "Loss               : 0.7031866908073425\n",
      "In-batch accuracy  : 0.5\n",
      "Validation accuracy: 0.5004\n",
      "\n",
      "Epoch 2. Global step 2112\n",
      "Loss               : 0.5050830245018005\n",
      "In-batch accuracy  : 0.75\n",
      "Validation accuracy: 0.5008\n",
      "\n",
      "Epoch 3. Global step 2816\n",
      "Loss               : 0.6299910545349121\n",
      "In-batch accuracy  : 0.75\n",
      "Validation accuracy: 0.5\n",
      "\n",
      "Epoch 4. Global step 3520\n",
      "Loss               : 0.7347459197044373\n",
      "In-batch accuracy  : 0.5\n",
      "Validation accuracy: 0.7084\n",
      "\n",
      "Epoch 5. Global step 4224\n",
      "Loss               : 0.15430842339992523\n",
      "In-batch accuracy  : 1.0\n",
      "Validation accuracy: 0.79\n",
      "\n",
      "Epoch 6. Global step 4928\n",
      "Loss               : 0.03517886996269226\n",
      "In-batch accuracy  : 1.0\n",
      "Validation accuracy: 0.8184\n",
      "\n",
      "Epoch 7. Global step 5632\n",
      "Loss               : 1.1862972974777222\n",
      "In-batch accuracy  : 0.5\n",
      "Validation accuracy: 0.7648\n",
      "\n",
      "Epoch 8. Global step 6336\n",
      "Loss               : 0.20773214101791382\n",
      "In-batch accuracy  : 0.75\n",
      "Validation accuracy: 0.8332\n",
      "\n",
      "Epoch 9. Global step 7040\n",
      "Loss               : 1.068119764328003\n",
      "In-batch accuracy  : 0.75\n",
      "Validation accuracy: 0.7864\n",
      "\n",
      "Epoch 11. Global step 8448\n",
      "Loss               : 0.07731851935386658\n",
      "In-batch accuracy  : 1.0\n",
      "Validation accuracy: 0.8436\n",
      "\n",
      "Epoch 12. Global step 9152\n",
      "Loss               : 0.24888820946216583\n",
      "In-batch accuracy  : 0.75\n",
      "Validation accuracy: 0.846\n",
      "\n",
      "Epoch 13. Global step 9856\n",
      "Loss               : 0.7355042695999146\n",
      "In-batch accuracy  : 0.75\n",
      "Validation accuracy: 0.8456\n",
      "\n",
      "Epoch 14. Global step 10560\n",
      "Loss               : 0.0801142156124115\n",
      "In-batch accuracy  : 1.0\n",
      "Validation accuracy: 0.8356\n",
      "\n",
      "Epoch 15. Global step 11264\n",
      "Loss               : 0.0933920294046402\n",
      "In-batch accuracy  : 1.0\n",
      "Validation accuracy: 0.8408\n",
      "\n",
      "Epoch 16. Global step 11968\n",
      "Loss               : 0.010582119226455688\n",
      "In-batch accuracy  : 1.0\n",
      "Validation accuracy: 0.8212\n",
      "\n",
      "Epoch 17. Global step 12672\n",
      "Loss               : 0.01149103045463562\n",
      "In-batch accuracy  : 1.0\n",
      "Validation accuracy: 0.8376\n",
      "\n",
      "Epoch 18. Global step 13376\n",
      "Loss               : 0.0494811087846756\n",
      "In-batch accuracy  : 1.0\n",
      "Validation accuracy: 0.8424\n",
      "\n",
      "Epoch 19. Global step 14080\n",
      "Loss               : 0.0008117556571960449\n",
      "In-batch accuracy  : 1.0\n",
      "Validation accuracy: 0.8316\n",
      "\n",
      "Epoch 21. Global step 15488\n",
      "Loss               : 0.0007413625717163086\n",
      "In-batch accuracy  : 1.0\n",
      "Validation accuracy: 0.8352\n",
      "\n",
      "Epoch 22. Global step 16192\n",
      "Loss               : 0.000470578670501709\n",
      "In-batch accuracy  : 1.0\n",
      "Validation accuracy: 0.8328\n",
      "\n",
      "Epoch 23. Global step 16896\n",
      "Loss               : 0.001531362533569336\n",
      "In-batch accuracy  : 1.0\n",
      "Validation accuracy: 0.8368\n",
      "\n",
      "Epoch 24. Global step 17600\n",
      "Loss               : 0.002270340919494629\n",
      "In-batch accuracy  : 1.0\n",
      "Validation accuracy: 0.8532\n",
      "\n",
      "Epoch 25. Global step 18304\n",
      "Loss               : 0.001860976219177246\n",
      "In-batch accuracy  : 1.0\n",
      "Validation accuracy: 0.8472\n",
      "\n",
      "Epoch 26. Global step 19008\n",
      "Loss               : 0.004954040050506592\n",
      "In-batch accuracy  : 1.0\n",
      "Validation accuracy: 0.8256\n",
      "\n",
      "Epoch 27. Global step 19712\n",
      "Loss               : 0.017662465572357178\n",
      "In-batch accuracy  : 1.0\n",
      "Validation accuracy: 0.8472\n",
      "\n",
      "Epoch 28. Global step 20416\n",
      "Loss               : 0.0011481642723083496\n",
      "In-batch accuracy  : 1.0\n",
      "Validation accuracy: 0.8388\n",
      "\n",
      "Epoch 29. Global step 21120\n",
      "Loss               : 0.405687153339386\n",
      "In-batch accuracy  : 0.75\n",
      "Validation accuracy: 0.846\n",
      "\n",
      "Final test accuracy: 0.84696\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = run_model_with(\n",
    "    noise_level=0.025, n_filters=256, cnn_kernel_size=5, hidden_dim_out=128, dropout=0.5,\n",
    "    lr=1e-3, epochs=30\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1. Global step 1408\n",
      "Loss               : 0.6502748727798462\n",
      "In-batch accuracy  : 0.5\n",
      "Validation accuracy: 0.4932\n",
      "\n",
      "Epoch 2. Global step 2112\n",
      "Loss               : 0.7270681262016296\n",
      "In-batch accuracy  : 0.25\n",
      "Validation accuracy: 0.5012\n",
      "\n",
      "Epoch 3. Global step 2816\n",
      "Loss               : 0.6907497644424438\n",
      "In-batch accuracy  : 0.25\n",
      "Validation accuracy: 0.4964\n",
      "\n",
      "Epoch 4. Global step 3520\n",
      "Loss               : 0.714634895324707\n",
      "In-batch accuracy  : 0.5\n",
      "Validation accuracy: 0.5076\n",
      "\n",
      "Epoch 5. Global step 4224\n",
      "Loss               : 0.8166068196296692\n",
      "In-batch accuracy  : 0.5\n",
      "Validation accuracy: 0.5996\n",
      "\n",
      "Epoch 6. Global step 4928\n",
      "Loss               : 0.31144529581069946\n",
      "In-batch accuracy  : 1.0\n",
      "Validation accuracy: 0.706\n",
      "\n",
      "Epoch 7. Global step 5632\n",
      "Loss               : 0.5351625084877014\n",
      "In-batch accuracy  : 0.75\n",
      "Validation accuracy: 0.6728\n",
      "\n",
      "Epoch 8. Global step 6336\n",
      "Loss               : 0.12647317349910736\n",
      "In-batch accuracy  : 1.0\n",
      "Validation accuracy: 0.7908\n",
      "\n",
      "Epoch 9. Global step 7040\n",
      "Loss               : 0.3563786745071411\n",
      "In-batch accuracy  : 0.75\n",
      "Validation accuracy: 0.7848\n",
      "\n",
      "Epoch 11. Global step 8448\n",
      "Loss               : 0.1313670575618744\n",
      "In-batch accuracy  : 1.0\n",
      "Validation accuracy: 0.8124\n",
      "\n",
      "Epoch 12. Global step 9152\n",
      "Loss               : 0.08252072334289551\n",
      "In-batch accuracy  : 1.0\n",
      "Validation accuracy: 0.824\n",
      "\n",
      "Epoch 13. Global step 9856\n",
      "Loss               : 0.19958239793777466\n",
      "In-batch accuracy  : 0.75\n",
      "Validation accuracy: 0.7996\n",
      "\n",
      "Epoch 14. Global step 10560\n",
      "Loss               : 0.2308722734451294\n",
      "In-batch accuracy  : 0.75\n",
      "Validation accuracy: 0.822\n",
      "\n",
      "Epoch 15. Global step 11264\n",
      "Loss               : 0.4843207001686096\n",
      "In-batch accuracy  : 0.75\n",
      "Validation accuracy: 0.8312\n",
      "\n",
      "Epoch 16. Global step 11968\n",
      "Loss               : 0.16900651156902313\n",
      "In-batch accuracy  : 1.0\n",
      "Validation accuracy: 0.8352\n",
      "\n",
      "Epoch 17. Global step 12672\n",
      "Loss               : 0.12670356035232544\n",
      "In-batch accuracy  : 1.0\n",
      "Validation accuracy: 0.8244\n",
      "\n",
      "Epoch 18. Global step 13376\n",
      "Loss               : 0.30379000306129456\n",
      "In-batch accuracy  : 0.75\n",
      "Validation accuracy: 0.8384\n",
      "\n",
      "Epoch 19. Global step 14080\n",
      "Loss               : 0.24916201829910278\n",
      "In-batch accuracy  : 0.75\n",
      "Validation accuracy: 0.8312\n",
      "\n",
      "Epoch 21. Global step 15488\n",
      "Loss               : 0.2568575143814087\n",
      "In-batch accuracy  : 0.75\n",
      "Validation accuracy: 0.8404\n",
      "\n",
      "Validation accuracy: 0.8408\n",
      "\n",
      "Epoch 23. Global step 16896\n",
      "Loss               : 0.7050740718841553\n",
      "In-batch accuracy  : 0.5\n",
      "Validation accuracy: 0.838\n",
      "\n",
      "Epoch 24. Global step 17600\n",
      "Loss               : 1.0641887187957764\n",
      "In-batch accuracy  : 0.75\n",
      "Validation accuracy: 0.838\n",
      "\n",
      "Epoch 25. Global step 18304\n",
      "Loss               : 0.0772596001625061\n",
      "In-batch accuracy  : 1.0\n",
      "Validation accuracy: 0.8412\n",
      "\n",
      "Epoch 26. Global step 19008\n",
      "Loss               : 0.19044381380081177\n",
      "In-batch accuracy  : 0.75\n",
      "Validation accuracy: 0.8268\n",
      "\n",
      "Epoch 27. Global step 19712\n",
      "Loss               : 0.18402442336082458\n",
      "In-batch accuracy  : 1.0\n",
      "Validation accuracy: 0.8408\n",
      "\n",
      "Epoch 28. Global step 20416\n",
      "Loss               : 0.11417007446289062\n",
      "In-batch accuracy  : 1.0\n",
      "Validation accuracy: 0.8344\n",
      "\n",
      "Epoch 29. Global step 21120\n",
      "Loss               : 1.0245332717895508\n",
      "In-batch accuracy  : 0.5\n",
      "Validation accuracy: 0.8284\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1/5 [1:12:09<4:48:37, 4329.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final test accuracy: 0.82904\n",
      "\n",
      "Epoch 1. Global step 1408\n",
      "Loss               : 0.6897575855255127\n",
      "In-batch accuracy  : 0.5\n",
      "Validation accuracy: 0.4948\n",
      "\n",
      "Epoch 2. Global step 2112\n",
      "Loss               : 0.6982558965682983\n",
      "In-batch accuracy  : 0.25\n",
      "Validation accuracy: 0.4956\n",
      "\n",
      "Epoch 3. Global step 2816\n",
      "Loss               : 0.7294421195983887\n",
      "In-batch accuracy  : 0.25\n",
      "Validation accuracy: 0.4932\n",
      "\n",
      "Epoch 4. Global step 3520\n",
      "Loss               : 0.6975125670433044\n",
      "In-batch accuracy  : 0.5\n",
      "Validation accuracy: 0.4892\n",
      "\n",
      "Epoch 5. Global step 4224\n",
      "Loss               : 0.6847110986709595\n",
      "In-batch accuracy  : 0.75\n",
      "Validation accuracy: 0.496\n",
      "\n",
      "Epoch 6. Global step 4928\n",
      "Loss               : 0.6590765714645386\n",
      "In-batch accuracy  : 0.75\n",
      "Validation accuracy: 0.5052\n",
      "\n",
      "Epoch 7. Global step 5632\n",
      "Loss               : 0.6474947929382324\n",
      "In-batch accuracy  : 0.75\n",
      "Validation accuracy: 0.5068\n",
      "\n",
      "Epoch 8. Global step 6336\n",
      "Loss               : 0.6409823298454285\n",
      "In-batch accuracy  : 0.75\n",
      "Validation accuracy: 0.7116\n",
      "\n",
      "Epoch 9. Global step 7040\n",
      "Loss               : 0.5735766291618347\n",
      "In-batch accuracy  : 0.75\n",
      "Validation accuracy: 0.7344\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for noise_level in tqdm([0.1, 0.125, 0.15, 0.175, 0.2]):  # and 0, 0.01, 0.025, 0.05, 0.075,\n",
    "    model = run_model_with(\n",
    "        noise_level=noise_level, n_filters=256, cnn_kernel_size=5, hidden_dim_out=128, dropout=0.5,\n",
    "        lr=1e-3, epochs=30\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Global step 704\n",
      "Loss               : 0.6782099604606628\n",
      "In-batch accuracy  : 0.75\n",
      "Validation accuracy: 0.4988\n",
      "\n",
      "Epoch 1. Global step 1408\n",
      "Loss               : 0.69975745677948\n",
      "In-batch accuracy  : 0.5\n",
      "Validation accuracy: 0.5012\n",
      "\n",
      "Epoch 2. Global step 2112\n",
      "Loss               : 0.6876887679100037\n",
      "In-batch accuracy  : 0.5\n",
      "Validation accuracy: 0.5032\n",
      "\n",
      "Epoch 3. Global step 2816\n",
      "Loss               : 0.7106724977493286\n",
      "In-batch accuracy  : 0.5\n",
      "Validation accuracy: 0.516\n",
      "\n",
      "Epoch 4. Global step 3520\n",
      "Loss               : 0.9923454523086548\n",
      "In-batch accuracy  : 0.5\n",
      "Validation accuracy: 0.5132\n",
      "\n",
      "Epoch 5. Global step 4224\n",
      "Loss               : 0.5627555847167969\n",
      "In-batch accuracy  : 0.75\n",
      "Validation accuracy: 0.6548\n",
      "\n",
      "Epoch 6. Global step 4928\n",
      "Loss               : 0.4199898838996887\n",
      "In-batch accuracy  : 0.75\n",
      "Validation accuracy: 0.7192\n",
      "\n",
      "Epoch 7. Global step 5632\n",
      "Loss               : 0.5541638135910034\n",
      "In-batch accuracy  : 0.75\n",
      "Validation accuracy: 0.8256\n",
      "\n",
      "Epoch 8. Global step 6336\n",
      "Loss               : 0.5349381566047668\n",
      "In-batch accuracy  : 0.75\n",
      "Validation accuracy: 0.8344\n",
      "\n",
      "Epoch 9. Global step 7040\n",
      "Loss               : 0.7254611253738403\n",
      "In-batch accuracy  : 0.75\n",
      "Validation accuracy: 0.8492\n",
      "\n",
      "Epoch 10. Global step 7744\n",
      "Loss               : 0.10496724396944046\n",
      "In-batch accuracy  : 1.0\n",
      "Validation accuracy: 0.8512\n",
      "\n",
      "Epoch 11. Global step 8448\n",
      "Loss               : 0.048270970582962036\n",
      "In-batch accuracy  : 1.0\n",
      "Validation accuracy: 0.8532\n",
      "\n",
      "Epoch 12. Global step 9152\n",
      "Loss               : 0.10557051002979279\n",
      "In-batch accuracy  : 1.0\n",
      "Validation accuracy: 0.7932\n",
      "\n",
      "Epoch 13. Global step 9856\n",
      "Loss               : 0.03192335367202759\n",
      "In-batch accuracy  : 1.0\n",
      "Validation accuracy: 0.8432\n",
      "\n",
      "Epoch 14. Global step 10560\n",
      "Loss               : 0.002422630786895752\n",
      "In-batch accuracy  : 1.0\n",
      "Validation accuracy: 0.8376\n",
      "\n",
      "Epoch 15. Global step 11264\n",
      "Loss               : 0.034755393862724304\n",
      "In-batch accuracy  : 1.0\n",
      "Validation accuracy: 0.8388\n",
      "\n",
      "Epoch 16. Global step 11968\n",
      "Loss               : 0.002298414707183838\n",
      "In-batch accuracy  : 1.0\n",
      "Validation accuracy: 0.8412\n",
      "\n",
      "Epoch 17. Global step 12672\n",
      "Loss               : 0.05393856763839722\n",
      "In-batch accuracy  : 1.0\n",
      "Validation accuracy: 0.8416\n",
      "\n",
      "Epoch 18. Global step 13376\n",
      "Loss               : 0.00013387203216552734\n",
      "In-batch accuracy  : 1.0\n",
      "Validation accuracy: 0.8376\n",
      "\n",
      "Epoch 19. Global step 14080\n",
      "Loss               : 0.0022661685943603516\n",
      "In-batch accuracy  : 1.0\n",
      "Validation accuracy: 0.8444\n",
      "\n",
      "Epoch 20. Global step 14784\n",
      "Loss               : 0.00010859966278076172\n",
      "In-batch accuracy  : 1.0\n",
      "Validation accuracy: 0.8412\n",
      "\n",
      "Epoch 21. Global step 15488\n",
      "Loss               : 5.799531936645508e-05\n",
      "In-batch accuracy  : 1.0\n",
      "Validation accuracy: 0.8428\n",
      "\n",
      "Epoch 22. Global step 16192\n",
      "Loss               : 0.00028121471405029297\n",
      "In-batch accuracy  : 1.0\n",
      "Validation accuracy: 0.8416\n",
      "\n",
      "Epoch 23. Global step 16896\n",
      "Loss               : 7.987022399902344e-06\n",
      "In-batch accuracy  : 1.0\n",
      "Validation accuracy: 0.8296\n",
      "\n",
      "Epoch 24. Global step 17600\n",
      "Loss               : 0.0014231204986572266\n",
      "In-batch accuracy  : 1.0\n",
      "Validation accuracy: 0.8372\n",
      "\n",
      "Epoch 25. Global step 18304\n",
      "Loss               : 0.0003555417060852051\n",
      "In-batch accuracy  : 1.0\n",
      "Validation accuracy: 0.8364\n",
      "\n",
      "Epoch 26. Global step 19008\n",
      "Loss               : 0.7203046679496765\n",
      "In-batch accuracy  : 0.75\n",
      "Validation accuracy: 0.8452\n",
      "\n",
      "Epoch 27. Global step 19712\n",
      "Loss               : 0.00013840198516845703\n",
      "In-batch accuracy  : 1.0\n",
      "Validation accuracy: 0.8468\n",
      "\n",
      "Epoch 28. Global step 20416\n",
      "Loss               : 1.430511474609375e-05\n",
      "In-batch accuracy  : 1.0\n",
      "Validation accuracy: 0.8412\n",
      "\n",
      "Epoch 29. Global step 21120\n",
      "Loss               : 1.6181766986846924\n",
      "In-batch accuracy  : 0.75\n",
      "Validation accuracy: 0.838\n",
      "\n",
      "Final test accuracy: 0.84784\n",
      "\n",
      "CPU times: user 1h 29min 1s, sys: 6min 23s, total: 1h 35min 24s\n",
      "Wall time: 1h 20min 56s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = run_model_with(\n",
    "    noise_level=0, n_filters=256, cnn_kernel_size=5, hidden_dim_out=128, dropout=0.5,\n",
    "    lr=1e-3, epochs=30\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0.01, 0.025, 0.05, 0.075, 0.1, 0.125, 0.15, 0.175, 0.2]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted([0, 0.1, 0.05, 0.01, 0.025, 0.075, 0.125, 0.15, 0.175, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Global step 704\n",
      "Loss               : 0.6931502223014832\n",
      "In-batch accuracy  : 0.5\n",
      "Validation accuracy: 0.502\n",
      "\n",
      "Epoch 2. Global step 2112\n",
      "Loss               : 0.6914504766464233\n",
      "In-batch accuracy  : 0.5\n",
      "Validation accuracy: 0.5096\n",
      "\n",
      "Epoch 3. Global step 2816\n",
      "Loss               : 0.7026252746582031\n",
      "In-batch accuracy  : 0.5\n",
      "Validation accuracy: 0.5092\n",
      "\n",
      "Epoch 4. Global step 3520\n",
      "Loss               : 0.6278555393218994\n",
      "In-batch accuracy  : 1.0\n",
      "Validation accuracy: 0.5184\n",
      "\n",
      "Epoch 5. Global step 4224\n",
      "Loss               : 0.6162382364273071\n",
      "In-batch accuracy  : 0.75\n",
      "Validation accuracy: 0.5192\n",
      "\n",
      "Epoch 6. Global step 4928\n",
      "Loss               : 0.6086032390594482\n",
      "In-batch accuracy  : 0.75\n",
      "Validation accuracy: 0.7548\n",
      "\n",
      "Epoch 7. Global step 5632\n",
      "Loss               : 0.10206340253353119\n",
      "In-batch accuracy  : 1.0\n",
      "Validation accuracy: 0.8308\n",
      "\n",
      "Epoch 8. Global step 6336\n",
      "Loss               : 0.08467632532119751\n",
      "In-batch accuracy  : 1.0\n",
      "Validation accuracy: 0.8128\n",
      "\n",
      "Epoch 9. Global step 7040\n",
      "Loss               : 0.7322097420692444\n",
      "In-batch accuracy  : 0.75\n",
      "Validation accuracy: 0.8492\n",
      "\n",
      "Epoch 10. Global step 7744\n",
      "Loss               : 0.809004545211792\n",
      "In-batch accuracy  : 0.5\n",
      "Validation accuracy: 0.8424\n",
      "\n",
      "Epoch 11. Global step 8448\n",
      "Loss               : 0.03934547305107117\n",
      "In-batch accuracy  : 1.0\n",
      "Validation accuracy: 0.8292\n",
      "\n",
      "Epoch 12. Global step 9152\n",
      "Loss               : 0.03142175078392029\n",
      "In-batch accuracy  : 1.0\n",
      "Validation accuracy: 0.8464\n",
      "\n",
      "Epoch 13. Global step 9856\n",
      "Loss               : 0.8024514317512512\n",
      "In-batch accuracy  : 0.75\n",
      "Validation accuracy: 0.8508\n",
      "\n",
      "Epoch 14. Global step 10560\n",
      "Loss               : 0.31346994638442993\n",
      "In-batch accuracy  : 0.75\n",
      "Validation accuracy: 0.8384\n",
      "\n",
      "Final test accuracy: 0.84052\n",
      "\n",
      "CPU times: user 45min 44s, sys: 3min 15s, total: 49min\n",
      "Wall time: 40min 54s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = run_model_with(\n",
    "    noise_level=0.01, n_filters=256, cnn_kernel_size=5, hidden_dim_out=128, dropout=0.5,\n",
    "    lr=1e-3, epochs=15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Global step 704\n",
      "Loss               : 0.689181923866272\n",
      "In-batch accuracy  : 0.5\n",
      "Validation accuracy: 0.496\n",
      "\n",
      "Epoch 1. Global step 1408\n",
      "Loss               : 0.6932675242424011\n",
      "In-batch accuracy  : 0.5\n",
      "Validation accuracy: 0.4952\n",
      "\n",
      "Epoch 2. Global step 2112\n",
      "Loss               : 0.6630696058273315\n",
      "In-batch accuracy  : 0.75\n",
      "Validation accuracy: 0.4912\n",
      "\n",
      "Epoch 3. Global step 2816\n",
      "Loss               : 0.6668293476104736\n",
      "In-batch accuracy  : 0.5\n",
      "Validation accuracy: 0.5108\n",
      "\n",
      "Epoch 4. Global step 3520\n",
      "Loss               : 0.6932008862495422\n",
      "In-batch accuracy  : 0.5\n",
      "Validation accuracy: 0.4996\n",
      "\n",
      "Epoch 5. Global step 4224\n",
      "Loss               : 0.6827026605606079\n",
      "In-batch accuracy  : 0.5\n",
      "Validation accuracy: 0.5124\n",
      "\n",
      "Epoch 6. Global step 4928\n",
      "Loss               : 0.7104420065879822\n",
      "In-batch accuracy  : 0.5\n",
      "Validation accuracy: 0.6064\n",
      "\n",
      "Epoch 7. Global step 5632\n",
      "Loss               : 0.6695060729980469\n",
      "In-batch accuracy  : 0.5\n",
      "Validation accuracy: 0.5512\n",
      "\n",
      "Epoch 8. Global step 6336\n",
      "Loss               : 0.996961236000061\n",
      "In-batch accuracy  : 0.5\n",
      "Validation accuracy: 0.7016\n",
      "\n",
      "Epoch 9. Global step 7040\n",
      "Loss               : 0.2792210280895233\n",
      "In-batch accuracy  : 1.0\n",
      "Validation accuracy: 0.826\n",
      "\n",
      "Epoch 10. Global step 7744\n",
      "Loss               : 0.17558465898036957\n",
      "In-batch accuracy  : 1.0\n",
      "Validation accuracy: 0.83\n",
      "\n",
      "Epoch 11. Global step 8448\n",
      "Loss               : 0.07813744246959686\n",
      "In-batch accuracy  : 1.0\n",
      "Validation accuracy: 0.8332\n",
      "\n",
      "Epoch 12. Global step 9152\n",
      "Loss               : 0.9678097367286682\n",
      "In-batch accuracy  : 0.75\n",
      "Validation accuracy: 0.8556\n",
      "\n",
      "Epoch 13. Global step 9856\n",
      "Loss               : 0.014568418264389038\n",
      "In-batch accuracy  : 1.0\n",
      "Validation accuracy: 0.8492\n",
      "\n",
      "Epoch 14. Global step 10560\n",
      "Loss               : 0.16788388788700104\n",
      "In-batch accuracy  : 1.0\n",
      "Validation accuracy: 0.8324\n",
      "\n",
      "Final test accuracy: 0.82988\n",
      "\n",
      "CPU times: user 45min 46s, sys: 3min 18s, total: 49min 4s\n",
      "Wall time: 40min 59s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = run_model_with(\n",
    "    noise_level=0.025, n_filters=256, cnn_kernel_size=5, hidden_dim_out=128, dropout=0.5,\n",
    "    lr=1e-3, epochs=15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Global step 704\n",
      "Loss               : 0.7043141722679138\n",
      "In-batch accuracy  : 0.0\n",
      "Validation accuracy: 0.498\n",
      "\n",
      "Epoch 1. Global step 1408\n",
      "Loss               : 0.6982800960540771\n",
      "In-batch accuracy  : 0.0\n",
      "Validation accuracy: 0.4948\n",
      "\n",
      "Epoch 2. Global step 2112\n",
      "Loss               : 0.7062378525733948\n",
      "In-batch accuracy  : 0.5\n",
      "Validation accuracy: 0.4996\n",
      "\n",
      "Epoch 3. Global step 2816\n",
      "Loss               : 0.6739117503166199\n",
      "In-batch accuracy  : 1.0\n",
      "Validation accuracy: 0.5044\n",
      "\n",
      "Epoch 4. Global step 3520\n",
      "Loss               : 0.7167145013809204\n",
      "In-batch accuracy  : 0.5\n",
      "Validation accuracy: 0.5016\n",
      "\n",
      "Epoch 5. Global step 4224\n",
      "Loss               : 0.7106537818908691\n",
      "In-batch accuracy  : 0.5\n",
      "Validation accuracy: 0.5104\n",
      "\n",
      "Epoch 6. Global step 4928\n",
      "Loss               : 0.6512269973754883\n",
      "In-batch accuracy  : 0.75\n",
      "Validation accuracy: 0.5224\n",
      "\n",
      "Epoch 7. Global step 5632\n",
      "Loss               : 0.5143400430679321\n",
      "In-batch accuracy  : 0.75\n",
      "Validation accuracy: 0.568\n",
      "\n",
      "Epoch 8. Global step 6336\n",
      "Loss               : 0.30754855275154114\n",
      "In-batch accuracy  : 1.0\n",
      "Validation accuracy: 0.6928\n",
      "\n",
      "Epoch 9. Global step 7040\n",
      "Loss               : 0.1553017795085907\n",
      "In-batch accuracy  : 1.0\n",
      "Validation accuracy: 0.7748\n",
      "\n",
      "Epoch 10. Global step 7744\n",
      "Loss               : 0.9577498435974121\n",
      "In-batch accuracy  : 0.5\n",
      "Validation accuracy: 0.7908\n",
      "\n",
      "Epoch 11. Global step 8448\n",
      "Loss               : 0.09498319029808044\n",
      "In-batch accuracy  : 1.0\n",
      "Validation accuracy: 0.83\n",
      "\n",
      "Epoch 12. Global step 9152\n",
      "Loss               : 0.08902865648269653\n",
      "In-batch accuracy  : 1.0\n",
      "Validation accuracy: 0.8404\n",
      "\n",
      "Epoch 13. Global step 9856\n",
      "Loss               : 0.5370571613311768\n",
      "In-batch accuracy  : 0.75\n",
      "Validation accuracy: 0.7908\n",
      "\n",
      "Epoch 14. Global step 10560\n",
      "Loss               : 0.3673233985900879\n",
      "In-batch accuracy  : 0.75\n",
      "Validation accuracy: 0.8312\n",
      "\n",
      "Final test accuracy: 0.8346\n",
      "\n",
      "CPU times: user 45min 44s, sys: 3min 16s, total: 49min\n",
      "Wall time: 40min 52s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = run_model_with(\n",
    "    noise_level=0.05, n_filters=256, cnn_kernel_size=5, hidden_dim_out=128, dropout=0.5,\n",
    "    lr=1e-3, epochs=15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Global step 704\n",
      "Loss               : 0.6651508212089539\n",
      "In-batch accuracy  : 0.75\n",
      "Validation accuracy: 0.4988\n",
      "\n",
      "Epoch 1. Global step 1408\n",
      "Loss               : 0.687353253364563\n",
      "In-batch accuracy  : 0.5\n",
      "Validation accuracy: 0.5124\n",
      "\n",
      "Epoch 2. Global step 2112\n",
      "Loss               : 0.693524181842804\n",
      "In-batch accuracy  : 0.25\n",
      "Validation accuracy: 0.5012\n",
      "\n",
      "Epoch 3. Global step 2816\n",
      "Loss               : 0.66690993309021\n",
      "In-batch accuracy  : 0.75\n",
      "Validation accuracy: 0.4992\n",
      "\n",
      "Epoch 4. Global step 3520\n",
      "Loss               : 0.6846578121185303\n",
      "In-batch accuracy  : 0.75\n",
      "Validation accuracy: 0.5136\n",
      "\n",
      "Epoch 5. Global step 4224\n",
      "Loss               : 0.6985011696815491\n",
      "In-batch accuracy  : 0.5\n",
      "Validation accuracy: 0.5096\n",
      "\n",
      "Epoch 6. Global step 4928\n",
      "Loss               : 0.7467365860939026\n",
      "In-batch accuracy  : 0.5\n",
      "Validation accuracy: 0.5952\n",
      "\n",
      "Epoch 7. Global step 5632\n",
      "Loss               : 0.15804770588874817\n",
      "In-batch accuracy  : 1.0\n",
      "Validation accuracy: 0.724\n",
      "\n",
      "Epoch 8. Global step 6336\n",
      "Loss               : 0.48063862323760986\n",
      "In-batch accuracy  : 0.75\n",
      "Validation accuracy: 0.7664\n",
      "\n",
      "Epoch 9. Global step 7040\n",
      "Loss               : 0.0969933271408081\n",
      "In-batch accuracy  : 1.0\n",
      "Validation accuracy: 0.7976\n",
      "\n",
      "Epoch 10. Global step 7744\n",
      "Loss               : 0.32424110174179077\n",
      "In-batch accuracy  : 0.75\n",
      "Validation accuracy: 0.8244\n",
      "\n",
      "Epoch 11. Global step 8448\n",
      "Loss               : 0.18430441617965698\n",
      "In-batch accuracy  : 1.0\n",
      "Validation accuracy: 0.836\n",
      "\n",
      "Epoch 12. Global step 9152\n",
      "Loss               : 0.6712824106216431\n",
      "In-batch accuracy  : 0.75\n",
      "Validation accuracy: 0.8472\n",
      "\n",
      "Epoch 13. Global step 9856\n",
      "Loss               : 0.23235192894935608\n",
      "In-batch accuracy  : 1.0\n",
      "Validation accuracy: 0.8356\n",
      "\n",
      "Epoch 14. Global step 10560\n",
      "Loss               : 0.10705998539924622\n",
      "In-batch accuracy  : 1.0\n",
      "Validation accuracy: 0.8424\n",
      "\n",
      "Final test accuracy: 0.84596\n",
      "\n",
      "CPU times: user 45min 45s, sys: 3min 16s, total: 49min 2s\n",
      "Wall time: 40min 53s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = run_model_with(\n",
    "    noise_level=0.075, n_filters=256, cnn_kernel_size=5, hidden_dim_out=128, dropout=0.5,\n",
    "    lr=1e-3, epochs=15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Global step 704\n",
      "Loss               : 0.7757641673088074\n",
      "In-batch accuracy  : 0.0\n",
      "Validation accuracy: 0.4988\n",
      "\n",
      "Epoch 1. Global step 1408\n",
      "Loss               : 0.7179602384567261\n",
      "In-batch accuracy  : 0.75\n",
      "Validation accuracy: 0.4988\n",
      "\n",
      "Epoch 2. Global step 2112\n",
      "Loss               : 0.607974648475647\n",
      "In-batch accuracy  : 0.75\n",
      "Validation accuracy: 0.4992\n",
      "\n",
      "Epoch 3. Global step 2816\n",
      "Loss               : 0.6833053827285767\n",
      "In-batch accuracy  : 0.75\n",
      "Validation accuracy: 0.5032\n",
      "\n",
      "Epoch 4. Global step 3520\n",
      "Loss               : 0.8045235872268677\n",
      "In-batch accuracy  : 0.0\n",
      "Validation accuracy: 0.5056\n",
      "\n",
      "Epoch 5. Global step 4224\n",
      "Loss               : 0.704215943813324\n",
      "In-batch accuracy  : 0.25\n",
      "Validation accuracy: 0.5092\n",
      "\n",
      "Epoch 6. Global step 4928\n",
      "Loss               : 0.6857036352157593\n",
      "In-batch accuracy  : 0.75\n",
      "Validation accuracy: 0.5028\n",
      "\n",
      "Epoch 7. Global step 5632\n",
      "Loss               : 0.6611541509628296\n",
      "In-batch accuracy  : 1.0\n",
      "Validation accuracy: 0.518\n",
      "\n",
      "Epoch 8. Global step 6336\n",
      "Loss               : 0.7001538276672363\n",
      "In-batch accuracy  : 0.5\n",
      "Validation accuracy: 0.512\n",
      "\n",
      "Epoch 9. Global step 7040\n",
      "Loss               : 0.6159611344337463\n",
      "In-batch accuracy  : 0.75\n",
      "Validation accuracy: 0.6004\n",
      "\n",
      "Epoch 10. Global step 7744\n",
      "Loss               : 0.6819078922271729\n",
      "In-batch accuracy  : 0.5\n",
      "Validation accuracy: 0.564\n",
      "\n",
      "Epoch 11. Global step 8448\n",
      "Loss               : 0.2528180480003357\n",
      "In-batch accuracy  : 1.0\n",
      "Validation accuracy: 0.6764\n",
      "\n",
      "Epoch 12. Global step 9152\n",
      "Loss               : 0.3513117730617523\n",
      "In-batch accuracy  : 1.0\n",
      "Validation accuracy: 0.744\n",
      "\n",
      "Epoch 13. Global step 9856\n",
      "Loss               : 0.4030495584011078\n",
      "In-batch accuracy  : 1.0\n",
      "Validation accuracy: 0.7944\n",
      "\n",
      "Epoch 14. Global step 10560\n",
      "Loss               : 1.1296072006225586\n",
      "In-batch accuracy  : 0.25\n",
      "Validation accuracy: 0.816\n",
      "\n",
      "Final test accuracy: 0.8142\n",
      "\n",
      "CPU times: user 45min 45s, sys: 3min 15s, total: 49min 1s\n",
      "Wall time: 40min 52s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = run_model_with(\n",
    "    noise_level=0.1, n_filters=256, cnn_kernel_size=5, hidden_dim_out=128, dropout=0.5,\n",
    "    lr=1e-3, epochs=15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Global step 704\n",
      "Loss               : 0.7001333832740784\n",
      "In-batch accuracy  : 0.5\n",
      "Validation accuracy: 0.5012\n",
      "\n",
      "Epoch 1. Global step 1408\n",
      "Loss               : 0.6955465078353882\n",
      "In-batch accuracy  : 0.25\n",
      "Validation accuracy: 0.5028\n",
      "\n",
      "Epoch 2. Global step 2112\n",
      "Loss               : 0.685840368270874\n",
      "In-batch accuracy  : 0.25\n",
      "Validation accuracy: 0.5116\n",
      "\n",
      "Epoch 3. Global step 2816\n",
      "Loss               : 0.703079342842102\n",
      "In-batch accuracy  : 0.0\n",
      "Validation accuracy: 0.5072\n",
      "\n",
      "Epoch 4. Global step 3520\n",
      "Loss               : 0.6830451488494873\n",
      "In-batch accuracy  : 0.5\n",
      "Validation accuracy: 0.5048\n",
      "\n",
      "Epoch 5. Global step 4224\n",
      "Loss               : 0.7280513048171997\n",
      "In-batch accuracy  : 0.25\n",
      "Validation accuracy: 0.5108\n",
      "\n",
      "Epoch 6. Global step 4928\n",
      "Loss               : 0.7047296166419983\n",
      "In-batch accuracy  : 0.25\n",
      "Validation accuracy: 0.5004\n",
      "\n",
      "Epoch 7. Global step 5632\n",
      "Loss               : 0.6933375000953674\n",
      "In-batch accuracy  : 0.5\n",
      "Validation accuracy: 0.5156\n",
      "\n",
      "Epoch 8. Global step 6336\n",
      "Loss               : 0.6982243061065674\n",
      "In-batch accuracy  : 0.5\n",
      "Validation accuracy: 0.6076\n",
      "\n",
      "Epoch 9. Global step 7040\n",
      "Loss               : 0.5652779340744019\n",
      "In-batch accuracy  : 0.5\n",
      "Validation accuracy: 0.6776\n",
      "\n",
      "Epoch 10. Global step 7744\n",
      "Loss               : 0.2691684365272522\n",
      "In-batch accuracy  : 0.75\n",
      "Validation accuracy: 0.7484\n",
      "\n",
      "Epoch 11. Global step 8448\n",
      "Loss               : 0.29218125343322754\n",
      "In-batch accuracy  : 0.75\n",
      "Validation accuracy: 0.7736\n",
      "\n",
      "Epoch 12. Global step 9152\n",
      "Loss               : 0.11319838464260101\n",
      "In-batch accuracy  : 1.0\n",
      "Validation accuracy: 0.8012\n",
      "\n",
      "Epoch 13. Global step 9856\n",
      "Loss               : 0.6226658821105957\n",
      "In-batch accuracy  : 0.5\n",
      "Validation accuracy: 0.7864\n",
      "\n",
      "Epoch 14. Global step 10560\n",
      "Loss               : 0.4601198732852936\n",
      "In-batch accuracy  : 0.75\n",
      "Validation accuracy: 0.8096\n",
      "\n",
      "Final test accuracy: 0.81624\n",
      "\n",
      "CPU times: user 45min 43s, sys: 3min 18s, total: 49min 1s\n",
      "Wall time: 40min 51s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = run_model_with(\n",
    "    noise_level=0.125, n_filters=256, cnn_kernel_size=5, hidden_dim_out=128, dropout=0.5,\n",
    "    lr=1e-3, epochs=15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Global step 704\n",
      "Loss               : 0.7144126892089844\n",
      "In-batch accuracy  : 0.5\n",
      "Validation accuracy: 0.4996\n",
      "\n",
      "Epoch 1. Global step 1408\n",
      "Loss               : 0.700042724609375\n",
      "In-batch accuracy  : 0.25\n",
      "Validation accuracy: 0.5068\n",
      "\n",
      "Epoch 2. Global step 2112\n",
      "Loss               : 0.6732940077781677\n",
      "In-batch accuracy  : 1.0\n",
      "Validation accuracy: 0.5036\n",
      "\n",
      "Epoch 3. Global step 2816\n",
      "Loss               : 0.7170482873916626\n",
      "In-batch accuracy  : 0.5\n",
      "Validation accuracy: 0.5016\n",
      "\n",
      "Epoch 4. Global step 3520\n",
      "Loss               : 0.6934946775436401\n",
      "In-batch accuracy  : 0.5\n",
      "Validation accuracy: 0.5076\n",
      "\n",
      "Epoch 5. Global step 4224\n",
      "Loss               : 0.6997778415679932\n",
      "In-batch accuracy  : 0.25\n",
      "Validation accuracy: 0.5008\n",
      "\n",
      "Epoch 6. Global step 4928\n",
      "Loss               : 0.7062572240829468\n",
      "In-batch accuracy  : 0.5\n",
      "Validation accuracy: 0.5092\n",
      "\n",
      "Epoch 7. Global step 5632\n",
      "Loss               : 0.6809993982315063\n",
      "In-batch accuracy  : 0.75\n",
      "Validation accuracy: 0.5088\n",
      "\n",
      "Epoch 8. Global step 6336\n",
      "Loss               : 0.6934332847595215\n",
      "In-batch accuracy  : 0.5\n",
      "Validation accuracy: 0.5112\n",
      "\n",
      "Epoch 9. Global step 7040\n",
      "Loss               : 0.6931496858596802\n",
      "In-batch accuracy  : 0.5\n",
      "Validation accuracy: 0.506\n",
      "\n",
      "Epoch 10. Global step 7744\n",
      "Loss               : 0.72096848487854\n",
      "In-batch accuracy  : 0.25\n",
      "Validation accuracy: 0.5276\n",
      "\n",
      "Epoch 11. Global step 8448\n",
      "Loss               : 0.7939106225967407\n",
      "In-batch accuracy  : 0.5\n",
      "Validation accuracy: 0.6408\n",
      "\n",
      "Epoch 12. Global step 9152\n",
      "Loss               : 0.5294139385223389\n",
      "In-batch accuracy  : 0.75\n",
      "Validation accuracy: 0.6448\n",
      "\n",
      "Epoch 13. Global step 9856\n",
      "Loss               : 0.876846432685852\n",
      "In-batch accuracy  : 0.25\n",
      "Validation accuracy: 0.6332\n",
      "\n",
      "Epoch 14. Global step 10560\n",
      "Loss               : 0.16553254425525665\n",
      "In-batch accuracy  : 1.0\n",
      "Validation accuracy: 0.78\n",
      "\n",
      "Final test accuracy: 0.78156\n",
      "\n",
      "CPU times: user 45min 46s, sys: 3min 17s, total: 49min 3s\n",
      "Wall time: 40min 52s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = run_model_with(\n",
    "    noise_level=0.15, n_filters=256, cnn_kernel_size=5, hidden_dim_out=128, dropout=0.5,\n",
    "    lr=1e-3, epochs=15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Global step 704\n",
      "Loss               : 0.696272075176239\n",
      "In-batch accuracy  : 0.25\n",
      "Validation accuracy: 0.5008\n",
      "\n",
      "Epoch 1. Global step 1408\n",
      "Loss               : 0.6981146335601807\n",
      "In-batch accuracy  : 0.25\n",
      "Validation accuracy: 0.4956\n",
      "\n",
      "Epoch 2. Global step 2112\n",
      "Loss               : 0.6997109651565552\n",
      "In-batch accuracy  : 0.25\n",
      "Validation accuracy: 0.5024\n",
      "\n",
      "Epoch 3. Global step 2816\n",
      "Loss               : 0.7123950719833374\n",
      "In-batch accuracy  : 0.25\n",
      "Validation accuracy: 0.5064\n",
      "\n",
      "Epoch 4. Global step 3520\n",
      "Loss               : 0.6806596517562866\n",
      "In-batch accuracy  : 0.75\n",
      "Validation accuracy: 0.5088\n",
      "\n",
      "Epoch 6. Global step 4928\n",
      "Loss               : 0.7148482799530029\n",
      "In-batch accuracy  : 0.5\n",
      "Validation accuracy: 0.5184\n",
      "\n",
      "Epoch 7. Global step 5632\n",
      "Loss               : 0.6834253072738647\n",
      "In-batch accuracy  : 0.75\n",
      "Validation accuracy: 0.5136\n",
      "\n",
      "Epoch 8. Global step 6336\n",
      "Loss               : 0.675246000289917\n",
      "In-batch accuracy  : 0.5\n",
      "Validation accuracy: 0.5232\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = run_model_with(\n",
    "    noise_level=0.175, n_filters=256, cnn_kernel_size=5, hidden_dim_out=128, dropout=0.5,\n",
    "    lr=1e-3, epochs=15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model = run_model_with(\n",
    "    noise_level=0.2, n_filters=256, cnn_kernel_size=5, hidden_dim_out=128, dropout=0.5,\n",
    "    lr=1e-3, epochs=15\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Не все модели обучились. Нужно поиграться с инициализацией"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Global step 704\n",
      "Loss               : 0.6851248741149902\n",
      "In-batch accuracy  : 0.5\n",
      "Validation accuracy: 0.506\n",
      "\n",
      "Epoch 1. Global step 1408\n",
      "Loss               : 0.6951147317886353\n",
      "In-batch accuracy  : 0.0\n",
      "Validation accuracy: 0.5024\n",
      "\n",
      "Epoch 2. Global step 2112\n",
      "Loss               : 0.698655366897583\n",
      "In-batch accuracy  : 0.25\n",
      "Validation accuracy: 0.5072\n",
      "\n",
      "Epoch 3. Global step 2816\n",
      "Loss               : 0.7049776315689087\n",
      "In-batch accuracy  : 0.5\n",
      "Validation accuracy: 0.6496\n",
      "\n",
      "Epoch 4. Global step 3520\n",
      "Loss               : 0.3944334387779236\n",
      "In-batch accuracy  : 0.75\n",
      "Validation accuracy: 0.804\n",
      "\n",
      "Epoch 5. Global step 4224\n",
      "Loss               : 0.23799654841423035\n",
      "In-batch accuracy  : 1.0\n",
      "Validation accuracy: 0.7676\n",
      "\n",
      "Epoch 6. Global step 4928\n",
      "Loss               : 0.11648879945278168\n",
      "In-batch accuracy  : 1.0\n",
      "Validation accuracy: 0.8436\n",
      "\n",
      "Epoch 7. Global step 5632\n",
      "Loss               : 0.2652057707309723\n",
      "In-batch accuracy  : 0.75\n",
      "Validation accuracy: 0.8436\n",
      "\n",
      "Epoch 8. Global step 6336\n",
      "Loss               : 0.7204177975654602\n",
      "In-batch accuracy  : 0.5\n",
      "Validation accuracy: 0.8616\n",
      "\n",
      "Epoch 9. Global step 7040\n",
      "Loss               : 1.4096298217773438\n",
      "In-batch accuracy  : 0.75\n",
      "Validation accuracy: 0.7944\n",
      "\n",
      "Epoch 10. Global step 7744\n",
      "Loss               : 0.15360575914382935\n",
      "In-batch accuracy  : 1.0\n",
      "Validation accuracy: 0.8524\n",
      "\n",
      "Epoch 11. Global step 8448\n",
      "Loss               : 0.08487223088741302\n",
      "In-batch accuracy  : 1.0\n",
      "Validation accuracy: 0.8556\n",
      "\n",
      "Epoch 12. Global step 9152\n",
      "Loss               : 0.34465551376342773\n",
      "In-batch accuracy  : 0.75\n",
      "Validation accuracy: 0.848\n",
      "\n",
      "Epoch 13. Global step 9856\n",
      "Loss               : 0.9962451457977295\n",
      "In-batch accuracy  : 0.75\n",
      "Validation accuracy: 0.8612\n",
      "\n",
      "Epoch 14. Global step 10560\n",
      "Loss               : 0.019312232732772827\n",
      "In-batch accuracy  : 1.0\n",
      "Validation accuracy: 0.8548\n",
      "\n",
      "Final test accuracy: 0.85504\n",
      "\n",
      "CPU times: user 45min 48s, sys: 3min 15s, total: 49min 4s\n",
      "Wall time: 40min 58s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = run_model_with(\n",
    "    noise_level=0, n_filters=256, cnn_kernel_size=5, hidden_dim_out=128, dropout=0.5,\n",
    "    lr=1e-3, init_function=init.xavier_normal, epochs=15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Global step 704\n",
      "Loss               : 0.7157818675041199\n",
      "In-batch accuracy  : 0.25\n",
      "Validation accuracy: 0.5\n",
      "\n",
      "Epoch 1. Global step 1408\n",
      "Loss               : 0.6972311735153198\n",
      "In-batch accuracy  : 0.5\n",
      "Validation accuracy: 0.4976\n",
      "\n",
      "Epoch 2. Global step 2112\n",
      "Loss               : 0.6931629180908203\n",
      "In-batch accuracy  : 0.5\n",
      "Validation accuracy: 0.5\n",
      "\n",
      "Epoch 3. Global step 2816\n",
      "Loss               : 0.6936551332473755\n",
      "In-batch accuracy  : 0.5\n",
      "Validation accuracy: 0.5124\n",
      "\n",
      "Epoch 4. Global step 3520\n",
      "Loss               : 0.7411141395568848\n",
      "In-batch accuracy  : 0.5\n",
      "Validation accuracy: 0.508\n",
      "\n",
      "Epoch 5. Global step 4224\n",
      "Loss               : 0.7026736736297607\n",
      "In-batch accuracy  : 0.5\n",
      "Validation accuracy: 0.5876\n",
      "\n",
      "Epoch 6. Global step 4928\n",
      "Loss               : 0.8475057482719421\n",
      "In-batch accuracy  : 0.25\n",
      "Validation accuracy: 0.6724\n",
      "\n",
      "Epoch 7. Global step 5632\n",
      "Loss               : 0.41776585578918457\n",
      "In-batch accuracy  : 0.75\n",
      "Validation accuracy: 0.762\n",
      "\n",
      "Epoch 8. Global step 6336\n",
      "Loss               : 0.9890729188919067\n",
      "In-batch accuracy  : 0.5\n",
      "Validation accuracy: 0.8032\n",
      "\n",
      "Epoch 9. Global step 7040\n",
      "Loss               : 0.4945303201675415\n",
      "In-batch accuracy  : 0.75\n",
      "Validation accuracy: 0.824\n",
      "\n",
      "Epoch 10. Global step 7744\n",
      "Loss               : 0.029467642307281494\n",
      "In-batch accuracy  : 1.0\n",
      "Validation accuracy: 0.7956\n",
      "\n",
      "Epoch 11. Global step 8448\n",
      "Loss               : 0.1441923975944519\n",
      "In-batch accuracy  : 1.0\n",
      "Validation accuracy: 0.822\n",
      "\n",
      "Epoch 12. Global step 9152\n",
      "Loss               : 0.43973618745803833\n",
      "In-batch accuracy  : 0.75\n",
      "Validation accuracy: 0.8372\n",
      "\n",
      "Epoch 13. Global step 9856\n",
      "Loss               : 1.1574376821517944\n",
      "In-batch accuracy  : 0.5\n",
      "Validation accuracy: 0.8556\n",
      "\n",
      "Epoch 14. Global step 10560\n",
      "Loss               : 0.7637718915939331\n",
      "In-batch accuracy  : 0.5\n",
      "Validation accuracy: 0.8344\n",
      "\n",
      "Final test accuracy: 0.83824\n",
      "\n",
      "CPU times: user 45min 56s, sys: 3min 17s, total: 49min 14s\n",
      "Wall time: 41min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = run_model_with(\n",
    "    noise_level=0.05, n_filters=256, cnn_kernel_size=5, hidden_dim_out=128, dropout=0.5,\n",
    "    lr=1e-3, init_function=init.xavier_normal, epochs=15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Global step 704\n",
      "Loss               : 0.6887755990028381\n",
      "In-batch accuracy  : 0.75\n",
      "Validation accuracy: 0.4988\n",
      "\n",
      "Epoch 1. Global step 1408\n",
      "Loss               : 0.6898736953735352\n",
      "In-batch accuracy  : 0.75\n",
      "Validation accuracy: 0.4896\n",
      "\n",
      "Epoch 2. Global step 2112\n",
      "Loss               : 0.6933225393295288\n",
      "In-batch accuracy  : 0.5\n",
      "Validation accuracy: 0.4924\n",
      "\n",
      "Epoch 3. Global step 2816\n",
      "Loss               : 0.7162558436393738\n",
      "In-batch accuracy  : 0.5\n",
      "Validation accuracy: 0.488\n",
      "\n",
      "Epoch 4. Global step 3520\n",
      "Loss               : 0.7071967720985413\n",
      "In-batch accuracy  : 0.25\n",
      "Validation accuracy: 0.496\n",
      "\n",
      "Epoch 5. Global step 4224\n",
      "Loss               : 0.6842800974845886\n",
      "In-batch accuracy  : 0.25\n",
      "Validation accuracy: 0.4992\n",
      "\n",
      "Epoch 6. Global step 4928\n",
      "Loss               : 0.742426335811615\n",
      "In-batch accuracy  : 0.5\n",
      "Validation accuracy: 0.5368\n",
      "\n",
      "Epoch 7. Global step 5632\n",
      "Loss               : 0.2838674485683441\n",
      "In-batch accuracy  : 0.75\n",
      "Validation accuracy: 0.736\n",
      "\n",
      "Epoch 8. Global step 6336\n",
      "Loss               : 0.21290379762649536\n",
      "In-batch accuracy  : 1.0\n",
      "Validation accuracy: 0.7748\n",
      "\n",
      "Epoch 9. Global step 7040\n",
      "Loss               : 0.1046539694070816\n",
      "In-batch accuracy  : 1.0\n",
      "Validation accuracy: 0.8088\n",
      "\n",
      "Epoch 10. Global step 7744\n",
      "Loss               : 0.1759295016527176\n",
      "In-batch accuracy  : 1.0\n",
      "Validation accuracy: 0.8048\n",
      "\n",
      "Epoch 11. Global step 8448\n",
      "Loss               : 0.9020839333534241\n",
      "In-batch accuracy  : 0.5\n",
      "Validation accuracy: 0.8124\n",
      "\n",
      "Epoch 12. Global step 9152\n",
      "Loss               : 0.3301304280757904\n",
      "In-batch accuracy  : 1.0\n",
      "Validation accuracy: 0.8172\n",
      "\n",
      "Epoch 13. Global step 9856\n",
      "Loss               : 0.07421059906482697\n",
      "In-batch accuracy  : 1.0\n",
      "Validation accuracy: 0.8\n",
      "\n",
      "Epoch 14. Global step 10560\n",
      "Loss               : 0.8647317886352539\n",
      "In-batch accuracy  : 0.75\n",
      "Validation accuracy: 0.8096\n",
      "\n",
      "Final test accuracy: 0.81452\n",
      "\n",
      "CPU times: user 45min 44s, sys: 3min 18s, total: 49min 2s\n",
      "Wall time: 40min 53s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = run_model_with(\n",
    "    noise_level=0.1, n_filters=256, cnn_kernel_size=5, hidden_dim_out=128, dropout=0.5,\n",
    "    lr=1e-3, init_function=init.xavier_normal, epochs=15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Global step 704\n",
      "Loss               : 0.6716256141662598\n",
      "In-batch accuracy  : 0.75\n",
      "Validation accuracy: 0.5012\n",
      "\n",
      "Epoch 1. Global step 1408\n",
      "Loss               : 0.6804771423339844\n",
      "In-batch accuracy  : 0.75\n",
      "Validation accuracy: 0.4972\n",
      "\n",
      "Epoch 2. Global step 2112\n",
      "Loss               : 0.6956794261932373\n",
      "In-batch accuracy  : 0.25\n",
      "Validation accuracy: 0.4924\n",
      "\n",
      "Epoch 3. Global step 2816\n",
      "Loss               : 0.6901111602783203\n",
      "In-batch accuracy  : 0.75\n",
      "Validation accuracy: 0.4924\n",
      "\n",
      "Epoch 4. Global step 3520\n",
      "Loss               : 0.6718950271606445\n",
      "In-batch accuracy  : 0.25\n",
      "Validation accuracy: 0.4924\n",
      "\n",
      "Epoch 5. Global step 4224\n",
      "Loss               : 0.6880216002464294\n",
      "In-batch accuracy  : 0.5\n",
      "Validation accuracy: 0.494\n",
      "\n",
      "Epoch 6. Global step 4928\n",
      "Loss               : 0.6308936476707458\n",
      "In-batch accuracy  : 1.0\n",
      "Validation accuracy: 0.4968\n",
      "\n",
      "Epoch 7. Global step 5632\n",
      "Loss               : 0.6977838277816772\n",
      "In-batch accuracy  : 0.5\n",
      "Validation accuracy: 0.504\n",
      "\n",
      "Epoch 8. Global step 6336\n",
      "Loss               : 0.729473888874054\n",
      "In-batch accuracy  : 0.5\n",
      "Validation accuracy: 0.6216\n",
      "\n",
      "Epoch 9. Global step 7040\n",
      "Loss               : 0.8312719464302063\n",
      "In-batch accuracy  : 0.5\n",
      "Validation accuracy: 0.6916\n",
      "\n",
      "Epoch 10. Global step 7744\n",
      "Loss               : 0.5746855735778809\n",
      "In-batch accuracy  : 0.75\n",
      "Validation accuracy: 0.7336\n",
      "\n",
      "Epoch 11. Global step 8448\n",
      "Loss               : 0.18357697129249573\n",
      "In-batch accuracy  : 1.0\n",
      "Validation accuracy: 0.7692\n",
      "\n",
      "Epoch 12. Global step 9152\n",
      "Loss               : 0.20997042953968048\n",
      "In-batch accuracy  : 1.0\n",
      "Validation accuracy: 0.76\n",
      "\n",
      "Epoch 13. Global step 9856\n",
      "Loss               : 0.11488768458366394\n",
      "In-batch accuracy  : 1.0\n",
      "Validation accuracy: 0.798\n",
      "\n",
      "Epoch 14. Global step 10560\n",
      "Loss               : 0.0934133231639862\n",
      "In-batch accuracy  : 1.0\n",
      "Validation accuracy: 0.7976\n",
      "\n",
      "Epoch 15. Global step 11264\n",
      "Loss               : 0.35225537419319153\n",
      "In-batch accuracy  : 1.0\n",
      "Validation accuracy: 0.8092\n",
      "\n",
      "Epoch 16. Global step 11968\n",
      "Loss               : 0.19677215814590454\n",
      "In-batch accuracy  : 1.0\n",
      "Validation accuracy: 0.8144\n",
      "\n",
      "Epoch 17. Global step 12672\n",
      "Loss               : 0.6499910950660706\n",
      "In-batch accuracy  : 0.75\n",
      "Validation accuracy: 0.8136\n",
      "\n",
      "Epoch 18. Global step 13376\n",
      "Loss               : 0.2291809767484665\n",
      "In-batch accuracy  : 0.75\n",
      "Validation accuracy: 0.8072\n",
      "\n",
      "Epoch 19. Global step 14080\n",
      "Loss               : 0.6312558054924011\n",
      "In-batch accuracy  : 0.75\n",
      "Validation accuracy: 0.82\n",
      "\n",
      "Final test accuracy: 0.82192\n",
      "\n",
      "CPU times: user 1h 4s, sys: 4min 23s, total: 1h 4min 27s\n",
      "Wall time: 54min 10s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = run_model_with(\n",
    "    noise_level=0.15, n_filters=256, cnn_kernel_size=5, hidden_dim_out=128, dropout=0.5,\n",
    "    lr=1e-3, init_function=init.xavier_normal, epochs=20\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Нужное ненужное"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'YoonKimModel(\\n  (embedding): Linear(in_features=74, out_features=74, bias=True)\\n  (chars_cnn): Sequential(\\n    (0): Conv1d(74, 256, kernel_size=(5,), stride=(1,), padding=(2,))\\n    (1): ReLU()\\n    (2): MaxPool1d(kernel_size=16, stride=16, padding=0, dilation=1, ceil_mode=False)\\n  )\\n  (words_rnn): GRU(256, 128, dropout=0.5)\\n  (projector): Linear(in_features=128, out_features=2, bias=True)\\n)'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_n_parameters(model):\n",
    "    return sum(np.prod(list(p.size())) for p in model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "can't use starred expression here (<ipython-input-29-dfa9e60b2f15>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-29-dfa9e60b2f15>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    *p.size()\u001b[0m\n\u001b[0m             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m can't use starred expression here\n"
     ]
    }
   ],
   "source": [
    "import operator\n",
    "import functools\n",
    "functools.reduce(operator.mul, [1,2,3,4,5,6], 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in dataloader:\n",
    "    item = i\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YoonKimModel(n_filters=256, cnn_kernel_size=5, hidden_dim_out=128, dropout=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "YoonKimModel(\n",
       "  (embedding): Linear(in_features=74, out_features=74, bias=True)\n",
       "  (chars_cnn): Sequential(\n",
       "    (0): Conv1d(74, 256, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool1d(kernel_size=16, stride=16, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (words_rnn): GRU(256, 128, dropout=0.5)\n",
       "  (projector): Linear(in_features=128, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = Variable(item[0].cuda()).permute(1, 0, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = Variable(torch.LongTensor(item[1])).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 0.6893\n",
       "[torch.cuda.FloatTensor of size 1 (GPU 0)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.cross_entropy(model(text), label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "(0 ,.,.) = \n",
       "   0   0   0  ...    0   0   0\n",
       "   0   0   0  ...    0   0   0\n",
       "   0   0   0  ...    0   0   0\n",
       "     ...       ⋱       ...    \n",
       "   0   0   0  ...    0   0   0\n",
       "   0   0   0  ...    0   0   0\n",
       "   0   0   0  ...    0   0   0\n",
       "\n",
       "(1 ,.,.) = \n",
       "   0   0   0  ...    0   0   0\n",
       "   0   0   0  ...    0   0   0\n",
       "   0   0   0  ...    0   0   0\n",
       "     ...       ⋱       ...    \n",
       "   0   0   0  ...    0   0   0\n",
       "   0   0   0  ...    0   0   0\n",
       "   0   0   0  ...    0   0   0\n",
       "\n",
       "(2 ,.,.) = \n",
       "   0   0   0  ...    0   0   0\n",
       "   0   0   0  ...    0   0   0\n",
       "   0   0   0  ...    0   0   0\n",
       "     ...       ⋱       ...    \n",
       "   0   0   0  ...    0   0   0\n",
       "   0   0   0  ...    0   0   0\n",
       "   0   0   0  ...    0   0   0\n",
       "...\n",
       "\n",
       "(29,.,.) = \n",
       "   0   0   0  ...    0   0   0\n",
       "   0   0   0  ...    0   0   0\n",
       "   0   0   0  ...    0   0   0\n",
       "     ...       ⋱       ...    \n",
       "   0   0   0  ...    0   0   0\n",
       "   0   0   0  ...    0   0   0\n",
       "   0   0   0  ...    0   0   0\n",
       "\n",
       "(30,.,.) = \n",
       "   0   0   0  ...    0   0   0\n",
       "   0   0   0  ...    0   0   0\n",
       "   0   0   0  ...    0   0   0\n",
       "     ...       ⋱       ...    \n",
       "   0   0   0  ...    0   0   0\n",
       "   0   0   0  ...    0   0   0\n",
       "   0   0   0  ...    0   0   0\n",
       "\n",
       "(31,.,.) = \n",
       "   0   0   0  ...    0   0   0\n",
       "   0   0   0  ...    0   0   0\n",
       "   0   0   0  ...    0   0   0\n",
       "     ...       ⋱       ...    \n",
       "   0   0   0  ...    0   0   0\n",
       "   0   0   0  ...    0   0   0\n",
       "   0   0   0  ...    0   0   0\n",
       "[torch.FloatTensor of size 32x74x16]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Variable(item[0][0:MAX_WORD_LEN,:]).permute(0, 2, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "(0 ,.,.) = \n",
       "   0   0   0  ...    0   0   0\n",
       "   0   0   0  ...    0   0   0\n",
       "   0   0   0  ...    0   0   0\n",
       "     ...       ⋱       ...    \n",
       "   0   0   0  ...    0   0   0\n",
       "   0   0   0  ...    0   0   0\n",
       "   0   0   0  ...    0   0   0\n",
       "\n",
       "(1 ,.,.) = \n",
       "   0   0   0  ...    0   0   0\n",
       "   0   0   0  ...    0   0   0\n",
       "   0   0   0  ...    0   0   0\n",
       "     ...       ⋱       ...    \n",
       "   0   0   0  ...    0   0   0\n",
       "   0   0   0  ...    0   0   0\n",
       "   0   0   0  ...    0   0   0\n",
       "\n",
       "(2 ,.,.) = \n",
       "   0   0   0  ...    0   0   0\n",
       "   0   0   0  ...    0   0   0\n",
       "   0   0   0  ...    0   0   0\n",
       "     ...       ⋱       ...    \n",
       "   0   0   0  ...    0   0   0\n",
       "   0   0   0  ...    0   0   0\n",
       "   0   0   0  ...    0   0   0\n",
       "...\n",
       "\n",
       "(29,.,.) = \n",
       "   0   0   0  ...    0   0   0\n",
       "   0   0   0  ...    0   0   0\n",
       "   0   0   0  ...    0   0   0\n",
       "     ...       ⋱       ...    \n",
       "   0   0   0  ...    0   0   0\n",
       "   0   0   0  ...    0   0   0\n",
       "   0   0   0  ...    0   0   0\n",
       "\n",
       "(30,.,.) = \n",
       "   0   0   0  ...    0   0   0\n",
       "   0   0   0  ...    0   0   0\n",
       "   0   0   0  ...    0   0   0\n",
       "     ...       ⋱       ...    \n",
       "   0   0   0  ...    0   0   0\n",
       "   0   0   0  ...    0   0   0\n",
       "   0   0   0  ...    0   0   0\n",
       "\n",
       "(31,.,.) = \n",
       "   0   0   0  ...    0   0   0\n",
       "   0   0   0  ...    0   0   0\n",
       "   0   0   0  ...    0   0   0\n",
       "     ...       ⋱       ...    \n",
       "   0   0   0  ...    0   0   0\n",
       "   0   0   0  ...    0   0   0\n",
       "   0   0   0  ...    0   0   0\n",
       "[torch.FloatTensor of size 32x74x16]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item[0].permute(1, 0, 2)[0:MAX_WORD_LEN,:].permute(1, 2, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "(0 ,.,.) = \n",
       "  0.1022 -0.0039 -0.0890  ...   0.0393  0.1056 -0.0511\n",
       "  0.0761 -0.0932  0.0260  ...  -0.0641  0.1511 -0.0113\n",
       "  0.0657 -0.0369  0.0588  ...   0.0276  0.1004  0.1344\n",
       "           ...             ⋱             ...          \n",
       "  0.1275 -0.0825  0.1114  ...   0.0630  0.0929  0.0624\n",
       " -0.0074 -0.0157 -0.0175  ...  -0.0782  0.0081  0.0615\n",
       " -0.0074 -0.0157 -0.0175  ...  -0.0782  0.0081  0.0615\n",
       "\n",
       "(1 ,.,.) = \n",
       "  0.0255 -0.0172  0.1354  ...  -0.0489 -0.0233  0.0601\n",
       "  0.1022 -0.0039 -0.0890  ...   0.0393  0.1056 -0.0511\n",
       "  0.0827 -0.0978  0.1132  ...  -0.0385  0.0374 -0.0501\n",
       "           ...             ⋱             ...          \n",
       "  0.0824 -0.0364  0.0261  ...  -0.0096  0.0528  0.0438\n",
       "  0.0761 -0.0932  0.0260  ...  -0.0641  0.1511 -0.0113\n",
       "  0.0761 -0.0932  0.0260  ...  -0.0641  0.1511 -0.0113\n",
       "\n",
       "(2 ,.,.) = \n",
       "  0.0824 -0.0364  0.0261  ...  -0.0096  0.0528  0.0438\n",
       "  0.0857  0.0450 -0.0070  ...  -0.0097 -0.0208  0.1098\n",
       "  0.1022 -0.0039 -0.0890  ...   0.0393  0.1056 -0.0511\n",
       "           ...             ⋱             ...          \n",
       "  0.0824 -0.0364  0.0261  ...  -0.0096  0.0528  0.0438\n",
       "  0.1275 -0.0825  0.1114  ...   0.0630  0.0929  0.0624\n",
       "  0.1275 -0.0825  0.1114  ...   0.0630  0.0929  0.0624\n",
       "...\n",
       "\n",
       "(13,.,.) = \n",
       "  0.0824 -0.0364  0.0261  ...  -0.0096  0.0528  0.0438\n",
       "  0.0824 -0.0364  0.0261  ...  -0.0096  0.0528  0.0438\n",
       "  0.0824 -0.0364  0.0261  ...  -0.0096  0.0528  0.0438\n",
       "           ...             ⋱             ...          \n",
       "  0.0824 -0.0364  0.0261  ...  -0.0096  0.0528  0.0438\n",
       "  0.0824 -0.0364  0.0261  ...  -0.0096  0.0528  0.0438\n",
       "  0.0824 -0.0364  0.0261  ...  -0.0096  0.0528  0.0438\n",
       "\n",
       "(14,.,.) = \n",
       "  0.0824 -0.0364  0.0261  ...  -0.0096  0.0528  0.0438\n",
       "  0.0824 -0.0364  0.0261  ...  -0.0096  0.0528  0.0438\n",
       "  0.0824 -0.0364  0.0261  ...  -0.0096  0.0528  0.0438\n",
       "           ...             ⋱             ...          \n",
       "  0.0824 -0.0364  0.0261  ...  -0.0096  0.0528  0.0438\n",
       "  0.0824 -0.0364  0.0261  ...  -0.0096  0.0528  0.0438\n",
       "  0.0824 -0.0364  0.0261  ...  -0.0096  0.0528  0.0438\n",
       "\n",
       "(15,.,.) = \n",
       "  0.0824 -0.0364  0.0261  ...  -0.0096  0.0528  0.0438\n",
       "  0.0824 -0.0364  0.0261  ...  -0.0096  0.0528  0.0438\n",
       "  0.0824 -0.0364  0.0261  ...  -0.0096  0.0528  0.0438\n",
       "           ...             ⋱             ...          \n",
       "  0.0824 -0.0364  0.0261  ...  -0.0096  0.0528  0.0438\n",
       "  0.0824 -0.0364  0.0261  ...  -0.0096  0.0528  0.0438\n",
       "  0.0824 -0.0364  0.0261  ...  -0.0096  0.0528  0.0438\n",
       "[torch.FloatTensor of size 16x32x74]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.chars_cnn(model.embedding(Variable(item[0].permute(1, 0, 2)[0:MAX_WORD_LEN,:])).permute(1, 2 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "( 0 ,.,.) = \n",
       "  0.1181\n",
       "  0.0433\n",
       "  0.0000\n",
       "   ⋮    \n",
       "  0.0908\n",
       "  0.0478\n",
       "  0.1122\n",
       "\n",
       "( 1 ,.,.) = \n",
       "  0.1486\n",
       "  0.0545\n",
       "  0.0115\n",
       "   ⋮    \n",
       "  0.1511\n",
       "  0.1207\n",
       "  0.0366\n",
       "\n",
       "( 2 ,.,.) = \n",
       "  0.1427\n",
       "  0.0846\n",
       "  0.0000\n",
       "   ⋮    \n",
       "  0.1290\n",
       "  0.1473\n",
       "  0.1483\n",
       "... \n",
       "\n",
       "(29 ,.,.) = \n",
       "  0.0911\n",
       "  0.0000\n",
       "  0.0000\n",
       "   ⋮    \n",
       "  0.0569\n",
       "  0.0477\n",
       "  0.0612\n",
       "\n",
       "(30 ,.,.) = \n",
       "  0.0919\n",
       "  0.0242\n",
       "  0.0022\n",
       "   ⋮    \n",
       "  0.1367\n",
       "  0.0610\n",
       "  0.0957\n",
       "\n",
       "(31 ,.,.) = \n",
       "  0.0919\n",
       "  0.0242\n",
       "  0.0022\n",
       "   ⋮    \n",
       "  0.1367\n",
       "  0.0610\n",
       "  0.0957\n",
       "[torch.FloatTensor of size 32x256x1]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.chars_cnn(\n",
    "    Variable(item[0].permute(1, 0, 2)[0:MAX_WORD_LEN,:].permute(1, 2, 0))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-env",
   "language": "python",
   "name": "pytorch-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
