{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "from tqdm import tqdm as tqdm\n",
    "\n",
    "CUDA = torch.cuda.is_available()\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import torchtext\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = torchtext.data.Field(\n",
    "    lower=True, include_lengths=False, fix_length=2048, tensor_type=torch.FloatTensor, batch_first=True,\n",
    "    tokenize=lambda x: x, use_vocab=False, sequential=False\n",
    ")\n",
    "label = torchtext.data.Field(sequential=False, use_vocab=False)\n",
    "\n",
    "train, test = torchtext.datasets.IMDB.splits(text, label)\n",
    "\n",
    "c = Counter(''.join([' '.join(t.text) for t in train]))\n",
    "\n",
    "ALPHABET = [char[0] for char in c.most_common(62)]  # all other chars used less ~ 100 times in a test\n",
    "ALPHABET.append('UNK')\n",
    "ALPHABET.append('PAD')\n",
    "\n",
    "ALPHABET_LEN = len(ALPHABET)\n",
    "\n",
    "char2int = {s: i for s, i in zip(ALPHABET, range(ALPHABET_LEN))}\n",
    "\n",
    "MAXLEN = 128\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "TEST_SIZE = 100\n",
    "\n",
    "def one_hot(char):\n",
    "    zeros = np.zeros(ALPHABET_LEN)\n",
    "    if char in char2int:\n",
    "        zeros[char2int[char]] = 1.\n",
    "    else:\n",
    "        zeros[char2int['UNK']] = 1.\n",
    "\n",
    "def preprocess_text(text, maxlen=MAXLEN, batch_size=BATCH_SIZE):\n",
    "    one_hotted_text = np.zeros((batch_size, maxlen, ALPHABET_LEN))\n",
    "    assert len(text) == batch_size\n",
    "    for bi, batch in enumerate(text):\n",
    "        for i, char in enumerate(batch):\n",
    "            if i >= MAXLEN:\n",
    "                break\n",
    "            one_hotted_text[bi, i, char2int.get(char, char2int['UNK'])] = 1.\n",
    "        if i < MAXLEN:\n",
    "            for j in range(i+1, MAXLEN):\n",
    "                one_hotted_text[bi, j, char2int['PAD']] = 1.\n",
    "\n",
    "    return torch.FloatTensor(one_hotted_text)\n",
    "\n",
    "def onehot2text(one_hotted_text):\n",
    "    texts = []\n",
    "\n",
    "    for s in one_hotted_text:\n",
    "        text = ''\n",
    "        _, idx = torch.max(s, 1)\n",
    "        for i in idx:\n",
    "            symb = ALPHABET[i]\n",
    "            if symb == 'PAD':\n",
    "                break\n",
    "            else:\n",
    "                text += symb\n",
    "        texts.append(text)\n",
    "    return texts\n",
    "\n",
    "all_texts = [t.text for t in train]\n",
    "all_labels = [int(t.label == 'pos') for t in train]\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "X, y = shuffle(all_texts, all_labels)\n",
    "\n",
    "batch_idx = 0\n",
    "\n",
    "def next_batch():\n",
    "    # BATCH_SIZE(32), ALPHABET_LEN(128), MAXLEN(512)\n",
    "    global batch_idx\n",
    "    batch = X[batch_idx:batch_idx+BATCH_SIZE], y[batch_idx:batch_idx+BATCH_SIZE]\n",
    "    batch_idx += BATCH_SIZE\n",
    "    return batch\n",
    "\n",
    "def clip_gradient(optimizer, grad_clip):\n",
    "    for group in optimizer.param_groups:\n",
    "        for param in group['params']:\n",
    "            if param.grad is not None and param.requires_grad:\n",
    "                param.grad.data.clamp_(-grad_clip, grad_clip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden_dim=256, dropout=0.5, num_layers=1):\n",
    "        super(CharRNN, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.rnn = nn.GRU(ALPHABET_LEN, hidden_dim, num_layers=num_layers, dropout=dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.projector = nn.Linear(hidden_dim, 2)\n",
    "        \n",
    "        self.hidden = self.init_hidden()\n",
    "\n",
    "    def init_hidden(self, batch_size=None):\n",
    "        if batch_size is None:\n",
    "            batch_size = BATCH_SIZE\n",
    "        \n",
    "        # 1 is num_layers\n",
    "        if CUDA:\n",
    "            h0 = Variable(torch.randn([self.num_layers, batch_size, self.hidden_dim]).cuda())\n",
    "#             c0 = Variable(torch.randn([1, batch_size, self.hidden_dim]).cuda())\n",
    "        else:\n",
    "            h0 = Variable(torch.randn([self.num_layers, batch_size, self.hidden_dim]))\n",
    "#             c0 = Variable(torch.randn([1, batch_size, self.hidden_dim]))\n",
    "\n",
    "        return h0\n",
    "    \n",
    "    def forward(self, inp):\n",
    "        self.hidden = self.init_hidden(inp.size()[1])\n",
    "        rnn_out, rnn_hidden = self.rnn(inp, self.hidden)\n",
    "        rnn_out_last = self.dropout(rnn_out[-1])\n",
    "        out = self.projector(rnn_out_last)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CharRNN(\n",
       "  (rnn): GRU(64, 512, dropout=0.5)\n",
       "  (dropout): Dropout(p=0.5)\n",
       "  (projector): Linear(in_features=512, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = CharRNN(512, dropout=0.5, num_layers=1)\n",
    "if CUDA:\n",
    "    model.cuda()\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter(comment='_char_GRU512_optimizer5_maxlen128_batch32_dropout05_layers1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(params=model.parameters(), lr=10**-5)\n",
    "optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_step = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_texts = [t.text for t in test]\n",
    "test_labels = [int(t.label == 'pos') for t in test]\n",
    "\n",
    "test_texts, test_labels = shuffle(test_texts, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "12500"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(test_labels))\n",
    "sum(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(step, last_prediction, last_labels):\n",
    "\n",
    "    _, idx = torch.max(last_prediction, 1)\n",
    "    acc = accuracy_score(last_labels.data.tolist(), idx.data.tolist())\n",
    "    writer.add_scalar('accuracy_train', acc, global_step=global_step)\n",
    "    print('In-batch accuracy:', acc)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    predictions = []\n",
    "\n",
    "    test_texts, test_labels = shuffle(test_texts, test_labels)\n",
    "    \n",
    "    for t in test_texts[:TEST_SIZE]:\n",
    "\n",
    "        ptex = preprocess_text([t], batch_size=1)\n",
    "        ptex = Variable(ptex.cuda())\n",
    "        ptex = ptex.permute(1, 0, 2)\n",
    "        pred = model(ptex)\n",
    "        _, idx = torch.max(pred, 1)\n",
    "\n",
    "        predictions.append(idx.data[0])\n",
    "    \n",
    "    lables = test_labels[:TEST_SIZE]\n",
    "    \n",
    "    acc = accuracy_score(lables, predictions)\n",
    "    print('Test accuracy:', acc)\n",
    "    writer.add_scalar('accuracy_test', acc, global_step=global_step)\n",
    "\n",
    "    model.train()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size:  32\n",
      "Optimizer:  {'lr': 1e-05, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Exception in thread Thread-6:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/threading.py\", line 914, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/phobos_aijun/.virtualenvs/pytorch-env/lib/python3.5/site-packages/tqdm/_tqdm.py\", line 144, in run\n",
      "    for instance in self.tqdm_cls._instances:\n",
      "  File \"/home/phobos_aijun/.virtualenvs/pytorch-env/lib/python3.5/_weakrefset.py\", line 60, in __iter__\n",
      "    for itemref in self.data:\n",
      "RuntimeError: Set changed size during iteration\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 0:\n",
      "0.6952921152114868\n",
      "In-batch accuracy: 0.4375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|█         | 1/10 [00:12<01:55, 12.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.49\n",
      "Loss after epoch 1:\n",
      "0.6852248311042786\n",
      "In-batch accuracy: 0.65625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|██        | 2/10 [00:25<01:43, 12.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.57\n",
      "Loss after epoch 2:\n",
      "0.6973998546600342\n",
      "In-batch accuracy: 0.40625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|███       | 3/10 [00:38<01:30, 12.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.48\n",
      "Loss after epoch 3:\n",
      "0.6884451508522034\n",
      "In-batch accuracy: 0.53125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|████      | 4/10 [00:51<01:17, 12.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.52\n",
      "Loss after epoch 4:\n",
      "0.7078744173049927\n",
      "In-batch accuracy: 0.4375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████     | 5/10 [01:04<01:04, 12.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.5\n",
      "Loss after epoch 5:\n",
      "0.6940731406211853\n",
      "In-batch accuracy: 0.46875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████    | 6/10 [01:17<00:51, 12.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.56\n",
      "Loss after epoch 6:\n",
      "0.6920180320739746\n",
      "In-batch accuracy: 0.59375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 70%|███████   | 7/10 [01:29<00:38, 12.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.59\n",
      "Loss after epoch 7:\n",
      "0.7015429139137268\n",
      "In-batch accuracy: 0.40625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|████████  | 8/10 [01:42<00:25, 12.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.51\n",
      "Loss after epoch 8:\n",
      "0.6840828061103821\n",
      "In-batch accuracy: 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 90%|█████████ | 9/10 [01:55<00:12, 12.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.54\n",
      "Loss after epoch 9:\n",
      "0.6917307376861572\n",
      "In-batch accuracy: 0.4375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|██████████| 10/10 [02:08<00:00, 12.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 10\n",
    "\n",
    "loss_f = F.cross_entropy\n",
    "\n",
    "print('Batch size: ', BATCH_SIZE)\n",
    "print('Optimizer: ', optimizer.defaults)\n",
    "\n",
    "for epoch in tqdm(range(N_EPOCHS)):\n",
    "    global batch_idx\n",
    "    batch_idx = 0\n",
    "    X, y = shuffle(X, y)\n",
    "    while batch_idx < len(X) - BATCH_SIZE:\n",
    "        text, label = next_batch()\n",
    "\n",
    "        label = Variable(torch.LongTensor(label).cuda()) if CUDA else Variable(torch.LongTensor(label))\n",
    "\n",
    "        global_step += 1\n",
    "\n",
    "        one_hotted_text = preprocess_text(text)\n",
    "        one_hotted_text = Variable(one_hotted_text.cuda()) if CUDA else Variable(one_hotted_text)\n",
    "        one_hotted_text = one_hotted_text.permute(1, 0, 2)\n",
    "        prediction = model(one_hotted_text)\n",
    "\n",
    "        loss = loss_f(prediction, label)\n",
    "\n",
    "        writer.add_scalar('loss', loss.data[0], global_step=global_step)\n",
    "\n",
    "        loss.backward()        \n",
    "        torch.nn.utils.clip_grad_norm(model.parameters(), 1e-1)\n",
    "        optimizer.step()\n",
    "\n",
    "    # evaluation\n",
    "    print('Loss after epoch %s:' % epoch)\n",
    "    print(loss.data[0])\n",
    "        \n",
    "    _, idx = torch.max(prediction, 1)\n",
    "    acc = accuracy_score(label.data.tolist(), idx.data.tolist())\n",
    "    writer.add_scalar('accuracy_train', acc, global_step=global_step)\n",
    "    print('In-batch accuracy:', acc)\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    predictions = []\n",
    "\n",
    "    test_texts, test_labels = shuffle(test_texts, test_labels)\n",
    "    \n",
    "    for t in test_texts[:TEST_SIZE]:\n",
    "\n",
    "        ptex = preprocess_text([t], batch_size=1)\n",
    "        ptex = Variable(ptex.cuda())\n",
    "        ptex = ptex.permute(1, 0, 2)\n",
    "        pred = model(ptex)\n",
    "        _, idx = torch.max(pred, 1)\n",
    "\n",
    "        predictions.append(idx.data[0])\n",
    "    \n",
    "    lables = test_labels[:TEST_SIZE]\n",
    "    \n",
    "    acc = accuracy_score(lables, predictions)\n",
    "    print('Test accuracy:', acc)\n",
    "    writer.add_scalar('accuracy_test', acc, global_step=global_step)\n",
    "\n",
    "    model.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('I am a Sociologist/Anthropologist specializing in the field of Symbolic Interactionism, and I must say that this film exhibits high quality in the symbolic context throughout the entire film. To anyone who has not yet seen this, I recommend that you also read \"Man\\'s Search For Ultimate Meaning\" by Victor E. Frankl. I think you will be able to draw some amazing correlations.<br /><br />That being said, I would like to say that despite the fact that the main characters are gay, this is not a story about being gay. This is a story about seeking out and finding meaning in life, despite the difficulties and challenges, the pain and terror that stand in your way. This is a story of seeking and finding balance and wholeness and happiness.',\n",
       " 1)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_texts[1], test_labels[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:10<00:00, 93.30it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CharRNN(\n",
       "  (rnn): LSTM(128, 256)\n",
       "  (projector): Linear(in_features=256, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "predictions = []\n",
    "\n",
    "for t in tqdm(test_texts[:1000], total=1000):\n",
    "\n",
    "    ptex = preprocess_text([t], batch_size=1)\n",
    "    ptex = Variable(ptex.cuda())\n",
    "    ptex = ptex.permute(1, 0, 2)\n",
    "    pred = model(ptex)\n",
    "    _, idx = torch.max(pred, 1)\n",
    "\n",
    "    predictions.append(idx.data[0])\n",
    "\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "lables = test_labels[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.511"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(lables, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "ptex = preprocess_text([test_texts[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512, 32, 128])"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ptex.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = test_texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'out of any category, this is one demented and over the edge film, even in todays standards. filmed entirely in crap-o-rama, this film will blow your mind (and something else too!)<br /><br />the amount of hilarious bad taste and sleaze is astonishing. the dialog is breathtakingly fast and campy. you\\'ll either love or hate this film, but give it go. i\\'ve seen it 4 times and absolutely love it. divine is in the quest for being the filthiest person alive, but so are her rivals too in this obscene and disgusting (but funny) and stylish little film. <br /><br />divine was phenomenal, and \"she\" will always be missed greatly. edith massey does the unforgettable performance as the \"egglady\" and don\\'t forget the energetic mink stole!<br /><br />über crazy s**t! <br /><br />recommended also for you sick little puppies;<br /><br />female trouble <br /><br />desperate living <br /><br />polyester'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-c24ece84ff18>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mone_hotted_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mone_hotted_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mone_hotted_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mone_hotted_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mone_hotted_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mone_hotted_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-656648c9c405>\u001b[0m in \u001b[0;36mpreprocess_text\u001b[0;34m(text, maxlen, batch_size)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpreprocess_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMAXLEN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mone_hotted_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mALPHABET_LEN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchar\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "one_hotted_text = preprocess_text(text)\n",
    "one_hotted_text = Variable(one_hotted_text.cuda())\n",
    "one_hotted_text = one_hotted_text.permute(1, 0, 2)\n",
    "\n",
    "pred = model(one_hotted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 2])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hotted_text = preprocess_text([t], batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = shuffle(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"the true story of the friendship that shook south africa and awakened the world.\" <br /><br />richard attenborough, who directed \"a bridge too far\" and \"gandhi\", wanted to bring the story of steve biko to life, and the journey and trouble that journalist donald woods went through in order to get his story told. the films uses wood's two books for it's information and basis - \"biko\" and \"asking for trouble\".<br /><br />the film takes place in the late 1970's, in south africa. south africa is in the grip of the terrible apartheid, which keeps the blacks separated from the whites and classifies the whites as the superior race. the blacks are forced to live in shantytowns on the outskirts of the cities and towns, and they come under frequent harassment by the police and the army. we are shown a dawn raid on a shantytown, as bulldozers and armed police force their way through the camp beating and even killing the inhabitants. then we are introduced to donald woods (kevin kline), who is the editor of a popular new\n",
      "\n",
      "\n",
      "1\n",
      "\n",
      "\n",
      "\n",
      "well, this is probably one of the best movies i've seen and i love it so much that i've memorized most of the script (especially the scene in the storage unit when jerry lee breaks wind) and even with the script in my head i still like to watch it for jerry lee, that german shepherd is hysterical and really is put to the test to see who's smarter. the tag line holds true as well. not to mention the acting is great, though christine tucci sounds different in a whisper (check filmography under csi if you don't know what i mean). it's too bad that this movie only contained the single issue dooley and jerry lee had to work with, it would have been pretty cool to see the tricks that zeus and welles had up their sleeve.\n",
      "\n",
      "\n",
      "1\n",
      "\n",
      "\n",
      "\n",
      "eddie murphy is one of the funniest comedians ever - probably the funniest. delirious is the best stand-up comedy i've ever seen and it is a must-have for anyone who loves a good laugh!! i've watched this movie hundreds of times and every time i see it - i still have side-splitting fun. this is definitely one for your video library. i guarantee that you will have to watch it several times in order to hear all the jokes because you will be laughing so much - that you will miss half of them! delirious is hilarious!<br /><br />although there are a lot of funny comedians out there - after watching this stand-up comedy, most of them will seem like second-class citizens. if you have never seen it - get it, watch it - and you will love it!! it will make you holler!!! :-)\n",
      "\n",
      "\n",
      "1\n",
      "\n",
      "\n",
      "\n",
      "it all begins with a series of thefts of seemingly unrelated objects in a hostel for students on hickory road, london. concerned for her sister, who is the housekeeper there, miss lemon asks hercule poirot to look into the matter. he agrees, but soon the stakes get higher when a girl, who had admitted that she was responsible for most (not all) of the thefts, is found murdered.<br /><br />\"hickory dickory dock\" is a solid brain exercise, without being as mind-numbingly complicated as \"one, two, buckle my shoe\". murder, theft and diamond smuggling are the crimes involved, and the final twist that ties everything together is revealed only in the last 2 minutes! the characters are interesting, particularly the psychology student colin mcnabb and the mysterious american girl sally finch, inspector japp has his funny moments (in perhaps the closest this series has come to \"toilet humor\"), and miss lemon gets a more integral part to the story than usual. (***)\n",
      "\n",
      "\n",
      "1\n",
      "\n",
      "\n",
      "\n",
      "rumors is a memorable entry in the wartime series of instructional cartoons starring \"private snafu.\" the films were aimed at servicemen and were directed, animated and scored by some of the top talent from warner bros.' termite terrace, including friz freleng, chuck jones, and carl stalling. the invaluable mel blanc supplied the voice for snafu, and the stories and rhyming narration for many of the films was supplied by theodor geisel, i.e. dr. seuss. the idea was to convey basic concepts with humor and vivid imagery, using the character of snafu as a perfect negative example: he was the dope, the little twerp who would do everything you're not supposed to do. according to chuck jones the scripts had to be approved by pentagon officials, but army brass also permitted the animators an unusual amount of freedom concerning language and bawdy jokes, certainly more than theatrical censorship of the time would allow-- all for the greater good, of course.<br /><br />as the title would indicate, this cartoon is an i\n",
      "\n",
      "\n",
      "1\n",
      "\n",
      "\n",
      "\n",
      "this could be a 10 if it wasn't for the quite predictable and hollywood-ish scenario. daniel day-lewis confirms its position as one of the leading actors of our time (why not the leading may i ask) and the rest of the cast stand in a very high level. i personally was impressed with hugh o' connor who played christy brown as a child. the very first scene i watched him was really strong. wow.\n",
      "\n",
      "\n",
      "1\n",
      "\n",
      "\n",
      "\n",
      "this film was totally not what i expected. <br /><br />if this film was called something else no one would even notice the difference between the two. <br /><br />its really strange because i cannot see the point . the prequel and sequel lets just say don't make sense, the don't even match . maybe i am naive but ain't a vol 1 & vol 2 meant to match up. <br /><br />carlito was in jail in the 1st one and dies in the original, and in the prequel he lives and don't go jail. <br /><br />the plot was ok , but they should have changed round some actors and some of the story line and the name of the film and it would have been a good film .<br /><br />i really expected it to end like the other one started. <br /><br />if some one has a opinion on this post it please.\n",
      "\n",
      "\n",
      "0\n",
      "\n",
      "\n",
      "\n",
      "the final installment in the action thriller franchise is just that probably the hardest hitting of the three films. it goes further to play the anti-bond theme. bourne doesn't like what he is doing and wants to know about his blurry past. everything about this film hits it on the nail from the cinematography to choreography/stunt work to the script to acting.<br /><br />the film starts out in a flurry as bourne is running from the moscow police. the story seems to pick up right where the first film left off. or does it? the time is a little muddled here, but we get the fact that bourne is remembering things. a sudden flashback while trying to clean himself up nearly gets him caught, but he makes it and doesn't kill anyone. they aren't his target. from there we get more of the intrigue of his past with a new player, noah vosen, who seems to know everything about bourne and will protect it at all costs. pamela landy is back as well as nicky parsons who seems to have a past with bourne as well.<br /><br />the c\n",
      "\n",
      "\n",
      "1\n",
      "\n",
      "\n",
      "\n",
      "an original uncensored print of this amazing film was discovered in 2004 in the library of congress, and has been shown in a few specialized theaters around the world in 2005. according to current reviews that i've found online, the original has all of the nastiest dialog and innuendos intact; they were later either removed or completely re-shot by the studio prior to initial release, in order to pass the new york state censors. i have also read that a dvd is \"expected in 2006\" and one can only hope! if we're really luckily, it will include comparisons between the 2 versions. note that the released censored version was originally available on laserdisc, which i have seen. stanwyck rules!\n",
      "\n",
      "\n",
      "1\n",
      "\n",
      "\n",
      "\n",
      "take a scifi original movie and mix in a little alternative/revisionist history, and you get \"aztec rex.\" apparently hernand cortes, before conquering the aztec empire, had to first conquer a tyrannosaurus rex and her mate. that's the thrust of this movie. given the plot it could have really sucked; the fact that it only kind of sucked is a tip of the cap to the writers. there are a few problems. for starters, cortes is played by ian ziering. even with a black wig, ziering as cortes is about as convincing as axl rose playing gandhi. and though cortes conquers the indigenous peoples of mexico, the aztecs here seem to be played by an all-hawaiian ensemble. casting aside, the t-rex(es) look reasonably good, though every time one of them gets shot it just oozed cgi. and they die too easily; i suppose if a t-rex were around in real life they probably could be felled or at least wounded by some rather rudimentary, 16th-century weaponry. but it takes something away from the movie. there are also some graphic t-rex-s\n",
      "\n",
      "\n",
      "0\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, text in enumerate(onehot2text(preprocess_text(X[:10], batch_size=10))):\n",
    "    print(text)\n",
    "    print('\\n')\n",
    "    print(y[i])\n",
    "    print('\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ошибок в кодировке по-видимому нет. Почему сеть не учится всё ещё неясно"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-env",
   "language": "python",
   "name": "pytorch-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
