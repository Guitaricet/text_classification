{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from time import time\n",
    "from random import random, choice\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.nn import init\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "import torchtext\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "from tqdm import tqdm as tqdm\n",
    "\n",
    "np.random.seed(42)\n",
    "CUDA = torch.cuda.is_available()\n",
    "\n",
    "CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alphabet from the paper\n",
    "# https://papers.nips.cc/paper/5782-character-level-convolutional-networks-for-text-classification.pdf\n",
    "ALPHABET = ['<UNK>'] + ['\\n'] + [s for s in \"\"\" abcdefghijklmnopqrstuvwxyz0123456789-,;.!?:'’’/\\|_@#$%ˆ&* ̃‘+-=<>()[]{}\"\"\"]\n",
    "char2int = {s: i for s, i in zip(ALPHABET, range(len(ALPHABET)))}\n",
    "\n",
    "MAX_WORD_LEN = 8  # chars in word (try 32?)\n",
    "MAX_TEXT_LEN = 256  # words in text\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "VALID_SIZE = 0.1\n",
    "\n",
    "NOISE_LEVELS = [0, 0.01, 0.025, 0.05, 0.075, 0.1, 0.125, 0.15, 0.175, 0.2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы использовать CNN на слова, нужно фиксировать длину слова."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HieracialIMDB(torchtext.datasets.imdb.IMDB):\n",
    "    \"\"\"\n",
    "    Zero vector used for padding\n",
    "    \"\"\"\n",
    "    noise_level = 0\n",
    "    alphabet = ALPHABET\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = super(HieracialIMDB, self).__getitem__(idx)\n",
    "        _text_tensor = self.preprocess(item.text)\n",
    "\n",
    "        label = int(item.label == 'pos')\n",
    "        return _text_tensor, label\n",
    "    \n",
    "    def preprocess(self, text, with_noise=True):\n",
    "        _text_tensor = torch.zeros([MAX_WORD_LEN * MAX_TEXT_LEN, len(self.alphabet)])\n",
    "\n",
    "        for i, token in enumerate(text):\n",
    "            if i >= MAX_TEXT_LEN:\n",
    "                break\n",
    "            if with_noise:\n",
    "                token = self.noise_generator(token)\n",
    "            for j, char in enumerate(token):\n",
    "                if j >= MAX_WORD_LEN:\n",
    "                    break\n",
    "                _text_tensor[i*MAX_WORD_LEN + j, char2int.get(char, char2int['<UNK>'])] = 1.\n",
    "        return _text_tensor\n",
    "    \n",
    "#     def _encode_word(self, word):\n",
    "#         word_tensor = torch.zeros([MAX_WORD_LEN, len(ALPHABET)])\n",
    "        \n",
    "#         for i, char in enumerate(word):\n",
    "#             word_tensor[i,char2int[char]] = 1.\n",
    "        \n",
    "#         return word_tensor\n",
    "\n",
    "    def noise_generator(self, string):\n",
    "        # removed '' symbol from alphabet for safety on word vectors\n",
    "        noised = \"\"\n",
    "        for c in string:\n",
    "            if random() > self.noise_level:\n",
    "                noised += c\n",
    "            if random() < self.noise_level:\n",
    "                noised += choice(self.alphabet)\n",
    "        return noised\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_valid_loader(dataset, valid_size, batch_size, random_seed=42, shuffle=True, num_workers=4):\n",
    "\n",
    "    len_dataset = len(dataset)\n",
    "    indices = list(range(len_dataset))\n",
    "\n",
    "    if shuffle:\n",
    "        np.random.seed(random_seed)\n",
    "        np.random.shuffle(indices)\n",
    "\n",
    "    val_actual_size = int(len_dataset * valid_size)\n",
    "\n",
    "    train_idx, valid_idx = indices[:-val_actual_size], indices[-val_actual_size:]\n",
    "\n",
    "    train_sampler = SubsetRandomSampler(train_idx)\n",
    "    valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        dataset, batch_size=batch_size, sampler=train_sampler, num_workers=4\n",
    "    )\n",
    "    valid_loader = torch.utils.data.DataLoader(\n",
    "        dataset, batch_size=batch_size, sampler=valid_sampler, num_workers=4\n",
    "    )\n",
    "\n",
    "    return train_loader, valid_loader\n",
    "\n",
    "def onehot2text(one_hotted_text, batch_size=None, show_pad=False):\n",
    "    if batch_size is None:\n",
    "        text = ''\n",
    "        max_values, idx = torch.max(one_hotted_text, 1)\n",
    "        for c, i in enumerate(idx):\n",
    "            if max_values[c] == 0:\n",
    "                if show_pad:\n",
    "                    symb = '<PAD>'\n",
    "                else:\n",
    "                    symb = ''\n",
    "            else:\n",
    "                symb = ALPHABET[i]\n",
    "            text += symb\n",
    "        return text\n",
    "    else:\n",
    "        texts = []\n",
    "        for text in one_hotted_text:\n",
    "            texts.append(onehot2text(one_hotted_text, batch_size=None))\n",
    "        return texts\n",
    "\n",
    "def get_metrics(model, test_data, noise_level=None):\n",
    "    \"\"\"\n",
    "    :param test_data: dataset or dataloader\n",
    "\n",
    "    Moder will be in TRAIN mode after that\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    predictions = []\n",
    "    lables = []\n",
    "    \n",
    "    if isinstance(test_data, torch.utils.data.Dataset):\n",
    "        if noise_level is not None:\n",
    "            test_data.noise_level = noise_level\n",
    "\n",
    "        test_dataloader = torch.utils.data.DataLoader(\n",
    "            test_data, batch_size=BATCH_SIZE\n",
    "        )\n",
    "    else:\n",
    "        assert isinstance(test_data, torch.utils.data.DataLoader)\n",
    "        test_dataloader = test_data\n",
    "\n",
    "    for text, label in test_dataloader:\n",
    "        if CUDA:\n",
    "            text = Variable(text.cuda())\n",
    "        else:\n",
    "            text = Variable(text)\n",
    "\n",
    "        text = text.permute(1, 0, 2)  # (1, 0, 2) for RNN\n",
    "        prediction = model(text)\n",
    "\n",
    "        _, idx = torch.max(prediction, 1)\n",
    "        predictions += idx.data.tolist()\n",
    "        lables += label.tolist()\n",
    "\n",
    "    acc = accuracy_score(lables, predictions)\n",
    "    f1 = f1_score(lables, predictions)\n",
    "    model.train()\n",
    "    return {'accuracy': acc, 'f1': f1}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# without spacy tokenizer it's commas all after the words =(\n",
    "\n",
    "text_field = torchtext.data.Field(\n",
    "    lower=True, include_lengths=False, fix_length=MAX_TEXT_LEN, tensor_type=torch.FloatTensor, batch_first=True,\n",
    "    use_vocab=False, tokenize='spacy'\n",
    ")\n",
    "label_field = torchtext.data.Field(sequential=False, use_vocab=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 7s, sys: 1.09 s, total: 1min 8s\n",
      "Wall time: 1min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train, test = HieracialIMDB.splits(text_field, label_field)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"thismusicalisdecidedlmixed,andnoneoftheelementsreallyfittogether,butitsomehowmanagestobemostlyenjoyabl.theplotcontainssomeoftheelementsofwodehous'snovel,butnoneofitsvirtues,thoughheco-wrotethescript.thesongs,thoughcharming,havenothingtodowiththisparticulfilm,andareunusuallcrudelysqueezedintotheplot,evenbypre-oklahomastandard.burnsandallendotheirusualshtickquitecompeten,butitmissesthetoneoftherestofthefilmbyaboutfortyiqpoints.</><br/>thereareafewhighpoints.reginaldgardinerdoesgoodworkwhenherememberthatthisisatalkie,andstopsmugginglikeasilentactor.andthereareafewbitsofwritingwhichcouldonlyhavebeenwrittenbywodehous,thoughmostofthefilmfeelsliketheproductiofoneofthehollywoomeetingshelaterparodied.\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onehot2text(train[0][0])  # no spaces is onehot2text problem, not a data one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader, val_dataloader = get_train_valid_loader(train, VALID_SIZE, BATCH_SIZE)\n",
    "\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "    test, batch_size=BATCH_SIZE\n",
    ")\n",
    "# from https://github.com/akurniawan/pytorch-transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/akurniawan/pytorch-transformer\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self,\n",
    "                 query_dim,\n",
    "                 key_dim,\n",
    "                 num_units,\n",
    "                 dropout_p=0.5,\n",
    "                 h=8,\n",
    "                 is_masked=False):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "\n",
    "        if query_dim != key_dim:\n",
    "            raise ValueError(\"query_dim and key_dim must be the same\")\n",
    "        if num_units % h != 0:\n",
    "            raise ValueError(\"num_units must be dividable by h\")\n",
    "        if query_dim != num_units:\n",
    "            raise ValueError(\"to employ residual connection, the number of \"\n",
    "                             \"query_dim and num_units must be the same\")\n",
    "\n",
    "        self._num_units = num_units\n",
    "        self._h = h\n",
    "        self._key_dim = Variable(torch.FloatTensor([key_dim]))\n",
    "        if CUDA:\n",
    "            self._key_dim = self._key_dim.cuda()\n",
    "        self._dropout_p = dropout_p\n",
    "        self._is_masked = is_masked\n",
    "\n",
    "        self.query_layer = nn.Linear(query_dim, num_units, bias=False)\n",
    "        self.key_layer = nn.Linear(key_dim, num_units, bias=False)\n",
    "        self.value_layer = nn.Linear(key_dim, num_units, bias=False)\n",
    "        self.bn = nn.BatchNorm1d(num_units)\n",
    "\n",
    "    def forward(self, query, keys):\n",
    "        Q = self.query_layer(query)\n",
    "        K = self.key_layer(keys)\n",
    "        V = self.value_layer(keys)\n",
    "\n",
    "        # split each Q, K and V into h different values from dim 2\n",
    "        # and then merge them back together in dim 0\n",
    "        chunk_size = int(self._num_units / self._h)\n",
    "        Q = torch.cat(Q.split(split_size=chunk_size, dim=2), dim=0)\n",
    "        K = torch.cat(K.split(split_size=chunk_size, dim=2), dim=0)\n",
    "        V = torch.cat(V.split(split_size=chunk_size, dim=2), dim=0)\n",
    "\n",
    "        # calculate QK^T\n",
    "        attention = torch.matmul(Q, K.transpose(1, 2))\n",
    "        # normalize with sqrt(dk)\n",
    "        attention = attention / torch.sqrt(self._key_dim)\n",
    "        # use masking (usually for decoder) to prevent leftward\n",
    "        # information flow and retains auto-regressive property\n",
    "        # as said in the paper\n",
    "        if self._is_masked:\n",
    "            diag_vals = attention[0].sign().abs()\n",
    "            diag_mat = diag_vals.tril()\n",
    "            diag_mat = diag_mat.unsqueeze(0).expand(attention.size())\n",
    "            # we need to enforce converting mask to Variable, since\n",
    "            # in pytorch we can't do operation between Tensor and\n",
    "            # Variable\n",
    "            mask = Variable(\n",
    "                torch.ones(diag_mat.size()) * (-2**32 + 1), requires_grad=False)\n",
    "            # this is some trick that I use to combine the lower diagonal\n",
    "            # matrix and its masking. (diag_mat-1).abs() will reverse the value\n",
    "            # inside diag_mat, from 0 to 1 and 1 to zero. with this\n",
    "            # we don't need loop operation andn could perform our calculation\n",
    "            # faster\n",
    "            attention = (attention * diag_mat) + (mask * (diag_mat-1).abs())\n",
    "        # put it to softmax\n",
    "        attention = F.softmax(attention, dim=-1)\n",
    "        # apply dropout\n",
    "        attention = F.dropout(attention, self._dropout_p)\n",
    "        # multiplyt it with V\n",
    "        attention = torch.matmul(attention, V)\n",
    "        # convert attention back to its input original size\n",
    "        restore_chunk_size = int(attention.size(0) / self._h)\n",
    "        attention = torch.cat(\n",
    "            attention.split(split_size=restore_chunk_size, dim=0), dim=2)\n",
    "        # residual connection\n",
    "        attention += query\n",
    "        # apply batch normalization\n",
    "#         attention = self.bn(attention.transpose(1, 2)).transpose(1, 2)\n",
    "\n",
    "        return attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionedYoonKimModel(nn.Module):\n",
    "    def __init__(self,\n",
    "                 n_filters,\n",
    "                 cnn_kernel_size,\n",
    "                 hidden_dim_out,\n",
    "                 dropout=0.5,\n",
    "                 init_function=None,\n",
    "                 embedding_dim=len(ALPHABET),\n",
    "                 pool_kernel_size=MAX_WORD_LEN,\n",
    "                 heads=1):\n",
    "        \"\"\"\n",
    "        CharCNN-WordRNN model with multi-head attention\n",
    "        Default pooling is MaxOverTime pooling\n",
    "        \"\"\"\n",
    "        assert cnn_kernel_size % 2  # for 'same' padding\n",
    "\n",
    "        super(AttentionedYoonKimModel, self).__init__()\n",
    "        self.dropout = dropout\n",
    "        self.init_function = init_function\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.n_filters = n_filters\n",
    "        self.cnn_kernel_size = cnn_kernel_size\n",
    "        self.hidden_dim_out = hidden_dim_out\n",
    "        self.heads = heads\n",
    "\n",
    "        self.embedding = nn.Linear(len(ALPHABET), embedding_dim)\n",
    "        self.chars_cnn = nn.Sequential(\n",
    "            nn.Conv1d(embedding_dim, n_filters, kernel_size=cnn_kernel_size, stride=1, padding=int(cnn_kernel_size - 1) // 2),  # 'same' padding\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=pool_kernel_size)\n",
    "        )\n",
    "        if init_function is not None:\n",
    "            self.chars_cnn[0].weight = init_function(self.chars_cnn[0].weight)\n",
    "\n",
    "        _conv_stride = 1  # by default\n",
    "        _pool_stride = pool_kernel_size  # by default\n",
    "        # I am not sure this formula is always correct:\n",
    "        self.conv_dim = n_filters * max(1, int(((MAX_WORD_LEN - cnn_kernel_size) / _conv_stride - pool_kernel_size) / _pool_stride + 1))\n",
    "\n",
    "        self.words_rnn = nn.GRU(self.conv_dim, hidden_dim_out, dropout=dropout)\n",
    "        self.attention = MultiHeadAttention(hidden_dim_out, hidden_dim_out, hidden_dim_out, dropout_p=self.dropout, h=self.heads)\n",
    "        self.projector = nn.Linear(hidden_dim_out, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(1)\n",
    "        # TODO: hadrcode! (for CUDA)\n",
    "        words_tensor = Variable(torch.zeros(MAX_TEXT_LEN, batch_size, self.conv_dim)).cuda()\n",
    "        \n",
    "        for i in range(MAX_TEXT_LEN):\n",
    "            word = x[i * MAX_WORD_LEN : (i + 1) * MAX_WORD_LEN, :]\n",
    "            word = self.embedding(word)\n",
    "            word = word.permute(1, 2, 0)\n",
    "            word = self.chars_cnn(word)\n",
    "            word = word.view(word.size(0), -1)\n",
    "            words_tensor[i, :] = word\n",
    "\n",
    "        x, _ = self.words_rnn(words_tensor)\n",
    "        x = self.attention(x, x)\n",
    "        x = self.projector(x[-1])\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_params_num(model):\n",
    "    return sum(np.prod(list(p.size())) for p in model.parameters())\n",
    "\n",
    "def mk_dataline(model_type, epochs, lr, noise_level_train, noise_level_test, acc_train, acc_test,\n",
    "                f1_train, f1_test, dropout, model, init_function=None):\n",
    "    return {\n",
    "        'task': 'IMDB binary classification',\n",
    "        'model_type': model_type,\n",
    "        'trainable_params': model_params_num(model), 'dropout': dropout, 'init_function': init_function,\n",
    "        'epochs': epochs, 'lr': lr,\n",
    "        'noise_level_train': noise_level_train, 'noise_level_test': noise_level_test,\n",
    "        'acc_train': acc_train, 'acc_test': acc_test,\n",
    "        'f1_train': f1_train, 'f1_test': f1_test,\n",
    "        'model_desc': str(model),\n",
    "        'data_desc': 'MAX_WORD_LEN = 8, MAX_TEXT_LEN = 256'\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model_with(noise_level, n_filters, cnn_kernel_size, hidden_dim_out, dropout=0.5,\n",
    "                   lr=1e-4, epochs=30, heads=1, print_every=10, init_function=None, _model=None):\n",
    "    start_time = time()\n",
    "    HieracialIMDB.noise_level = noise_level\n",
    "\n",
    "    if _model is None:\n",
    "        model = AttentionedYoonKimModel(\n",
    "            n_filters=n_filters, cnn_kernel_size=cnn_kernel_size, hidden_dim_out=hidden_dim_out, dropout=dropout,\n",
    "            init_function=init_function, heads=heads\n",
    "        )\n",
    "        if CUDA:\n",
    "            model.cuda()\n",
    "        model.train()\n",
    "    \n",
    "    else:\n",
    "        model = _model\n",
    "    \n",
    "    model_name = '_AttentionedYoonKim_lr%s_dropout%s_noise_level%s_spacy_wordlen8_heads%s' % (\n",
    "        int(-np.log10(lr)), model.dropout, noise_level, model.heads\n",
    "    )\n",
    "\n",
    "    writer = SummaryWriter(comment=model_name)\n",
    "    print('Writer: %s' % list(writer.all_writers.keys()))\n",
    "\n",
    "    optimizer = optim.Adam(params=model.parameters(), lr=lr)\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    global_step = 0\n",
    "\n",
    "    loss_f = F.cross_entropy\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        for batch_idx, (text, label) in enumerate(dataloader):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            if CUDA:\n",
    "                text = Variable(text.cuda())\n",
    "                label = Variable(torch.LongTensor(label).cuda())\n",
    "            else:\n",
    "                text = Variable(text)\n",
    "                label = Variable(torch.LongTensor(label))\n",
    "\n",
    "            text = text.permute(1, 0, 2)\n",
    "            prediction = model(text)\n",
    "            loss = loss_f(prediction, label)\n",
    "\n",
    "            writer.add_scalar('loss', loss.data[0], global_step=global_step)\n",
    "\n",
    "            loss.backward()        \n",
    "            torch.nn.utils.clip_grad_norm(model.parameters(), 1e-1)\n",
    "            optimizer.step()\n",
    "\n",
    "            if CUDA:\n",
    "                torch.cuda.synchronize()\n",
    "            global_step += 1\n",
    "\n",
    "        # evaluation\n",
    "        if epoch % print_every == 0:\n",
    "            print('Epoch %s. Global step %s. T=%s min' % (epoch, global_step, (time() - start_time) / 60.))\n",
    "            print('Loss               : %s' % loss.data[0])\n",
    "\n",
    "        # in-batch\n",
    "        _, idx = torch.max(prediction, 1)\n",
    "        _labels = label.data.tolist()\n",
    "        _predictions = idx.data.tolist()\n",
    "        acc = accuracy_score(_labels, _predictions)\n",
    "        f1 = f1_score(_labels, _predictions)\n",
    "        writer.add_scalar('accuracy_train', acc, global_step=global_step)\n",
    "        writer.add_scalar('f1_train', f1, global_step=global_step)\n",
    "        if epoch % print_every == 0:\n",
    "            print('In-batch accuracy  :', acc)\n",
    "\n",
    "        # validation\n",
    "        metrics = get_metrics(model, val_dataloader)\n",
    "        if epoch % print_every == 0:\n",
    "            print('Validation accuracy: %s, f1: %s' % (metrics['accuracy'], metrics['f1']))\n",
    "            print()\n",
    "\n",
    "        writer.add_scalar('accuracy_val', metrics['accuracy'], global_step=global_step)\n",
    "        writer.add_scalar('f1_val', metrics['f1'], global_step=global_step)\n",
    "\n",
    "    # Test\n",
    "    metrics_test = None\n",
    "\n",
    "    print('Calculating validation metrics... Time %s min' % ((time() - start_time) / 60.))\n",
    "    metrics_train = get_metrics(model, dataloader)\n",
    "    acc_train = metrics_train['accuracy']\n",
    "    f1_train = metrics_train['f1']\n",
    "\n",
    "    for test_noise in NOISE_LEVELS:\n",
    "        metrics = get_metrics(model, test, test_noise)\n",
    "        if test_noise == noise_level:\n",
    "            metrics_test = metrics\n",
    "\n",
    "        acc_test = metrics['accuracy']\n",
    "        f1_test = metrics['f1']\n",
    "        results.append(mk_dataline(\n",
    "            model_type='AttentionedYoonKim', epochs=epochs, lr=lr,\n",
    "            noise_level_train=noise_level, acc_train=acc_train, f1_train=f1_train,\n",
    "            noise_level_test=test_noise, acc_test=acc_test, f1_test=f1_test,\n",
    "            dropout=dropout, model=model,\n",
    "            init_function=init_function\n",
    "        ))\n",
    "\n",
    "    print('Final test metrics: %s, Time %s min' % (metrics_test, ((time() - start_time) / 60.)))\n",
    "    if metrics_test is not None:\n",
    "        writer.add_scalar('accuracy_test_final', metrics_test['accuracy'], global_step=global_step)\n",
    "        writer.add_scalar('f1_test_final', metrics_test['f1'], global_step=global_step)\n",
    "    print()\n",
    "    model.eval()\n",
    "    # model is in EVAL mode!\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writer: ['runs/May08_18-58-19_phobos-aijun_AttentionedYoonKim_lr3_dropout0.5_noise_level0_spacy_wordlen8_heads1']\n",
      "Epoch 0. Global step 704. T=2.3250306129455565 min\n",
      "Loss               : 0.6998939514160156\n",
      "In-batch accuracy  : 0.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phobos_aijun/.virtualenvs/pytorch-env/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.4976, f1: 0.1581769436997319\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phobos_aijun/.virtualenvs/pytorch-env/lib/python3.5/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10. Global step 7744. T=26.54800899028778 min\n",
      "Loss               : 0.0964764803647995\n",
      "In-batch accuracy  : 1.0\n",
      "Validation accuracy: 0.8564, f1: 0.8547146904087414\n",
      "\n",
      "Epoch 20. Global step 14784. T=50.86627300182978 min\n",
      "Loss               : 3.135204315185547e-05\n",
      "In-batch accuracy  : 1.0\n",
      "Validation accuracy: 0.8524, f1: 0.8540925266903914\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for noise_level in tqdm(NOISE_LEVELS):\n",
    "    run_model_with(\n",
    "        noise_level=noise_level, n_filters=256, cnn_kernel_size=5, hidden_dim_out=128, dropout=0.5, lr=1e-3, epochs=30, heads=1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writer: ['runs/May09_13-43-24_phobos-aijun_AttentionedYoonKim_lr3_dropout0.5_noise_level0_spacy_wordlen8_heads4']\n",
      "Epoch 0. Global step 704. T=2.3463613828023275 min\n",
      "Loss               : 0.6847973465919495\n",
      "In-batch accuracy  : 1.0\n",
      "Validation accuracy: 0.5036, f1: 0.25195901145268235\n",
      "\n",
      "Epoch 10. Global step 7744. T=26.84283440510432 min\n",
      "Loss               : 0.06819652020931244\n",
      "In-batch accuracy  : 1.0\n",
      "Validation accuracy: 0.8536, f1: 0.8604118993135013\n",
      "\n",
      "Epoch 20. Global step 14784. T=51.34217491547267 min\n",
      "Loss               : 0.0001227855682373047\n",
      "In-batch accuracy  : 1.0\n",
      "Validation accuracy: 0.85, f1: 0.851602690937871\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phobos_aijun/.virtualenvs/pytorch-env/lib/python3.5/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "/home/phobos_aijun/.virtualenvs/pytorch-env/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating validation metrics... Time 73.48504288991292 min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [1:26:44<13:00:43, 5204.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final test metrics: {'accuracy': 0.72036, 'f1': 0.7240794095591429}, Time 86.74774930874507 min\n",
      "\n",
      "Writer: ['runs/May09_15-10-09_phobos-aijun_AttentionedYoonKim_lr3_dropout0.5_noise_level0.01_spacy_wordlen8_heads4']\n",
      "Epoch 0. Global step 704. T=2.336410439014435 min\n",
      "Loss               : 0.6932069659233093\n",
      "In-batch accuracy  : 0.5\n",
      "Validation accuracy: 0.4932, f1: 0.6207722238850644\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phobos_aijun/.virtualenvs/pytorch-env/lib/python3.5/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10. Global step 7744. T=26.901063871383666 min\n",
      "Loss               : 0.31047651171684265\n",
      "In-batch accuracy  : 0.75\n",
      "Validation accuracy: 0.8504, f1: 0.8458367683429514\n",
      "\n",
      "Epoch 20. Global step 14784. T=51.47357052564621 min\n",
      "Loss               : 1.9057666063308716\n",
      "In-batch accuracy  : 0.75\n",
      "Validation accuracy: 0.846, f1: 0.8385744234800839\n",
      "\n",
      "Calculating validation metrics... Time 73.69015954732895 min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [2:53:45<11:35:03, 5212.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final test metrics: {'accuracy': 0.77724, 'f1': 0.7659002059775526}, Time 87.01751616795858 min\n",
      "\n",
      "Writer: ['runs/May09_16-37-10_phobos-aijun_AttentionedYoonKim_lr3_dropout0.5_noise_level0.025_spacy_wordlen8_heads4']\n",
      "Epoch 0. Global step 704. T=2.3406466404596964 min\n",
      "Loss               : 0.6962195634841919\n",
      "In-batch accuracy  : 0.25\n",
      "Validation accuracy: 0.498, f1: 0.17704918032786884\n",
      "\n",
      "Epoch 10. Global step 7744. T=26.86790764729182 min\n",
      "Loss               : 0.3474670350551605\n",
      "In-batch accuracy  : 0.75\n",
      "Validation accuracy: 0.8552, f1: 0.8604471858134156\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phobos_aijun/.virtualenvs/pytorch-env/lib/python3.5/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20. Global step 14784. T=51.39845228592555 min\n",
      "Loss               : 0.32556477189064026\n",
      "In-batch accuracy  : 0.75\n",
      "Validation accuracy: 0.8472, f1: 0.8520526723470179\n",
      "\n",
      "Calculating validation metrics... Time 73.52286429802577 min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [4:20:34<10:08:01, 5211.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final test metrics: {'accuracy': 0.80208, 'f1': 0.8010614345448698}, Time 86.81639763514201 min\n",
      "\n",
      "Writer: ['runs/May09_18-03-59_phobos-aijun_AttentionedYoonKim_lr3_dropout0.5_noise_level0.05_spacy_wordlen8_heads4']\n",
      "Epoch 0. Global step 704. T=2.340618352095286 min\n",
      "Loss               : 0.6931796073913574\n",
      "In-batch accuracy  : 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phobos_aijun/.virtualenvs/pytorch-env/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.5012, f1: 0.29026750142287994\n",
      "\n",
      "Epoch 10. Global step 7744. T=26.870443864663443 min\n",
      "Loss               : 0.23138980567455292\n",
      "In-batch accuracy  : 0.75\n",
      "Validation accuracy: 0.8512, f1: 0.8498789346246973\n",
      "\n",
      "Epoch 20. Global step 14784. T=51.39888294935226 min\n",
      "Loss               : 0.01804119348526001\n",
      "In-batch accuracy  : 1.0\n",
      "Validation accuracy: 0.8448, f1: 0.8375209380234506\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phobos_aijun/.virtualenvs/pytorch-env/lib/python3.5/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating validation metrics... Time 73.58182549476624 min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [5:47:28<8:41:12, 5212.08s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final test metrics: {'accuracy': 0.7522, 'f1': 0.7483241925655089}, Time 86.889908182621 min\n",
      "\n",
      "Writer: ['runs/May09_19-30-53_phobos-aijun_AttentionedYoonKim_lr3_dropout0.5_noise_level0.075_spacy_wordlen8_heads4']\n",
      "Epoch 0. Global step 704. T=2.332847269376119 min\n",
      "Loss               : 0.6834204792976379\n",
      "In-batch accuracy  : 0.75\n",
      "Validation accuracy: 0.4972, f1: 0.08448652585579024\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phobos_aijun/.virtualenvs/pytorch-env/lib/python3.5/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "for noise_level in tqdm(NOISE_LEVELS):\n",
    "    run_model_with(\n",
    "        noise_level=noise_level, n_filters=256, cnn_kernel_size=5, hidden_dim_out=128, dropout=0.5, lr=1e-3, epochs=30, heads=4\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AttentionedYoonKimModel(\n",
      "  (embedding): Linear(in_features=74, out_features=74, bias=True)\n",
      "  (chars_cnn): Sequential(\n",
      "    (0): Conv1d(74, 256, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool1d(kernel_size=8, stride=8, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (words_rnn): GRU(256, 128, dropout=0.5)\n",
      "  (attention): MultiHeadAttention(\n",
      "    (query_layer): Linear(in_features=128, out_features=128, bias=False)\n",
      "    (key_layer): Linear(in_features=128, out_features=128, bias=False)\n",
      "    (value_layer): Linear(in_features=128, out_features=128, bias=False)\n",
      "    (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "  )\n",
      "  (projector): Linear(in_features=128, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(results_df['model_desc'][218])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AttentionedYoonKimModel(\n",
      "  (embedding): Linear(in_features=74, out_features=74, bias=True)\n",
      "  (chars_cnn): Sequential(\n",
      "    (0): Conv1d(74, 256, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool1d(kernel_size=8, stride=8, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (words_rnn): GRU(256, 128, dropout=0.5)\n",
      "  (attention): MultiHeadAttention(\n",
      "    (query_layer): Linear(in_features=128, out_features=128, bias=False)\n",
      "    (key_layer): Linear(in_features=128, out_features=128, bias=False)\n",
      "    (value_layer): Linear(in_features=128, out_features=128, bias=False)\n",
      "    (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "  )\n",
      "  (projector): Linear(in_features=128, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(results_df['model_desc'][100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_4heads = results_df.iloc[-100:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc_test</th>\n",
       "      <th>acc_train</th>\n",
       "      <th>data_desc</th>\n",
       "      <th>dropout</th>\n",
       "      <th>epochs</th>\n",
       "      <th>f1_test</th>\n",
       "      <th>f1_train</th>\n",
       "      <th>init_function</th>\n",
       "      <th>lr</th>\n",
       "      <th>model_desc</th>\n",
       "      <th>model_type</th>\n",
       "      <th>noise_level_test</th>\n",
       "      <th>noise_level_train</th>\n",
       "      <th>task</th>\n",
       "      <th>trainable_params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>0.72036</td>\n",
       "      <td>0.983822</td>\n",
       "      <td>MAX_WORD_LEN = 8, MAX_TEXT_LEN = 256</td>\n",
       "      <td>0.5</td>\n",
       "      <td>30</td>\n",
       "      <td>0.724079</td>\n",
       "      <td>0.984064</td>\n",
       "      <td>None</td>\n",
       "      <td>0.001</td>\n",
       "      <td>AttentionedYoonKimModel(\\n  (embedding): Linea...</td>\n",
       "      <td>AttentionedYoonKim</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>IMDB binary classification</td>\n",
       "      <td>298416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>0.71260</td>\n",
       "      <td>0.983822</td>\n",
       "      <td>MAX_WORD_LEN = 8, MAX_TEXT_LEN = 256</td>\n",
       "      <td>0.5</td>\n",
       "      <td>30</td>\n",
       "      <td>0.718268</td>\n",
       "      <td>0.984064</td>\n",
       "      <td>None</td>\n",
       "      <td>0.001</td>\n",
       "      <td>AttentionedYoonKimModel(\\n  (embedding): Linea...</td>\n",
       "      <td>AttentionedYoonKim</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.000</td>\n",
       "      <td>IMDB binary classification</td>\n",
       "      <td>298416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>0.68996</td>\n",
       "      <td>0.983822</td>\n",
       "      <td>MAX_WORD_LEN = 8, MAX_TEXT_LEN = 256</td>\n",
       "      <td>0.5</td>\n",
       "      <td>30</td>\n",
       "      <td>0.698815</td>\n",
       "      <td>0.984064</td>\n",
       "      <td>None</td>\n",
       "      <td>0.001</td>\n",
       "      <td>AttentionedYoonKimModel(\\n  (embedding): Linea...</td>\n",
       "      <td>AttentionedYoonKim</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.000</td>\n",
       "      <td>IMDB binary classification</td>\n",
       "      <td>298416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>0.67116</td>\n",
       "      <td>0.983822</td>\n",
       "      <td>MAX_WORD_LEN = 8, MAX_TEXT_LEN = 256</td>\n",
       "      <td>0.5</td>\n",
       "      <td>30</td>\n",
       "      <td>0.683162</td>\n",
       "      <td>0.984064</td>\n",
       "      <td>None</td>\n",
       "      <td>0.001</td>\n",
       "      <td>AttentionedYoonKimModel(\\n  (embedding): Linea...</td>\n",
       "      <td>AttentionedYoonKim</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.000</td>\n",
       "      <td>IMDB binary classification</td>\n",
       "      <td>298416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>0.65020</td>\n",
       "      <td>0.983822</td>\n",
       "      <td>MAX_WORD_LEN = 8, MAX_TEXT_LEN = 256</td>\n",
       "      <td>0.5</td>\n",
       "      <td>30</td>\n",
       "      <td>0.669013</td>\n",
       "      <td>0.984064</td>\n",
       "      <td>None</td>\n",
       "      <td>0.001</td>\n",
       "      <td>AttentionedYoonKimModel(\\n  (embedding): Linea...</td>\n",
       "      <td>AttentionedYoonKim</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.000</td>\n",
       "      <td>IMDB binary classification</td>\n",
       "      <td>298416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>0.62444</td>\n",
       "      <td>0.983822</td>\n",
       "      <td>MAX_WORD_LEN = 8, MAX_TEXT_LEN = 256</td>\n",
       "      <td>0.5</td>\n",
       "      <td>30</td>\n",
       "      <td>0.649363</td>\n",
       "      <td>0.984064</td>\n",
       "      <td>None</td>\n",
       "      <td>0.001</td>\n",
       "      <td>AttentionedYoonKimModel(\\n  (embedding): Linea...</td>\n",
       "      <td>AttentionedYoonKim</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.000</td>\n",
       "      <td>IMDB binary classification</td>\n",
       "      <td>298416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>0.60816</td>\n",
       "      <td>0.983822</td>\n",
       "      <td>MAX_WORD_LEN = 8, MAX_TEXT_LEN = 256</td>\n",
       "      <td>0.5</td>\n",
       "      <td>30</td>\n",
       "      <td>0.639137</td>\n",
       "      <td>0.984064</td>\n",
       "      <td>None</td>\n",
       "      <td>0.001</td>\n",
       "      <td>AttentionedYoonKimModel(\\n  (embedding): Linea...</td>\n",
       "      <td>AttentionedYoonKim</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.000</td>\n",
       "      <td>IMDB binary classification</td>\n",
       "      <td>298416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>0.59036</td>\n",
       "      <td>0.983822</td>\n",
       "      <td>MAX_WORD_LEN = 8, MAX_TEXT_LEN = 256</td>\n",
       "      <td>0.5</td>\n",
       "      <td>30</td>\n",
       "      <td>0.623257</td>\n",
       "      <td>0.984064</td>\n",
       "      <td>None</td>\n",
       "      <td>0.001</td>\n",
       "      <td>AttentionedYoonKimModel(\\n  (embedding): Linea...</td>\n",
       "      <td>AttentionedYoonKim</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.000</td>\n",
       "      <td>IMDB binary classification</td>\n",
       "      <td>298416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>0.57716</td>\n",
       "      <td>0.983822</td>\n",
       "      <td>MAX_WORD_LEN = 8, MAX_TEXT_LEN = 256</td>\n",
       "      <td>0.5</td>\n",
       "      <td>30</td>\n",
       "      <td>0.614464</td>\n",
       "      <td>0.984064</td>\n",
       "      <td>None</td>\n",
       "      <td>0.001</td>\n",
       "      <td>AttentionedYoonKimModel(\\n  (embedding): Linea...</td>\n",
       "      <td>AttentionedYoonKim</td>\n",
       "      <td>0.175</td>\n",
       "      <td>0.000</td>\n",
       "      <td>IMDB binary classification</td>\n",
       "      <td>298416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>0.56752</td>\n",
       "      <td>0.983822</td>\n",
       "      <td>MAX_WORD_LEN = 8, MAX_TEXT_LEN = 256</td>\n",
       "      <td>0.5</td>\n",
       "      <td>30</td>\n",
       "      <td>0.612556</td>\n",
       "      <td>0.984064</td>\n",
       "      <td>None</td>\n",
       "      <td>0.001</td>\n",
       "      <td>AttentionedYoonKimModel(\\n  (embedding): Linea...</td>\n",
       "      <td>AttentionedYoonKim</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.000</td>\n",
       "      <td>IMDB binary classification</td>\n",
       "      <td>298416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>0.78628</td>\n",
       "      <td>0.984889</td>\n",
       "      <td>MAX_WORD_LEN = 8, MAX_TEXT_LEN = 256</td>\n",
       "      <td>0.5</td>\n",
       "      <td>30</td>\n",
       "      <td>0.776228</td>\n",
       "      <td>0.984722</td>\n",
       "      <td>None</td>\n",
       "      <td>0.001</td>\n",
       "      <td>AttentionedYoonKimModel(\\n  (embedding): Linea...</td>\n",
       "      <td>AttentionedYoonKim</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.010</td>\n",
       "      <td>IMDB binary classification</td>\n",
       "      <td>298416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>0.77724</td>\n",
       "      <td>0.984889</td>\n",
       "      <td>MAX_WORD_LEN = 8, MAX_TEXT_LEN = 256</td>\n",
       "      <td>0.5</td>\n",
       "      <td>30</td>\n",
       "      <td>0.765900</td>\n",
       "      <td>0.984722</td>\n",
       "      <td>None</td>\n",
       "      <td>0.001</td>\n",
       "      <td>AttentionedYoonKimModel(\\n  (embedding): Linea...</td>\n",
       "      <td>AttentionedYoonKim</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>IMDB binary classification</td>\n",
       "      <td>298416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>0.76576</td>\n",
       "      <td>0.984889</td>\n",
       "      <td>MAX_WORD_LEN = 8, MAX_TEXT_LEN = 256</td>\n",
       "      <td>0.5</td>\n",
       "      <td>30</td>\n",
       "      <td>0.753036</td>\n",
       "      <td>0.984722</td>\n",
       "      <td>None</td>\n",
       "      <td>0.001</td>\n",
       "      <td>AttentionedYoonKimModel(\\n  (embedding): Linea...</td>\n",
       "      <td>AttentionedYoonKim</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.010</td>\n",
       "      <td>IMDB binary classification</td>\n",
       "      <td>298416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>0.75360</td>\n",
       "      <td>0.984889</td>\n",
       "      <td>MAX_WORD_LEN = 8, MAX_TEXT_LEN = 256</td>\n",
       "      <td>0.5</td>\n",
       "      <td>30</td>\n",
       "      <td>0.739270</td>\n",
       "      <td>0.984722</td>\n",
       "      <td>None</td>\n",
       "      <td>0.001</td>\n",
       "      <td>AttentionedYoonKimModel(\\n  (embedding): Linea...</td>\n",
       "      <td>AttentionedYoonKim</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.010</td>\n",
       "      <td>IMDB binary classification</td>\n",
       "      <td>298416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>0.73672</td>\n",
       "      <td>0.984889</td>\n",
       "      <td>MAX_WORD_LEN = 8, MAX_TEXT_LEN = 256</td>\n",
       "      <td>0.5</td>\n",
       "      <td>30</td>\n",
       "      <td>0.718285</td>\n",
       "      <td>0.984722</td>\n",
       "      <td>None</td>\n",
       "      <td>0.001</td>\n",
       "      <td>AttentionedYoonKimModel(\\n  (embedding): Linea...</td>\n",
       "      <td>AttentionedYoonKim</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.010</td>\n",
       "      <td>IMDB binary classification</td>\n",
       "      <td>298416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>0.72252</td>\n",
       "      <td>0.984889</td>\n",
       "      <td>MAX_WORD_LEN = 8, MAX_TEXT_LEN = 256</td>\n",
       "      <td>0.5</td>\n",
       "      <td>30</td>\n",
       "      <td>0.702006</td>\n",
       "      <td>0.984722</td>\n",
       "      <td>None</td>\n",
       "      <td>0.001</td>\n",
       "      <td>AttentionedYoonKimModel(\\n  (embedding): Linea...</td>\n",
       "      <td>AttentionedYoonKim</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.010</td>\n",
       "      <td>IMDB binary classification</td>\n",
       "      <td>298416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>0.70428</td>\n",
       "      <td>0.984889</td>\n",
       "      <td>MAX_WORD_LEN = 8, MAX_TEXT_LEN = 256</td>\n",
       "      <td>0.5</td>\n",
       "      <td>30</td>\n",
       "      <td>0.679943</td>\n",
       "      <td>0.984722</td>\n",
       "      <td>None</td>\n",
       "      <td>0.001</td>\n",
       "      <td>AttentionedYoonKimModel(\\n  (embedding): Linea...</td>\n",
       "      <td>AttentionedYoonKim</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.010</td>\n",
       "      <td>IMDB binary classification</td>\n",
       "      <td>298416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>0.68124</td>\n",
       "      <td>0.984889</td>\n",
       "      <td>MAX_WORD_LEN = 8, MAX_TEXT_LEN = 256</td>\n",
       "      <td>0.5</td>\n",
       "      <td>30</td>\n",
       "      <td>0.651994</td>\n",
       "      <td>0.984722</td>\n",
       "      <td>None</td>\n",
       "      <td>0.001</td>\n",
       "      <td>AttentionedYoonKimModel(\\n  (embedding): Linea...</td>\n",
       "      <td>AttentionedYoonKim</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.010</td>\n",
       "      <td>IMDB binary classification</td>\n",
       "      <td>298416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>0.67236</td>\n",
       "      <td>0.984889</td>\n",
       "      <td>MAX_WORD_LEN = 8, MAX_TEXT_LEN = 256</td>\n",
       "      <td>0.5</td>\n",
       "      <td>30</td>\n",
       "      <td>0.643575</td>\n",
       "      <td>0.984722</td>\n",
       "      <td>None</td>\n",
       "      <td>0.001</td>\n",
       "      <td>AttentionedYoonKimModel(\\n  (embedding): Linea...</td>\n",
       "      <td>AttentionedYoonKim</td>\n",
       "      <td>0.175</td>\n",
       "      <td>0.010</td>\n",
       "      <td>IMDB binary classification</td>\n",
       "      <td>298416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>0.65240</td>\n",
       "      <td>0.984889</td>\n",
       "      <td>MAX_WORD_LEN = 8, MAX_TEXT_LEN = 256</td>\n",
       "      <td>0.5</td>\n",
       "      <td>30</td>\n",
       "      <td>0.614942</td>\n",
       "      <td>0.984722</td>\n",
       "      <td>None</td>\n",
       "      <td>0.001</td>\n",
       "      <td>AttentionedYoonKimModel(\\n  (embedding): Linea...</td>\n",
       "      <td>AttentionedYoonKim</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.010</td>\n",
       "      <td>IMDB binary classification</td>\n",
       "      <td>298416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>0.81180</td>\n",
       "      <td>0.979378</td>\n",
       "      <td>MAX_WORD_LEN = 8, MAX_TEXT_LEN = 256</td>\n",
       "      <td>0.5</td>\n",
       "      <td>30</td>\n",
       "      <td>0.810778</td>\n",
       "      <td>0.979339</td>\n",
       "      <td>None</td>\n",
       "      <td>0.001</td>\n",
       "      <td>AttentionedYoonKimModel(\\n  (embedding): Linea...</td>\n",
       "      <td>AttentionedYoonKim</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.025</td>\n",
       "      <td>IMDB binary classification</td>\n",
       "      <td>298416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>0.80708</td>\n",
       "      <td>0.979378</td>\n",
       "      <td>MAX_WORD_LEN = 8, MAX_TEXT_LEN = 256</td>\n",
       "      <td>0.5</td>\n",
       "      <td>30</td>\n",
       "      <td>0.805720</td>\n",
       "      <td>0.979339</td>\n",
       "      <td>None</td>\n",
       "      <td>0.001</td>\n",
       "      <td>AttentionedYoonKimModel(\\n  (embedding): Linea...</td>\n",
       "      <td>AttentionedYoonKim</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.025</td>\n",
       "      <td>IMDB binary classification</td>\n",
       "      <td>298416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>0.80208</td>\n",
       "      <td>0.979378</td>\n",
       "      <td>MAX_WORD_LEN = 8, MAX_TEXT_LEN = 256</td>\n",
       "      <td>0.5</td>\n",
       "      <td>30</td>\n",
       "      <td>0.801061</td>\n",
       "      <td>0.979339</td>\n",
       "      <td>None</td>\n",
       "      <td>0.001</td>\n",
       "      <td>AttentionedYoonKimModel(\\n  (embedding): Linea...</td>\n",
       "      <td>AttentionedYoonKim</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.025</td>\n",
       "      <td>IMDB binary classification</td>\n",
       "      <td>298416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>0.78724</td>\n",
       "      <td>0.979378</td>\n",
       "      <td>MAX_WORD_LEN = 8, MAX_TEXT_LEN = 256</td>\n",
       "      <td>0.5</td>\n",
       "      <td>30</td>\n",
       "      <td>0.786068</td>\n",
       "      <td>0.979339</td>\n",
       "      <td>None</td>\n",
       "      <td>0.001</td>\n",
       "      <td>AttentionedYoonKimModel(\\n  (embedding): Linea...</td>\n",
       "      <td>AttentionedYoonKim</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.025</td>\n",
       "      <td>IMDB binary classification</td>\n",
       "      <td>298416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>0.76688</td>\n",
       "      <td>0.979378</td>\n",
       "      <td>MAX_WORD_LEN = 8, MAX_TEXT_LEN = 256</td>\n",
       "      <td>0.5</td>\n",
       "      <td>30</td>\n",
       "      <td>0.766450</td>\n",
       "      <td>0.979339</td>\n",
       "      <td>None</td>\n",
       "      <td>0.001</td>\n",
       "      <td>AttentionedYoonKimModel(\\n  (embedding): Linea...</td>\n",
       "      <td>AttentionedYoonKim</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.025</td>\n",
       "      <td>IMDB binary classification</td>\n",
       "      <td>298416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>0.75596</td>\n",
       "      <td>0.979378</td>\n",
       "      <td>MAX_WORD_LEN = 8, MAX_TEXT_LEN = 256</td>\n",
       "      <td>0.5</td>\n",
       "      <td>30</td>\n",
       "      <td>0.755657</td>\n",
       "      <td>0.979339</td>\n",
       "      <td>None</td>\n",
       "      <td>0.001</td>\n",
       "      <td>AttentionedYoonKimModel(\\n  (embedding): Linea...</td>\n",
       "      <td>AttentionedYoonKim</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.025</td>\n",
       "      <td>IMDB binary classification</td>\n",
       "      <td>298416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>0.74260</td>\n",
       "      <td>0.979378</td>\n",
       "      <td>MAX_WORD_LEN = 8, MAX_TEXT_LEN = 256</td>\n",
       "      <td>0.5</td>\n",
       "      <td>30</td>\n",
       "      <td>0.743227</td>\n",
       "      <td>0.979339</td>\n",
       "      <td>None</td>\n",
       "      <td>0.001</td>\n",
       "      <td>AttentionedYoonKimModel(\\n  (embedding): Linea...</td>\n",
       "      <td>AttentionedYoonKim</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.025</td>\n",
       "      <td>IMDB binary classification</td>\n",
       "      <td>298416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>0.72836</td>\n",
       "      <td>0.979378</td>\n",
       "      <td>MAX_WORD_LEN = 8, MAX_TEXT_LEN = 256</td>\n",
       "      <td>0.5</td>\n",
       "      <td>30</td>\n",
       "      <td>0.731719</td>\n",
       "      <td>0.979339</td>\n",
       "      <td>None</td>\n",
       "      <td>0.001</td>\n",
       "      <td>AttentionedYoonKimModel(\\n  (embedding): Linea...</td>\n",
       "      <td>AttentionedYoonKim</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.025</td>\n",
       "      <td>IMDB binary classification</td>\n",
       "      <td>298416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>0.71012</td>\n",
       "      <td>0.979378</td>\n",
       "      <td>MAX_WORD_LEN = 8, MAX_TEXT_LEN = 256</td>\n",
       "      <td>0.5</td>\n",
       "      <td>30</td>\n",
       "      <td>0.714719</td>\n",
       "      <td>0.979339</td>\n",
       "      <td>None</td>\n",
       "      <td>0.001</td>\n",
       "      <td>AttentionedYoonKimModel(\\n  (embedding): Linea...</td>\n",
       "      <td>AttentionedYoonKim</td>\n",
       "      <td>0.175</td>\n",
       "      <td>0.025</td>\n",
       "      <td>IMDB binary classification</td>\n",
       "      <td>298416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>0.69392</td>\n",
       "      <td>0.979378</td>\n",
       "      <td>MAX_WORD_LEN = 8, MAX_TEXT_LEN = 256</td>\n",
       "      <td>0.5</td>\n",
       "      <td>30</td>\n",
       "      <td>0.701909</td>\n",
       "      <td>0.979339</td>\n",
       "      <td>None</td>\n",
       "      <td>0.001</td>\n",
       "      <td>AttentionedYoonKimModel(\\n  (embedding): Linea...</td>\n",
       "      <td>AttentionedYoonKim</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.025</td>\n",
       "      <td>IMDB binary classification</td>\n",
       "      <td>298416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>0.79716</td>\n",
       "      <td>0.894311</td>\n",
       "      <td>MAX_WORD_LEN = 8, MAX_TEXT_LEN = 256</td>\n",
       "      <td>0.5</td>\n",
       "      <td>30</td>\n",
       "      <td>0.800346</td>\n",
       "      <td>0.896212</td>\n",
       "      <td>None</td>\n",
       "      <td>0.001</td>\n",
       "      <td>AttentionedYoonKimModel(\\n  (embedding): Linea...</td>\n",
       "      <td>AttentionedYoonKim</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.150</td>\n",
       "      <td>IMDB binary classification</td>\n",
       "      <td>298416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>0.79416</td>\n",
       "      <td>0.894311</td>\n",
       "      <td>MAX_WORD_LEN = 8, MAX_TEXT_LEN = 256</td>\n",
       "      <td>0.5</td>\n",
       "      <td>30</td>\n",
       "      <td>0.797609</td>\n",
       "      <td>0.896212</td>\n",
       "      <td>None</td>\n",
       "      <td>0.001</td>\n",
       "      <td>AttentionedYoonKimModel(\\n  (embedding): Linea...</td>\n",
       "      <td>AttentionedYoonKim</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.150</td>\n",
       "      <td>IMDB binary classification</td>\n",
       "      <td>298416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>0.78868</td>\n",
       "      <td>0.894311</td>\n",
       "      <td>MAX_WORD_LEN = 8, MAX_TEXT_LEN = 256</td>\n",
       "      <td>0.5</td>\n",
       "      <td>30</td>\n",
       "      <td>0.792685</td>\n",
       "      <td>0.896212</td>\n",
       "      <td>None</td>\n",
       "      <td>0.001</td>\n",
       "      <td>AttentionedYoonKimModel(\\n  (embedding): Linea...</td>\n",
       "      <td>AttentionedYoonKim</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.150</td>\n",
       "      <td>IMDB binary classification</td>\n",
       "      <td>298416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>0.78204</td>\n",
       "      <td>0.894311</td>\n",
       "      <td>MAX_WORD_LEN = 8, MAX_TEXT_LEN = 256</td>\n",
       "      <td>0.5</td>\n",
       "      <td>30</td>\n",
       "      <td>0.786707</td>\n",
       "      <td>0.896212</td>\n",
       "      <td>None</td>\n",
       "      <td>0.001</td>\n",
       "      <td>AttentionedYoonKimModel(\\n  (embedding): Linea...</td>\n",
       "      <td>AttentionedYoonKim</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.150</td>\n",
       "      <td>IMDB binary classification</td>\n",
       "      <td>298416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>0.76808</td>\n",
       "      <td>0.894311</td>\n",
       "      <td>MAX_WORD_LEN = 8, MAX_TEXT_LEN = 256</td>\n",
       "      <td>0.5</td>\n",
       "      <td>30</td>\n",
       "      <td>0.773339</td>\n",
       "      <td>0.896212</td>\n",
       "      <td>None</td>\n",
       "      <td>0.001</td>\n",
       "      <td>AttentionedYoonKimModel(\\n  (embedding): Linea...</td>\n",
       "      <td>AttentionedYoonKim</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.150</td>\n",
       "      <td>IMDB binary classification</td>\n",
       "      <td>298416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>0.76208</td>\n",
       "      <td>0.894311</td>\n",
       "      <td>MAX_WORD_LEN = 8, MAX_TEXT_LEN = 256</td>\n",
       "      <td>0.5</td>\n",
       "      <td>30</td>\n",
       "      <td>0.768686</td>\n",
       "      <td>0.896212</td>\n",
       "      <td>None</td>\n",
       "      <td>0.001</td>\n",
       "      <td>AttentionedYoonKimModel(\\n  (embedding): Linea...</td>\n",
       "      <td>AttentionedYoonKim</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.150</td>\n",
       "      <td>IMDB binary classification</td>\n",
       "      <td>298416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>0.74852</td>\n",
       "      <td>0.894311</td>\n",
       "      <td>MAX_WORD_LEN = 8, MAX_TEXT_LEN = 256</td>\n",
       "      <td>0.5</td>\n",
       "      <td>30</td>\n",
       "      <td>0.756780</td>\n",
       "      <td>0.896212</td>\n",
       "      <td>None</td>\n",
       "      <td>0.001</td>\n",
       "      <td>AttentionedYoonKimModel(\\n  (embedding): Linea...</td>\n",
       "      <td>AttentionedYoonKim</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.150</td>\n",
       "      <td>IMDB binary classification</td>\n",
       "      <td>298416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>0.73620</td>\n",
       "      <td>0.894311</td>\n",
       "      <td>MAX_WORD_LEN = 8, MAX_TEXT_LEN = 256</td>\n",
       "      <td>0.5</td>\n",
       "      <td>30</td>\n",
       "      <td>0.746707</td>\n",
       "      <td>0.896212</td>\n",
       "      <td>None</td>\n",
       "      <td>0.001</td>\n",
       "      <td>AttentionedYoonKimModel(\\n  (embedding): Linea...</td>\n",
       "      <td>AttentionedYoonKim</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.150</td>\n",
       "      <td>IMDB binary classification</td>\n",
       "      <td>298416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>0.72872</td>\n",
       "      <td>0.894311</td>\n",
       "      <td>MAX_WORD_LEN = 8, MAX_TEXT_LEN = 256</td>\n",
       "      <td>0.5</td>\n",
       "      <td>30</td>\n",
       "      <td>0.740113</td>\n",
       "      <td>0.896212</td>\n",
       "      <td>None</td>\n",
       "      <td>0.001</td>\n",
       "      <td>AttentionedYoonKimModel(\\n  (embedding): Linea...</td>\n",
       "      <td>AttentionedYoonKim</td>\n",
       "      <td>0.175</td>\n",
       "      <td>0.150</td>\n",
       "      <td>IMDB binary classification</td>\n",
       "      <td>298416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>0.71140</td>\n",
       "      <td>0.894311</td>\n",
       "      <td>MAX_WORD_LEN = 8, MAX_TEXT_LEN = 256</td>\n",
       "      <td>0.5</td>\n",
       "      <td>30</td>\n",
       "      <td>0.725926</td>\n",
       "      <td>0.896212</td>\n",
       "      <td>None</td>\n",
       "      <td>0.001</td>\n",
       "      <td>AttentionedYoonKimModel(\\n  (embedding): Linea...</td>\n",
       "      <td>AttentionedYoonKim</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.150</td>\n",
       "      <td>IMDB binary classification</td>\n",
       "      <td>298416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>0.77540</td>\n",
       "      <td>0.883556</td>\n",
       "      <td>MAX_WORD_LEN = 8, MAX_TEXT_LEN = 256</td>\n",
       "      <td>0.5</td>\n",
       "      <td>30</td>\n",
       "      <td>0.780878</td>\n",
       "      <td>0.886619</td>\n",
       "      <td>None</td>\n",
       "      <td>0.001</td>\n",
       "      <td>AttentionedYoonKimModel(\\n  (embedding): Linea...</td>\n",
       "      <td>AttentionedYoonKim</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.175</td>\n",
       "      <td>IMDB binary classification</td>\n",
       "      <td>298416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>0.77196</td>\n",
       "      <td>0.883556</td>\n",
       "      <td>MAX_WORD_LEN = 8, MAX_TEXT_LEN = 256</td>\n",
       "      <td>0.5</td>\n",
       "      <td>30</td>\n",
       "      <td>0.777903</td>\n",
       "      <td>0.886619</td>\n",
       "      <td>None</td>\n",
       "      <td>0.001</td>\n",
       "      <td>AttentionedYoonKimModel(\\n  (embedding): Linea...</td>\n",
       "      <td>AttentionedYoonKim</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.175</td>\n",
       "      <td>IMDB binary classification</td>\n",
       "      <td>298416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>0.76844</td>\n",
       "      <td>0.883556</td>\n",
       "      <td>MAX_WORD_LEN = 8, MAX_TEXT_LEN = 256</td>\n",
       "      <td>0.5</td>\n",
       "      <td>30</td>\n",
       "      <td>0.774475</td>\n",
       "      <td>0.886619</td>\n",
       "      <td>None</td>\n",
       "      <td>0.001</td>\n",
       "      <td>AttentionedYoonKimModel(\\n  (embedding): Linea...</td>\n",
       "      <td>AttentionedYoonKim</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.175</td>\n",
       "      <td>IMDB binary classification</td>\n",
       "      <td>298416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>0.76024</td>\n",
       "      <td>0.883556</td>\n",
       "      <td>MAX_WORD_LEN = 8, MAX_TEXT_LEN = 256</td>\n",
       "      <td>0.5</td>\n",
       "      <td>30</td>\n",
       "      <td>0.766461</td>\n",
       "      <td>0.886619</td>\n",
       "      <td>None</td>\n",
       "      <td>0.001</td>\n",
       "      <td>AttentionedYoonKimModel(\\n  (embedding): Linea...</td>\n",
       "      <td>AttentionedYoonKim</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.175</td>\n",
       "      <td>IMDB binary classification</td>\n",
       "      <td>298416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>0.75036</td>\n",
       "      <td>0.883556</td>\n",
       "      <td>MAX_WORD_LEN = 8, MAX_TEXT_LEN = 256</td>\n",
       "      <td>0.5</td>\n",
       "      <td>30</td>\n",
       "      <td>0.757207</td>\n",
       "      <td>0.886619</td>\n",
       "      <td>None</td>\n",
       "      <td>0.001</td>\n",
       "      <td>AttentionedYoonKimModel(\\n  (embedding): Linea...</td>\n",
       "      <td>AttentionedYoonKim</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.175</td>\n",
       "      <td>IMDB binary classification</td>\n",
       "      <td>298416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>0.74504</td>\n",
       "      <td>0.883556</td>\n",
       "      <td>MAX_WORD_LEN = 8, MAX_TEXT_LEN = 256</td>\n",
       "      <td>0.5</td>\n",
       "      <td>30</td>\n",
       "      <td>0.752312</td>\n",
       "      <td>0.886619</td>\n",
       "      <td>None</td>\n",
       "      <td>0.001</td>\n",
       "      <td>AttentionedYoonKimModel(\\n  (embedding): Linea...</td>\n",
       "      <td>AttentionedYoonKim</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.175</td>\n",
       "      <td>IMDB binary classification</td>\n",
       "      <td>298416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>0.73468</td>\n",
       "      <td>0.883556</td>\n",
       "      <td>MAX_WORD_LEN = 8, MAX_TEXT_LEN = 256</td>\n",
       "      <td>0.5</td>\n",
       "      <td>30</td>\n",
       "      <td>0.742478</td>\n",
       "      <td>0.886619</td>\n",
       "      <td>None</td>\n",
       "      <td>0.001</td>\n",
       "      <td>AttentionedYoonKimModel(\\n  (embedding): Linea...</td>\n",
       "      <td>AttentionedYoonKim</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.175</td>\n",
       "      <td>IMDB binary classification</td>\n",
       "      <td>298416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>0.72680</td>\n",
       "      <td>0.883556</td>\n",
       "      <td>MAX_WORD_LEN = 8, MAX_TEXT_LEN = 256</td>\n",
       "      <td>0.5</td>\n",
       "      <td>30</td>\n",
       "      <td>0.736334</td>\n",
       "      <td>0.886619</td>\n",
       "      <td>None</td>\n",
       "      <td>0.001</td>\n",
       "      <td>AttentionedYoonKimModel(\\n  (embedding): Linea...</td>\n",
       "      <td>AttentionedYoonKim</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.175</td>\n",
       "      <td>IMDB binary classification</td>\n",
       "      <td>298416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>0.71676</td>\n",
       "      <td>0.883556</td>\n",
       "      <td>MAX_WORD_LEN = 8, MAX_TEXT_LEN = 256</td>\n",
       "      <td>0.5</td>\n",
       "      <td>30</td>\n",
       "      <td>0.725893</td>\n",
       "      <td>0.886619</td>\n",
       "      <td>None</td>\n",
       "      <td>0.001</td>\n",
       "      <td>AttentionedYoonKimModel(\\n  (embedding): Linea...</td>\n",
       "      <td>AttentionedYoonKim</td>\n",
       "      <td>0.175</td>\n",
       "      <td>0.175</td>\n",
       "      <td>IMDB binary classification</td>\n",
       "      <td>298416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>0.70712</td>\n",
       "      <td>0.883556</td>\n",
       "      <td>MAX_WORD_LEN = 8, MAX_TEXT_LEN = 256</td>\n",
       "      <td>0.5</td>\n",
       "      <td>30</td>\n",
       "      <td>0.716838</td>\n",
       "      <td>0.886619</td>\n",
       "      <td>None</td>\n",
       "      <td>0.001</td>\n",
       "      <td>AttentionedYoonKimModel(\\n  (embedding): Linea...</td>\n",
       "      <td>AttentionedYoonKim</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.175</td>\n",
       "      <td>IMDB binary classification</td>\n",
       "      <td>298416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>0.72200</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>MAX_WORD_LEN = 8, MAX_TEXT_LEN = 256</td>\n",
       "      <td>0.5</td>\n",
       "      <td>30</td>\n",
       "      <td>0.719351</td>\n",
       "      <td>0.865652</td>\n",
       "      <td>None</td>\n",
       "      <td>0.001</td>\n",
       "      <td>AttentionedYoonKimModel(\\n  (embedding): Linea...</td>\n",
       "      <td>AttentionedYoonKim</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.200</td>\n",
       "      <td>IMDB binary classification</td>\n",
       "      <td>298416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>0.71828</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>MAX_WORD_LEN = 8, MAX_TEXT_LEN = 256</td>\n",
       "      <td>0.5</td>\n",
       "      <td>30</td>\n",
       "      <td>0.715767</td>\n",
       "      <td>0.865652</td>\n",
       "      <td>None</td>\n",
       "      <td>0.001</td>\n",
       "      <td>AttentionedYoonKimModel(\\n  (embedding): Linea...</td>\n",
       "      <td>AttentionedYoonKim</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.200</td>\n",
       "      <td>IMDB binary classification</td>\n",
       "      <td>298416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>0.71216</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>MAX_WORD_LEN = 8, MAX_TEXT_LEN = 256</td>\n",
       "      <td>0.5</td>\n",
       "      <td>30</td>\n",
       "      <td>0.709111</td>\n",
       "      <td>0.865652</td>\n",
       "      <td>None</td>\n",
       "      <td>0.001</td>\n",
       "      <td>AttentionedYoonKimModel(\\n  (embedding): Linea...</td>\n",
       "      <td>AttentionedYoonKim</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.200</td>\n",
       "      <td>IMDB binary classification</td>\n",
       "      <td>298416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>0.70480</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>MAX_WORD_LEN = 8, MAX_TEXT_LEN = 256</td>\n",
       "      <td>0.5</td>\n",
       "      <td>30</td>\n",
       "      <td>0.701263</td>\n",
       "      <td>0.865652</td>\n",
       "      <td>None</td>\n",
       "      <td>0.001</td>\n",
       "      <td>AttentionedYoonKimModel(\\n  (embedding): Linea...</td>\n",
       "      <td>AttentionedYoonKim</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.200</td>\n",
       "      <td>IMDB binary classification</td>\n",
       "      <td>298416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>0.69164</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>MAX_WORD_LEN = 8, MAX_TEXT_LEN = 256</td>\n",
       "      <td>0.5</td>\n",
       "      <td>30</td>\n",
       "      <td>0.688109</td>\n",
       "      <td>0.865652</td>\n",
       "      <td>None</td>\n",
       "      <td>0.001</td>\n",
       "      <td>AttentionedYoonKimModel(\\n  (embedding): Linea...</td>\n",
       "      <td>AttentionedYoonKim</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.200</td>\n",
       "      <td>IMDB binary classification</td>\n",
       "      <td>298416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>0.68248</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>MAX_WORD_LEN = 8, MAX_TEXT_LEN = 256</td>\n",
       "      <td>0.5</td>\n",
       "      <td>30</td>\n",
       "      <td>0.679376</td>\n",
       "      <td>0.865652</td>\n",
       "      <td>None</td>\n",
       "      <td>0.001</td>\n",
       "      <td>AttentionedYoonKimModel(\\n  (embedding): Linea...</td>\n",
       "      <td>AttentionedYoonKim</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.200</td>\n",
       "      <td>IMDB binary classification</td>\n",
       "      <td>298416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>0.67208</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>MAX_WORD_LEN = 8, MAX_TEXT_LEN = 256</td>\n",
       "      <td>0.5</td>\n",
       "      <td>30</td>\n",
       "      <td>0.668687</td>\n",
       "      <td>0.865652</td>\n",
       "      <td>None</td>\n",
       "      <td>0.001</td>\n",
       "      <td>AttentionedYoonKimModel(\\n  (embedding): Linea...</td>\n",
       "      <td>AttentionedYoonKim</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.200</td>\n",
       "      <td>IMDB binary classification</td>\n",
       "      <td>298416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>0.66332</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>MAX_WORD_LEN = 8, MAX_TEXT_LEN = 256</td>\n",
       "      <td>0.5</td>\n",
       "      <td>30</td>\n",
       "      <td>0.659245</td>\n",
       "      <td>0.865652</td>\n",
       "      <td>None</td>\n",
       "      <td>0.001</td>\n",
       "      <td>AttentionedYoonKimModel(\\n  (embedding): Linea...</td>\n",
       "      <td>AttentionedYoonKim</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.200</td>\n",
       "      <td>IMDB binary classification</td>\n",
       "      <td>298416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>0.65204</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>MAX_WORD_LEN = 8, MAX_TEXT_LEN = 256</td>\n",
       "      <td>0.5</td>\n",
       "      <td>30</td>\n",
       "      <td>0.647971</td>\n",
       "      <td>0.865652</td>\n",
       "      <td>None</td>\n",
       "      <td>0.001</td>\n",
       "      <td>AttentionedYoonKimModel(\\n  (embedding): Linea...</td>\n",
       "      <td>AttentionedYoonKim</td>\n",
       "      <td>0.175</td>\n",
       "      <td>0.200</td>\n",
       "      <td>IMDB binary classification</td>\n",
       "      <td>298416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>0.64168</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>MAX_WORD_LEN = 8, MAX_TEXT_LEN = 256</td>\n",
       "      <td>0.5</td>\n",
       "      <td>30</td>\n",
       "      <td>0.638061</td>\n",
       "      <td>0.865652</td>\n",
       "      <td>None</td>\n",
       "      <td>0.001</td>\n",
       "      <td>AttentionedYoonKimModel(\\n  (embedding): Linea...</td>\n",
       "      <td>AttentionedYoonKim</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.200</td>\n",
       "      <td>IMDB binary classification</td>\n",
       "      <td>298416</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     acc_test  acc_train                             data_desc  dropout  \\\n",
       "120   0.72036   0.983822  MAX_WORD_LEN = 8, MAX_TEXT_LEN = 256      0.5   \n",
       "121   0.71260   0.983822  MAX_WORD_LEN = 8, MAX_TEXT_LEN = 256      0.5   \n",
       "122   0.68996   0.983822  MAX_WORD_LEN = 8, MAX_TEXT_LEN = 256      0.5   \n",
       "123   0.67116   0.983822  MAX_WORD_LEN = 8, MAX_TEXT_LEN = 256      0.5   \n",
       "124   0.65020   0.983822  MAX_WORD_LEN = 8, MAX_TEXT_LEN = 256      0.5   \n",
       "125   0.62444   0.983822  MAX_WORD_LEN = 8, MAX_TEXT_LEN = 256      0.5   \n",
       "126   0.60816   0.983822  MAX_WORD_LEN = 8, MAX_TEXT_LEN = 256      0.5   \n",
       "127   0.59036   0.983822  MAX_WORD_LEN = 8, MAX_TEXT_LEN = 256      0.5   \n",
       "128   0.57716   0.983822  MAX_WORD_LEN = 8, MAX_TEXT_LEN = 256      0.5   \n",
       "129   0.56752   0.983822  MAX_WORD_LEN = 8, MAX_TEXT_LEN = 256      0.5   \n",
       "130   0.78628   0.984889  MAX_WORD_LEN = 8, MAX_TEXT_LEN = 256      0.5   \n",
       "131   0.77724   0.984889  MAX_WORD_LEN = 8, MAX_TEXT_LEN = 256      0.5   \n",
       "132   0.76576   0.984889  MAX_WORD_LEN = 8, MAX_TEXT_LEN = 256      0.5   \n",
       "133   0.75360   0.984889  MAX_WORD_LEN = 8, MAX_TEXT_LEN = 256      0.5   \n",
       "134   0.73672   0.984889  MAX_WORD_LEN = 8, MAX_TEXT_LEN = 256      0.5   \n",
       "135   0.72252   0.984889  MAX_WORD_LEN = 8, MAX_TEXT_LEN = 256      0.5   \n",
       "136   0.70428   0.984889  MAX_WORD_LEN = 8, MAX_TEXT_LEN = 256      0.5   \n",
       "137   0.68124   0.984889  MAX_WORD_LEN = 8, MAX_TEXT_LEN = 256      0.5   \n",
       "138   0.67236   0.984889  MAX_WORD_LEN = 8, MAX_TEXT_LEN = 256      0.5   \n",
       "139   0.65240   0.984889  MAX_WORD_LEN = 8, MAX_TEXT_LEN = 256      0.5   \n",
       "140   0.81180   0.979378  MAX_WORD_LEN = 8, MAX_TEXT_LEN = 256      0.5   \n",
       "141   0.80708   0.979378  MAX_WORD_LEN = 8, MAX_TEXT_LEN = 256      0.5   \n",
       "142   0.80208   0.979378  MAX_WORD_LEN = 8, MAX_TEXT_LEN = 256      0.5   \n",
       "143   0.78724   0.979378  MAX_WORD_LEN = 8, MAX_TEXT_LEN = 256      0.5   \n",
       "144   0.76688   0.979378  MAX_WORD_LEN = 8, MAX_TEXT_LEN = 256      0.5   \n",
       "145   0.75596   0.979378  MAX_WORD_LEN = 8, MAX_TEXT_LEN = 256      0.5   \n",
       "146   0.74260   0.979378  MAX_WORD_LEN = 8, MAX_TEXT_LEN = 256      0.5   \n",
       "147   0.72836   0.979378  MAX_WORD_LEN = 8, MAX_TEXT_LEN = 256      0.5   \n",
       "148   0.71012   0.979378  MAX_WORD_LEN = 8, MAX_TEXT_LEN = 256      0.5   \n",
       "149   0.69392   0.979378  MAX_WORD_LEN = 8, MAX_TEXT_LEN = 256      0.5   \n",
       "..        ...        ...                                   ...      ...   \n",
       "190   0.79716   0.894311  MAX_WORD_LEN = 8, MAX_TEXT_LEN = 256      0.5   \n",
       "191   0.79416   0.894311  MAX_WORD_LEN = 8, MAX_TEXT_LEN = 256      0.5   \n",
       "192   0.78868   0.894311  MAX_WORD_LEN = 8, MAX_TEXT_LEN = 256      0.5   \n",
       "193   0.78204   0.894311  MAX_WORD_LEN = 8, MAX_TEXT_LEN = 256      0.5   \n",
       "194   0.76808   0.894311  MAX_WORD_LEN = 8, MAX_TEXT_LEN = 256      0.5   \n",
       "195   0.76208   0.894311  MAX_WORD_LEN = 8, MAX_TEXT_LEN = 256      0.5   \n",
       "196   0.74852   0.894311  MAX_WORD_LEN = 8, MAX_TEXT_LEN = 256      0.5   \n",
       "197   0.73620   0.894311  MAX_WORD_LEN = 8, MAX_TEXT_LEN = 256      0.5   \n",
       "198   0.72872   0.894311  MAX_WORD_LEN = 8, MAX_TEXT_LEN = 256      0.5   \n",
       "199   0.71140   0.894311  MAX_WORD_LEN = 8, MAX_TEXT_LEN = 256      0.5   \n",
       "200   0.77540   0.883556  MAX_WORD_LEN = 8, MAX_TEXT_LEN = 256      0.5   \n",
       "201   0.77196   0.883556  MAX_WORD_LEN = 8, MAX_TEXT_LEN = 256      0.5   \n",
       "202   0.76844   0.883556  MAX_WORD_LEN = 8, MAX_TEXT_LEN = 256      0.5   \n",
       "203   0.76024   0.883556  MAX_WORD_LEN = 8, MAX_TEXT_LEN = 256      0.5   \n",
       "204   0.75036   0.883556  MAX_WORD_LEN = 8, MAX_TEXT_LEN = 256      0.5   \n",
       "205   0.74504   0.883556  MAX_WORD_LEN = 8, MAX_TEXT_LEN = 256      0.5   \n",
       "206   0.73468   0.883556  MAX_WORD_LEN = 8, MAX_TEXT_LEN = 256      0.5   \n",
       "207   0.72680   0.883556  MAX_WORD_LEN = 8, MAX_TEXT_LEN = 256      0.5   \n",
       "208   0.71676   0.883556  MAX_WORD_LEN = 8, MAX_TEXT_LEN = 256      0.5   \n",
       "209   0.70712   0.883556  MAX_WORD_LEN = 8, MAX_TEXT_LEN = 256      0.5   \n",
       "210   0.72200   0.866667  MAX_WORD_LEN = 8, MAX_TEXT_LEN = 256      0.5   \n",
       "211   0.71828   0.866667  MAX_WORD_LEN = 8, MAX_TEXT_LEN = 256      0.5   \n",
       "212   0.71216   0.866667  MAX_WORD_LEN = 8, MAX_TEXT_LEN = 256      0.5   \n",
       "213   0.70480   0.866667  MAX_WORD_LEN = 8, MAX_TEXT_LEN = 256      0.5   \n",
       "214   0.69164   0.866667  MAX_WORD_LEN = 8, MAX_TEXT_LEN = 256      0.5   \n",
       "215   0.68248   0.866667  MAX_WORD_LEN = 8, MAX_TEXT_LEN = 256      0.5   \n",
       "216   0.67208   0.866667  MAX_WORD_LEN = 8, MAX_TEXT_LEN = 256      0.5   \n",
       "217   0.66332   0.866667  MAX_WORD_LEN = 8, MAX_TEXT_LEN = 256      0.5   \n",
       "218   0.65204   0.866667  MAX_WORD_LEN = 8, MAX_TEXT_LEN = 256      0.5   \n",
       "219   0.64168   0.866667  MAX_WORD_LEN = 8, MAX_TEXT_LEN = 256      0.5   \n",
       "\n",
       "     epochs   f1_test  f1_train init_function     lr  \\\n",
       "120      30  0.724079  0.984064          None  0.001   \n",
       "121      30  0.718268  0.984064          None  0.001   \n",
       "122      30  0.698815  0.984064          None  0.001   \n",
       "123      30  0.683162  0.984064          None  0.001   \n",
       "124      30  0.669013  0.984064          None  0.001   \n",
       "125      30  0.649363  0.984064          None  0.001   \n",
       "126      30  0.639137  0.984064          None  0.001   \n",
       "127      30  0.623257  0.984064          None  0.001   \n",
       "128      30  0.614464  0.984064          None  0.001   \n",
       "129      30  0.612556  0.984064          None  0.001   \n",
       "130      30  0.776228  0.984722          None  0.001   \n",
       "131      30  0.765900  0.984722          None  0.001   \n",
       "132      30  0.753036  0.984722          None  0.001   \n",
       "133      30  0.739270  0.984722          None  0.001   \n",
       "134      30  0.718285  0.984722          None  0.001   \n",
       "135      30  0.702006  0.984722          None  0.001   \n",
       "136      30  0.679943  0.984722          None  0.001   \n",
       "137      30  0.651994  0.984722          None  0.001   \n",
       "138      30  0.643575  0.984722          None  0.001   \n",
       "139      30  0.614942  0.984722          None  0.001   \n",
       "140      30  0.810778  0.979339          None  0.001   \n",
       "141      30  0.805720  0.979339          None  0.001   \n",
       "142      30  0.801061  0.979339          None  0.001   \n",
       "143      30  0.786068  0.979339          None  0.001   \n",
       "144      30  0.766450  0.979339          None  0.001   \n",
       "145      30  0.755657  0.979339          None  0.001   \n",
       "146      30  0.743227  0.979339          None  0.001   \n",
       "147      30  0.731719  0.979339          None  0.001   \n",
       "148      30  0.714719  0.979339          None  0.001   \n",
       "149      30  0.701909  0.979339          None  0.001   \n",
       "..      ...       ...       ...           ...    ...   \n",
       "190      30  0.800346  0.896212          None  0.001   \n",
       "191      30  0.797609  0.896212          None  0.001   \n",
       "192      30  0.792685  0.896212          None  0.001   \n",
       "193      30  0.786707  0.896212          None  0.001   \n",
       "194      30  0.773339  0.896212          None  0.001   \n",
       "195      30  0.768686  0.896212          None  0.001   \n",
       "196      30  0.756780  0.896212          None  0.001   \n",
       "197      30  0.746707  0.896212          None  0.001   \n",
       "198      30  0.740113  0.896212          None  0.001   \n",
       "199      30  0.725926  0.896212          None  0.001   \n",
       "200      30  0.780878  0.886619          None  0.001   \n",
       "201      30  0.777903  0.886619          None  0.001   \n",
       "202      30  0.774475  0.886619          None  0.001   \n",
       "203      30  0.766461  0.886619          None  0.001   \n",
       "204      30  0.757207  0.886619          None  0.001   \n",
       "205      30  0.752312  0.886619          None  0.001   \n",
       "206      30  0.742478  0.886619          None  0.001   \n",
       "207      30  0.736334  0.886619          None  0.001   \n",
       "208      30  0.725893  0.886619          None  0.001   \n",
       "209      30  0.716838  0.886619          None  0.001   \n",
       "210      30  0.719351  0.865652          None  0.001   \n",
       "211      30  0.715767  0.865652          None  0.001   \n",
       "212      30  0.709111  0.865652          None  0.001   \n",
       "213      30  0.701263  0.865652          None  0.001   \n",
       "214      30  0.688109  0.865652          None  0.001   \n",
       "215      30  0.679376  0.865652          None  0.001   \n",
       "216      30  0.668687  0.865652          None  0.001   \n",
       "217      30  0.659245  0.865652          None  0.001   \n",
       "218      30  0.647971  0.865652          None  0.001   \n",
       "219      30  0.638061  0.865652          None  0.001   \n",
       "\n",
       "                                            model_desc          model_type  \\\n",
       "120  AttentionedYoonKimModel(\\n  (embedding): Linea...  AttentionedYoonKim   \n",
       "121  AttentionedYoonKimModel(\\n  (embedding): Linea...  AttentionedYoonKim   \n",
       "122  AttentionedYoonKimModel(\\n  (embedding): Linea...  AttentionedYoonKim   \n",
       "123  AttentionedYoonKimModel(\\n  (embedding): Linea...  AttentionedYoonKim   \n",
       "124  AttentionedYoonKimModel(\\n  (embedding): Linea...  AttentionedYoonKim   \n",
       "125  AttentionedYoonKimModel(\\n  (embedding): Linea...  AttentionedYoonKim   \n",
       "126  AttentionedYoonKimModel(\\n  (embedding): Linea...  AttentionedYoonKim   \n",
       "127  AttentionedYoonKimModel(\\n  (embedding): Linea...  AttentionedYoonKim   \n",
       "128  AttentionedYoonKimModel(\\n  (embedding): Linea...  AttentionedYoonKim   \n",
       "129  AttentionedYoonKimModel(\\n  (embedding): Linea...  AttentionedYoonKim   \n",
       "130  AttentionedYoonKimModel(\\n  (embedding): Linea...  AttentionedYoonKim   \n",
       "131  AttentionedYoonKimModel(\\n  (embedding): Linea...  AttentionedYoonKim   \n",
       "132  AttentionedYoonKimModel(\\n  (embedding): Linea...  AttentionedYoonKim   \n",
       "133  AttentionedYoonKimModel(\\n  (embedding): Linea...  AttentionedYoonKim   \n",
       "134  AttentionedYoonKimModel(\\n  (embedding): Linea...  AttentionedYoonKim   \n",
       "135  AttentionedYoonKimModel(\\n  (embedding): Linea...  AttentionedYoonKim   \n",
       "136  AttentionedYoonKimModel(\\n  (embedding): Linea...  AttentionedYoonKim   \n",
       "137  AttentionedYoonKimModel(\\n  (embedding): Linea...  AttentionedYoonKim   \n",
       "138  AttentionedYoonKimModel(\\n  (embedding): Linea...  AttentionedYoonKim   \n",
       "139  AttentionedYoonKimModel(\\n  (embedding): Linea...  AttentionedYoonKim   \n",
       "140  AttentionedYoonKimModel(\\n  (embedding): Linea...  AttentionedYoonKim   \n",
       "141  AttentionedYoonKimModel(\\n  (embedding): Linea...  AttentionedYoonKim   \n",
       "142  AttentionedYoonKimModel(\\n  (embedding): Linea...  AttentionedYoonKim   \n",
       "143  AttentionedYoonKimModel(\\n  (embedding): Linea...  AttentionedYoonKim   \n",
       "144  AttentionedYoonKimModel(\\n  (embedding): Linea...  AttentionedYoonKim   \n",
       "145  AttentionedYoonKimModel(\\n  (embedding): Linea...  AttentionedYoonKim   \n",
       "146  AttentionedYoonKimModel(\\n  (embedding): Linea...  AttentionedYoonKim   \n",
       "147  AttentionedYoonKimModel(\\n  (embedding): Linea...  AttentionedYoonKim   \n",
       "148  AttentionedYoonKimModel(\\n  (embedding): Linea...  AttentionedYoonKim   \n",
       "149  AttentionedYoonKimModel(\\n  (embedding): Linea...  AttentionedYoonKim   \n",
       "..                                                 ...                 ...   \n",
       "190  AttentionedYoonKimModel(\\n  (embedding): Linea...  AttentionedYoonKim   \n",
       "191  AttentionedYoonKimModel(\\n  (embedding): Linea...  AttentionedYoonKim   \n",
       "192  AttentionedYoonKimModel(\\n  (embedding): Linea...  AttentionedYoonKim   \n",
       "193  AttentionedYoonKimModel(\\n  (embedding): Linea...  AttentionedYoonKim   \n",
       "194  AttentionedYoonKimModel(\\n  (embedding): Linea...  AttentionedYoonKim   \n",
       "195  AttentionedYoonKimModel(\\n  (embedding): Linea...  AttentionedYoonKim   \n",
       "196  AttentionedYoonKimModel(\\n  (embedding): Linea...  AttentionedYoonKim   \n",
       "197  AttentionedYoonKimModel(\\n  (embedding): Linea...  AttentionedYoonKim   \n",
       "198  AttentionedYoonKimModel(\\n  (embedding): Linea...  AttentionedYoonKim   \n",
       "199  AttentionedYoonKimModel(\\n  (embedding): Linea...  AttentionedYoonKim   \n",
       "200  AttentionedYoonKimModel(\\n  (embedding): Linea...  AttentionedYoonKim   \n",
       "201  AttentionedYoonKimModel(\\n  (embedding): Linea...  AttentionedYoonKim   \n",
       "202  AttentionedYoonKimModel(\\n  (embedding): Linea...  AttentionedYoonKim   \n",
       "203  AttentionedYoonKimModel(\\n  (embedding): Linea...  AttentionedYoonKim   \n",
       "204  AttentionedYoonKimModel(\\n  (embedding): Linea...  AttentionedYoonKim   \n",
       "205  AttentionedYoonKimModel(\\n  (embedding): Linea...  AttentionedYoonKim   \n",
       "206  AttentionedYoonKimModel(\\n  (embedding): Linea...  AttentionedYoonKim   \n",
       "207  AttentionedYoonKimModel(\\n  (embedding): Linea...  AttentionedYoonKim   \n",
       "208  AttentionedYoonKimModel(\\n  (embedding): Linea...  AttentionedYoonKim   \n",
       "209  AttentionedYoonKimModel(\\n  (embedding): Linea...  AttentionedYoonKim   \n",
       "210  AttentionedYoonKimModel(\\n  (embedding): Linea...  AttentionedYoonKim   \n",
       "211  AttentionedYoonKimModel(\\n  (embedding): Linea...  AttentionedYoonKim   \n",
       "212  AttentionedYoonKimModel(\\n  (embedding): Linea...  AttentionedYoonKim   \n",
       "213  AttentionedYoonKimModel(\\n  (embedding): Linea...  AttentionedYoonKim   \n",
       "214  AttentionedYoonKimModel(\\n  (embedding): Linea...  AttentionedYoonKim   \n",
       "215  AttentionedYoonKimModel(\\n  (embedding): Linea...  AttentionedYoonKim   \n",
       "216  AttentionedYoonKimModel(\\n  (embedding): Linea...  AttentionedYoonKim   \n",
       "217  AttentionedYoonKimModel(\\n  (embedding): Linea...  AttentionedYoonKim   \n",
       "218  AttentionedYoonKimModel(\\n  (embedding): Linea...  AttentionedYoonKim   \n",
       "219  AttentionedYoonKimModel(\\n  (embedding): Linea...  AttentionedYoonKim   \n",
       "\n",
       "     noise_level_test  noise_level_train                        task  \\\n",
       "120             0.000              0.000  IMDB binary classification   \n",
       "121             0.010              0.000  IMDB binary classification   \n",
       "122             0.025              0.000  IMDB binary classification   \n",
       "123             0.050              0.000  IMDB binary classification   \n",
       "124             0.075              0.000  IMDB binary classification   \n",
       "125             0.100              0.000  IMDB binary classification   \n",
       "126             0.125              0.000  IMDB binary classification   \n",
       "127             0.150              0.000  IMDB binary classification   \n",
       "128             0.175              0.000  IMDB binary classification   \n",
       "129             0.200              0.000  IMDB binary classification   \n",
       "130             0.000              0.010  IMDB binary classification   \n",
       "131             0.010              0.010  IMDB binary classification   \n",
       "132             0.025              0.010  IMDB binary classification   \n",
       "133             0.050              0.010  IMDB binary classification   \n",
       "134             0.075              0.010  IMDB binary classification   \n",
       "135             0.100              0.010  IMDB binary classification   \n",
       "136             0.125              0.010  IMDB binary classification   \n",
       "137             0.150              0.010  IMDB binary classification   \n",
       "138             0.175              0.010  IMDB binary classification   \n",
       "139             0.200              0.010  IMDB binary classification   \n",
       "140             0.000              0.025  IMDB binary classification   \n",
       "141             0.010              0.025  IMDB binary classification   \n",
       "142             0.025              0.025  IMDB binary classification   \n",
       "143             0.050              0.025  IMDB binary classification   \n",
       "144             0.075              0.025  IMDB binary classification   \n",
       "145             0.100              0.025  IMDB binary classification   \n",
       "146             0.125              0.025  IMDB binary classification   \n",
       "147             0.150              0.025  IMDB binary classification   \n",
       "148             0.175              0.025  IMDB binary classification   \n",
       "149             0.200              0.025  IMDB binary classification   \n",
       "..                ...                ...                         ...   \n",
       "190             0.000              0.150  IMDB binary classification   \n",
       "191             0.010              0.150  IMDB binary classification   \n",
       "192             0.025              0.150  IMDB binary classification   \n",
       "193             0.050              0.150  IMDB binary classification   \n",
       "194             0.075              0.150  IMDB binary classification   \n",
       "195             0.100              0.150  IMDB binary classification   \n",
       "196             0.125              0.150  IMDB binary classification   \n",
       "197             0.150              0.150  IMDB binary classification   \n",
       "198             0.175              0.150  IMDB binary classification   \n",
       "199             0.200              0.150  IMDB binary classification   \n",
       "200             0.000              0.175  IMDB binary classification   \n",
       "201             0.010              0.175  IMDB binary classification   \n",
       "202             0.025              0.175  IMDB binary classification   \n",
       "203             0.050              0.175  IMDB binary classification   \n",
       "204             0.075              0.175  IMDB binary classification   \n",
       "205             0.100              0.175  IMDB binary classification   \n",
       "206             0.125              0.175  IMDB binary classification   \n",
       "207             0.150              0.175  IMDB binary classification   \n",
       "208             0.175              0.175  IMDB binary classification   \n",
       "209             0.200              0.175  IMDB binary classification   \n",
       "210             0.000              0.200  IMDB binary classification   \n",
       "211             0.010              0.200  IMDB binary classification   \n",
       "212             0.025              0.200  IMDB binary classification   \n",
       "213             0.050              0.200  IMDB binary classification   \n",
       "214             0.075              0.200  IMDB binary classification   \n",
       "215             0.100              0.200  IMDB binary classification   \n",
       "216             0.125              0.200  IMDB binary classification   \n",
       "217             0.150              0.200  IMDB binary classification   \n",
       "218             0.175              0.200  IMDB binary classification   \n",
       "219             0.200              0.200  IMDB binary classification   \n",
       "\n",
       "     trainable_params  \n",
       "120            298416  \n",
       "121            298416  \n",
       "122            298416  \n",
       "123            298416  \n",
       "124            298416  \n",
       "125            298416  \n",
       "126            298416  \n",
       "127            298416  \n",
       "128            298416  \n",
       "129            298416  \n",
       "130            298416  \n",
       "131            298416  \n",
       "132            298416  \n",
       "133            298416  \n",
       "134            298416  \n",
       "135            298416  \n",
       "136            298416  \n",
       "137            298416  \n",
       "138            298416  \n",
       "139            298416  \n",
       "140            298416  \n",
       "141            298416  \n",
       "142            298416  \n",
       "143            298416  \n",
       "144            298416  \n",
       "145            298416  \n",
       "146            298416  \n",
       "147            298416  \n",
       "148            298416  \n",
       "149            298416  \n",
       "..                ...  \n",
       "190            298416  \n",
       "191            298416  \n",
       "192            298416  \n",
       "193            298416  \n",
       "194            298416  \n",
       "195            298416  \n",
       "196            298416  \n",
       "197            298416  \n",
       "198            298416  \n",
       "199            298416  \n",
       "200            298416  \n",
       "201            298416  \n",
       "202            298416  \n",
       "203            298416  \n",
       "204            298416  \n",
       "205            298416  \n",
       "206            298416  \n",
       "207            298416  \n",
       "208            298416  \n",
       "209            298416  \n",
       "210            298416  \n",
       "211            298416  \n",
       "212            298416  \n",
       "213            298416  \n",
       "214            298416  \n",
       "215            298416  \n",
       "216            298416  \n",
       "217            298416  \n",
       "218            298416  \n",
       "219            298416  \n",
       "\n",
       "[100 rows x 15 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_4heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_4heads.to_csv('results/Attentioned4HYoonKim.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_1heads = results_df.iloc[:-100]\n",
    "results_1heads.to_csv('results/Attentioned1HYoonKim.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "acc_test                                                         0.686\n",
       "acc_train                                                     0.992178\n",
       "data_desc                         MAX_WORD_LEN = 8, MAX_TEXT_LEN = 256\n",
       "dropout                                                            0.5\n",
       "epochs                                                              30\n",
       "f1_test                                                       0.665445\n",
       "f1_train                                                       0.99213\n",
       "init_function                                                     None\n",
       "lr                                                               0.001\n",
       "model_desc           AttentionedYoonKimModel(\\n  (embedding): Linea...\n",
       "model_type                                          AttentionedYoonKim\n",
       "noise_level_test                                                 0.075\n",
       "noise_level_train                                                    0\n",
       "task                                        IMDB binary classification\n",
       "trainable_params                                                298416\n",
       "Name: 4, dtype: object"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_1heads.iloc[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "acc_test                                                        0.6502\n",
       "acc_train                                                     0.983822\n",
       "data_desc                         MAX_WORD_LEN = 8, MAX_TEXT_LEN = 256\n",
       "dropout                                                            0.5\n",
       "epochs                                                              30\n",
       "f1_test                                                       0.669013\n",
       "f1_train                                                      0.984064\n",
       "init_function                                                     None\n",
       "lr                                                               0.001\n",
       "model_desc           AttentionedYoonKimModel(\\n  (embedding): Linea...\n",
       "model_type                                          AttentionedYoonKim\n",
       "noise_level_test                                                 0.075\n",
       "noise_level_train                                                    0\n",
       "task                                        IMDB binary classification\n",
       "trainable_params                                                298416\n",
       "Name: 124, dtype: object"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_4heads.iloc[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/9 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writer: ['runs/May08_13-32-35_phobos-aijun_AttentionedYoonKim_lr3_dropout0.5_noise_level0.01_spacy_wordlen8_heads1']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Exception in thread Thread-5:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/threading.py\", line 914, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/phobos_aijun/.virtualenvs/pytorch-env/lib/python3.5/site-packages/tqdm/_tqdm.py\", line 144, in run\n",
      "    for instance in self.tqdm_cls._instances:\n",
      "  File \"/home/phobos_aijun/.virtualenvs/pytorch-env/lib/python3.5/_weakrefset.py\", line 60, in __iter__\n",
      "    for itemref in self.data:\n",
      "RuntimeError: Set changed size during iteration\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Global step 704. T=2.350466295083364 min\n",
      "Loss               : 0.6909279823303223\n",
      "In-batch accuracy  : 0.5\n",
      "Validation accuracy: 0.4972, f1: 0.6334208223972003\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phobos_aijun/.virtualenvs/pytorch-env/lib/python3.5/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10. Global step 7744. T=27.021090630690257 min\n",
      "Loss               : 0.12470026314258575\n",
      "In-batch accuracy  : 1.0\n",
      "Validation accuracy: 0.8424, f1: 0.8330508474576271\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for noise_level in tqdm(NOISE_LEVELS[1:]):\n",
    "    run_model_with(\n",
    "        noise_level=noise_level, n_filters=256, cnn_kernel_size=5, hidden_dim_out=128, dropout=0.5, lr=1e-3, epochs=30, heads=1\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other exps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writer: ['runs/May08_18-24-24_phobos-aijun_AttentionedYoonKim_lr3_dropout0.5_noise_level0.025_spacy_wordlen8_heads1']\n",
      "Epoch 0. Global step 704. T=2.3408157149950664 min\n",
      "Loss               : 0.7010097503662109\n",
      "In-batch accuracy  : 0.25\n",
      "Validation accuracy: 0.5012, f1: 0.2728862973760933\n",
      "\n",
      "Calculating validation metrics... Time 2.4388691782951355 min\n",
      "Final test metrics: {'accuracy': 0.49872, 'f1': 0.25324752711238235}, Time 15.579216667016347 min\n",
      "\n",
      "CPU times: user 32min 16s, sys: 34.8 s, total: 32min 51s\n",
      "Wall time: 15min 34s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AttentionedYoonKimModel(\n",
       "  (embedding): Linear(in_features=74, out_features=74, bias=True)\n",
       "  (chars_cnn): Sequential(\n",
       "    (0): Conv1d(74, 256, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool1d(kernel_size=8, stride=8, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (words_rnn): GRU(256, 128, dropout=0.5)\n",
       "  (attention): MultiHeadAttention(\n",
       "    (query_layer): Linear(in_features=128, out_features=128, bias=False)\n",
       "    (key_layer): Linear(in_features=128, out_features=128, bias=False)\n",
       "    (value_layer): Linear(in_features=128, out_features=128, bias=False)\n",
       "    (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True)\n",
       "  )\n",
       "  (projector): Linear(in_features=128, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "run_model_with(\n",
    "    noise_level=0.025, n_filters=256, cnn_kernel_size=5, hidden_dim_out=128, dropout=0.5, lr=1e-3, epochs=1, heads=1, print_every=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writer: ['runs/May07_19-43-00_phobos-aijun_AttentionedYoonKim_lr3_dropout0.5_noise_level0.1_spacy_wordlen8_heads1']\n",
      "Epoch 0. Global step 704. T=2.3722752690315247 min\n",
      "Loss               : 0.689220130443573\n",
      "In-batch accuracy  : 0.5\n",
      "Validation accuracy: 0.496, f1: 0.4241316270566728\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phobos_aijun/.virtualenvs/pytorch-env/lib/python3.5/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10. Global step 7744. T=27.09961405197779 min\n",
      "Loss               : 0.25278401374816895\n",
      "In-batch accuracy  : 1.0\n",
      "Validation accuracy: 0.8252, f1: 0.8232915487262433\n",
      "\n",
      "Epoch 20. Global step 14784. T=51.80704816977183 min\n",
      "Loss               : 0.019888877868652344\n",
      "In-batch accuracy  : 1.0\n"
     ]
    }
   ],
   "source": [
    "run_model_with(\n",
    "    noise_level=0.1, n_filters=256, cnn_kernel_size=5, hidden_dim_out=128, dropout=0.5, lr=1e-3, epochs=30, heads=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Global step 704\n",
      "Loss               : 0.6714678406715393\n",
      "In-batch accuracy  : 1.0\n",
      "Validation accuracy: 0.5048\n",
      "\n",
      "Epoch 10. Global step 7744\n",
      "Loss               : 0.0345512330532074\n",
      "In-batch accuracy  : 1.0\n",
      "Validation accuracy: 0.8504\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = run_model_with(\n",
    "    noise_level=0.025, n_filters=256, cnn_kernel_size=5, hidden_dim_out=128, dropout=0.5,\n",
    "    lr=1e-3, epochs=30, heads=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 32, 128])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "cuDNN requires contiguous input tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-fb107ef4929d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m model = run_model_with(\n\u001b[1;32m      2\u001b[0m     \u001b[0mnoise_level\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_filters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcnn_kernel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_dim_out\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheads\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m )\n",
      "\u001b[0;32m<ipython-input-34-fa878d10b455>\u001b[0m in \u001b[0;36mrun_model_with\u001b[0;34m(noise_level, n_filters, cnn_kernel_size, hidden_dim_out, dropout, lr, epochs, heads, init_function, _model)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m             \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/pytorch-env/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-37-ab32ba46d4a8>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprojector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/pytorch-env/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-29-83314f758efd>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, query, keys)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0mattention\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;31m# apply batch normalization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         \u001b[0mattention\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mattention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/pytorch-env/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/pytorch-env/lib/python3.5/site-packages/torch/nn/modules/batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     35\u001b[0m         return F.batch_norm(\n\u001b[1;32m     36\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunning_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunning_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m             self.training, self.momentum, self.eps)\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/pytorch-env/lib/python3.5/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   1011\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Expected more than 1 value per channel when training, got input size {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1012\u001b[0m     \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_functions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBatchNorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrunning_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1013\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1014\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1015\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: cuDNN requires contiguous input tensor"
     ]
    }
   ],
   "source": [
    "model = run_model_with(\n",
    "    noise_level=0.025, n_filters=256, cnn_kernel_size=5, hidden_dim_out=128, dropout=0.5,\n",
    "    lr=1e-3, epochs=30, heads=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 32, 128])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "cuDNN requires contiguous input tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-fb107ef4929d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m model = run_model_with(\n\u001b[1;32m      2\u001b[0m     \u001b[0mnoise_level\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_filters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcnn_kernel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_dim_out\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheads\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m )\n",
      "\u001b[0;32m<ipython-input-25-8a83f4623738>\u001b[0m in \u001b[0;36mrun_model_with\u001b[0;34m(noise_level, n_filters, cnn_kernel_size, hidden_dim_out, dropout, lr, epochs, heads, init_function, _model)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/pytorch-env/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-24-3ea1ab894f25>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords_rnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprojector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/pytorch-env/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-29-932d6c0416ac>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, query, keys)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0mattention\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;31m# apply batch normalization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         \u001b[0mattention\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mattention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/pytorch-env/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/pytorch-env/lib/python3.5/site-packages/torch/nn/modules/batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     35\u001b[0m         return F.batch_norm(\n\u001b[1;32m     36\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunning_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunning_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m             self.training, self.momentum, self.eps)\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/pytorch-env/lib/python3.5/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   1011\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Expected more than 1 value per channel when training, got input size {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1012\u001b[0m     \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_functions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBatchNorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrunning_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1013\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1014\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1015\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: cuDNN requires contiguous input tensor"
     ]
    }
   ],
   "source": [
    "model = run_model_with(\n",
    "    noise_level=0.025, n_filters=256, cnn_kernel_size=5, hidden_dim_out=128, dropout=0.5,\n",
    "    lr=1e-3, epochs=30, heads=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-env",
   "language": "python",
   "name": "pytorch-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
