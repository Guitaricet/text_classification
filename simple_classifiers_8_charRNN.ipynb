{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "from random import random, choice\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "from tqdm import tqdm as tqdm\n",
    "\n",
    "CUDA = torch.cuda.is_available()\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import torchtext\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAXLEN = 256\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "VALID_SIZE = 0.1\n",
    "\n",
    "NOISE_LEVEL = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_field = torchtext.data.Field(\n",
    "    lower=True, include_lengths=False, fix_length=2048, tensor_type=torch.FloatTensor, batch_first=True,\n",
    "    tokenize=lambda x: x, use_vocab=False, sequential=False\n",
    ")\n",
    "label_field = torchtext.data.Field(sequential=False, use_vocab=False)\n",
    "\n",
    "ALPHABET = [' ', 'e', 't', 'a', 'i', 'o', 's', 'n', 'r', 'h', 'l', 'd', 'c', 'm', 'u', 'f', 'g', 'y', 'b', 'w', 'p',\\\n",
    "            '.', 'v', ',', 'k', \"'\", '/', '>', '<', '-', '\"', 'j', 'x', ')', '(', '!', 'z', 'q', '0', '1', '?', ':',\\\n",
    "            '9', '2', '*', ';', '3', '5', '8', '4', '7', '&', '6', 'Ã©', '\\x96', '`', '$', '\\x85', '_', '%', '=', '#',\\\n",
    "            'UNK', 'PAD']\n",
    "\n",
    "ALPHABET_LEN = len(ALPHABET)\n",
    "\n",
    "char2int = {s: i for s, i in zip(ALPHABET, range(ALPHABET_LEN))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(char):\n",
    "    zeros = np.zeros(ALPHABET_LEN)\n",
    "    if char in char2int:\n",
    "        zeros[char2int[char]] = 1.\n",
    "    else:\n",
    "        zeros[char2int['UNK']] = 1.\n",
    "\n",
    "def preprocess_text_nobatch(text, maxlen=MAXLEN):\n",
    "    one_hotted_text = np.zeros((maxlen, ALPHABET_LEN))\n",
    "    for i, char in enumerate(text):\n",
    "        if i >= MAXLEN:\n",
    "            break\n",
    "        one_hotted_text[i, char2int.get(char, char2int['UNK'])] = 1.\n",
    "    if i < MAXLEN:\n",
    "        for j in range(i+1, MAXLEN):\n",
    "            one_hotted_text[j, char2int['PAD']] = 1.\n",
    "\n",
    "    return torch.FloatTensor(one_hotted_text)\n",
    "\n",
    "def onehot2text(one_hotted_text, batch_size=None):\n",
    "    if batch_size is None:\n",
    "        text = ''\n",
    "        _, idx = torch.max(one_hotted_text, 1)\n",
    "        for i in idx:\n",
    "            symb = ALPHABET[i]\n",
    "            if symb == 'PAD':\n",
    "                break\n",
    "            else:\n",
    "                text += symb\n",
    "        return text\n",
    "    else:\n",
    "        texts = []\n",
    "        for text in one_hotted_text:\n",
    "            texts.append(onehot2text(one_hotted_text, batch_size=None))\n",
    "        return texts\n",
    "\n",
    "def noise_generator(string, noise_level, chars=ALPHABET+['']):\n",
    "    noised = \"\"\n",
    "    for c in string:\n",
    "        if random() > noise_level:\n",
    "            noised += c\n",
    "        if random() < noise_level:\n",
    "            noised += choice(chars)\n",
    "    return noised\n",
    "\n",
    "class CharIMDB(torchtext.datasets.imdb.IMDB):\n",
    "    noise_level = 0\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = super(CharIMDB, self).__getitem__(idx)\n",
    "        text = item.text\n",
    "        text = noise_generator(text, self.noise_level)\n",
    "        label = int(item.label == 'pos')\n",
    "        return preprocess_text_nobatch(text), label\n",
    "\n",
    "def get_train_valid_loader(dataset, valid_size, batch_size, random_seed=42, shuffle=True, num_workers=4):\n",
    "\n",
    "    len_dataset = len(dataset)\n",
    "    indices = list(range(len_dataset))\n",
    "\n",
    "    if shuffle:\n",
    "        np.random.seed(random_seed)\n",
    "        np.random.shuffle(indices)\n",
    "\n",
    "    val_actual_size = int(len_dataset * valid_size)\n",
    "\n",
    "    train_idx, valid_idx = indices[:-val_actual_size], indices[-val_actual_size:]\n",
    "    \n",
    "    train_sampler = SubsetRandomSampler(train_idx)\n",
    "    valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        dataset, batch_size=batch_size, sampler=train_sampler, num_workers=4\n",
    "    )\n",
    "    valid_loader = torch.utils.data.DataLoader(\n",
    "        dataset, batch_size=batch_size, sampler=valid_sampler, num_workers=4\n",
    "    )\n",
    "\n",
    "    return train_loader, valid_loader\n",
    "\n",
    "def get_accuracy(model, test_dataset):\n",
    "    \"\"\"\n",
    "    Moder will be in TRAIN mode after that\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    predictions = []\n",
    "    lables = []\n",
    "\n",
    "    for text, label in test_dataset:\n",
    "        if CUDA:\n",
    "            text = Variable(text.cuda())\n",
    "        else:\n",
    "            text = Variable(text)\n",
    "\n",
    "        text = text.permute(1, 0, 2)\n",
    "        prediction = model(text)\n",
    "\n",
    "        _, idx = torch.max(prediction, 1)\n",
    "        predictions += idx.data.tolist()\n",
    "        lables += label.tolist()\n",
    "\n",
    "    acc = accuracy_score(lables, predictions)\n",
    "    model.train()\n",
    "    return acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "CharIMDB.noise_level = NOISE_LEVEL\n",
    "train, test = CharIMDB.splits(text_field, label_field)\n",
    "\n",
    "dataloader, val_dataloader = get_train_valid_loader(train, valid_size=VALID_SIZE, batch_size=BATCH_SIZE)\n",
    "\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "    test, batch_size=BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sru import SRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharRNN(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden, dropout=0.5, num_layers=2):\n",
    "        super(CharRNN, self).__init__()\n",
    "\n",
    "        self.embed = nn.Linear(ALPHABET_LEN, ALPHABET_LEN)\n",
    "        self.embed.weight = init.xavier_normal(self.embed.weight)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "#         self.rnn = SRU(ALPHABET_LEN, hidden, dropout=dropout, num_layers=num_layers)\n",
    "        self.rnn = nn.GRU(ALPHABET_LEN, hidden, dropout=dropout, num_layers=num_layers)\n",
    "        self.fc = nn.Linear(hidden, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: (seq_len, batch_size, signal_dim)\n",
    "        \"\"\"\n",
    "        x = self.embed(x)\n",
    "        x = self.dropout(x)\n",
    "        x, c_states = self.rnn(x)\n",
    "        x = self.fc(x[-1])\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model_with(noise_level, hidden, lr=1e-4, dropout=0.5, layers=1, epochs=30):\n",
    "    CharIMDB.noise_level = noise_level\n",
    "\n",
    "    model = CharRNN(hidden, num_layers=layers)\n",
    "    if CUDA:\n",
    "        model.cuda()\n",
    "    model.train()\n",
    "\n",
    "    writer = SummaryWriter(comment='_charGRU_dropout%s_embed_layers%s' % (dropout, layers))\n",
    "    \n",
    "    optimizer = optim.Adam(params=model.parameters(), lr=lr)\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    global_step = 0\n",
    "\n",
    "    loss_f = F.cross_entropy\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        for batch_idx, (text, label) in enumerate(dataloader):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            if CUDA:\n",
    "                text = Variable(text.cuda())\n",
    "                label = Variable(torch.LongTensor(label).cuda())\n",
    "            else:\n",
    "                text = Variable(text)\n",
    "                label = Variable(torch.LongTensor(label))\n",
    "\n",
    "            text = text.permute(1, 0, 2)\n",
    "            prediction = model(text)\n",
    "\n",
    "            loss = loss_f(prediction, label)\n",
    "\n",
    "            writer.add_scalar('loss', loss.data[0], global_step=global_step)\n",
    "\n",
    "            loss.backward()        \n",
    "            torch.nn.utils.clip_grad_norm(model.parameters(), 1e-1)\n",
    "            optimizer.step()\n",
    "\n",
    "            if CUDA:\n",
    "                torch.cuda.synchronize()\n",
    "            global_step += 1\n",
    "\n",
    "        # evaluation\n",
    "        print('Global step: %s' % global_step)\n",
    "        print('Loss after epoch %s: %s' % (epoch, loss.data[0]))\n",
    "\n",
    "        _, idx = torch.max(prediction, 1)\n",
    "        acc = accuracy_score(label.data.tolist(), idx.data.tolist())\n",
    "        writer.add_scalar('accuracy_train', acc, global_step=global_step)\n",
    "        print('In-batch accuracy:', acc)\n",
    "\n",
    "        acc = get_accuracy(model, val_dataloader)\n",
    "        print('Validation accuracy:', acc)\n",
    "        writer.add_scalar('accuracy_val', acc, global_step=global_step)\n",
    "        print()\n",
    "\n",
    "    # Test\n",
    "\n",
    "    acc = get_accuracy(model, test_dataloader)\n",
    "    print('Final test accuracy:', acc)\n",
    "    writer.add_scalar('accuracy_test_final', acc, global_step=global_step)\n",
    "    print()\n",
    "    model.eval()\n",
    "    # model is in EVAL mode!\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SRU loaded for gpu 0\n",
      "Global step: 704\n",
      "Loss after epoch 0: 0.6822301745414734\n",
      "In-batch accuracy: 0.75\n",
      "Validation accuracy: 0.4972\n",
      "\n",
      "Global step: 1408\n",
      "Loss after epoch 1: 0.6573439240455627\n",
      "In-batch accuracy: 1.0\n",
      "Validation accuracy: 0.4988\n",
      "\n",
      "Global step: 2112\n",
      "Loss after epoch 2: 0.6799705028533936\n",
      "In-batch accuracy: 0.75\n",
      "Validation accuracy: 0.4936\n",
      "\n",
      "Global step: 2816\n",
      "Loss after epoch 3: 0.7229750156402588\n",
      "In-batch accuracy: 0.25\n",
      "Validation accuracy: 0.4856\n",
      "\n",
      "Global step: 3520\n",
      "Loss after epoch 4: 0.7095352411270142\n",
      "In-batch accuracy: 0.25\n",
      "Validation accuracy: 0.4916\n",
      "\n",
      "Global step: 4224\n",
      "Loss after epoch 5: 0.7096357345581055\n",
      "In-batch accuracy: 0.25\n",
      "Validation accuracy: 0.4948\n",
      "\n",
      "Global step: 4928\n",
      "Loss after epoch 6: 0.643151581287384\n",
      "In-batch accuracy: 1.0\n",
      "Validation accuracy: 0.494\n",
      "\n",
      "Global step: 5632\n",
      "Loss after epoch 7: 0.7027839422225952\n",
      "In-batch accuracy: 0.5\n",
      "Validation accuracy: 0.492\n",
      "\n",
      "Global step: 6336\n",
      "Loss after epoch 8: 0.6687096953392029\n",
      "In-batch accuracy: 1.0\n",
      "Validation accuracy: 0.4912\n",
      "\n",
      "Global step: 7040\n",
      "Loss after epoch 9: 0.7099725604057312\n",
      "In-batch accuracy: 0.25\n",
      "Validation accuracy: 0.4856\n",
      "\n",
      "Global step: 7744\n",
      "Loss after epoch 10: 0.719476580619812\n",
      "In-batch accuracy: 0.25\n",
      "Validation accuracy: 0.4868\n",
      "\n",
      "Global step: 8448\n",
      "Loss after epoch 11: 0.6901521682739258\n",
      "In-batch accuracy: 0.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-93:\n",
      "Process Process-96:\n",
      "Process Process-94:\n",
      "Process Process-95:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/phobos_aijun/.virtualenvs/pytorch-env/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 55, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/phobos_aijun/.virtualenvs/pytorch-env/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 55, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"<ipython-input-4-cd72a0080c8b>\", line 54, in __getitem__\n",
      "    return preprocess_text_nobatch(text), label\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/phobos_aijun/.virtualenvs/pytorch-env/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 55, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/phobos_aijun/.virtualenvs/pytorch-env/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 50, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"<ipython-input-4-cd72a0080c8b>\", line 13, in preprocess_text_nobatch\n",
      "    one_hotted_text[i, char2int.get(char, char2int['UNK'])] = 1.\n",
      "  File \"/home/phobos_aijun/.virtualenvs/pytorch-env/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 50, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/home/phobos_aijun/.virtualenvs/pytorch-env/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 135, in default_collate\n",
      "    return [default_collate(samples) for samples in transposed]\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 343, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/home/phobos_aijun/.virtualenvs/pytorch-env/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 135, in <listcomp>\n",
      "    return [default_collate(samples) for samples in transposed]\n",
      "  File \"/usr/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/home/phobos_aijun/.virtualenvs/pytorch-env/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 112, in default_collate\n",
      "    return torch.stack(batch, 0, out=out)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/phobos_aijun/.virtualenvs/pytorch-env/lib/python3.5/site-packages/torch/functional.py\", line 66, in stack\n",
      "    return torch.cat(inputs, dim, out=out)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "Exception ignored in: <bound method DataLoaderIter.__del__ of <torch.utils.data.dataloader.DataLoaderIter object at 0x7fafe7d1e828>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/phobos_aijun/.virtualenvs/pytorch-env/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 333, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/phobos_aijun/.virtualenvs/pytorch-env/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 319, in _shutdown_workers\n",
      "    self.data_queue.get()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 345, in get\n",
      "    return ForkingPickler.loads(res)\n",
      "  File \"/home/phobos_aijun/.virtualenvs/pytorch-env/lib/python3.5/site-packages/torch/multiprocessing/reductions.py\", line 70, in rebuild_storage_fd\n",
      "    fd = df.detach()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/resource_sharer.py\", line 57, in detach\n",
      "    with _resource_sharer.get_connection(self._id) as conn:\n",
      "  File \"/usr/lib/python3.5/multiprocessing/resource_sharer.py\", line 87, in get_connection\n",
      "    c = Client(address, authkey=process.current_process().authkey)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 487, in Client\n",
      "    c = SocketClient(address)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 614, in SocketClient\n",
      "    s.connect(address)\n",
      "FileNotFoundError: [Errno 2] No such file or directory\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-cdf26e98ab9f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrun_model_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoise_level\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-3405dbdf3f2f>\u001b[0m in \u001b[0;36mrun_model_with\u001b[0;34m(noise_level, hidden, lr, dropout, epochs)\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'In-batch accuracy:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Validation accuracy:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'accuracy_val'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-cd72a0080c8b>\u001b[0m in \u001b[0;36mget_accuracy\u001b[0;34m(model, test_dataset)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_dataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mCUDA\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m             \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/pytorch-env/lib/python3.5/site-packages/torch/_utils.py\u001b[0m in \u001b[0;36m_cuda\u001b[0;34m(self, device, async)\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0mnew_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnew_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0masync\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "run_model_with(noise_level=0, hidden=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global step: 704\n",
      "Loss after epoch 0: 0.6877833604812622\n",
      "In-batch accuracy: 0.5\n",
      "Validation accuracy: 0.5012\n",
      "\n",
      "Global step: 1408\n",
      "Loss after epoch 1: 0.6759294271469116\n",
      "In-batch accuracy: 0.5\n",
      "Validation accuracy: 0.5224\n",
      "\n",
      "Global step: 2112\n",
      "Loss after epoch 2: 0.6749019026756287\n",
      "In-batch accuracy: 0.5\n",
      "Validation accuracy: 0.526\n",
      "\n",
      "Global step: 2816\n",
      "Loss after epoch 3: 0.74471116065979\n",
      "In-batch accuracy: 0.0\n",
      "Validation accuracy: 0.5172\n",
      "\n",
      "Global step: 3520\n",
      "Loss after epoch 4: 0.6857150793075562\n",
      "In-batch accuracy: 0.5\n",
      "Validation accuracy: 0.5076\n",
      "\n",
      "Global step: 4224\n",
      "Loss after epoch 5: 0.6910227537155151\n",
      "In-batch accuracy: 0.75\n",
      "Validation accuracy: 0.514\n",
      "\n",
      "Global step: 4928\n",
      "Loss after epoch 6: 0.696495532989502\n",
      "In-batch accuracy: 0.75\n",
      "Validation accuracy: 0.512\n",
      "\n",
      "Global step: 5632\n",
      "Loss after epoch 7: 0.7819569110870361\n",
      "In-batch accuracy: 0.0\n",
      "Validation accuracy: 0.5048\n",
      "\n",
      "Global step: 6336\n",
      "Loss after epoch 8: 0.629566490650177\n",
      "In-batch accuracy: 0.75\n",
      "Validation accuracy: 0.5008\n",
      "\n",
      "Global step: 7040\n",
      "Loss after epoch 9: 0.660133957862854\n",
      "In-batch accuracy: 0.5\n",
      "Validation accuracy: 0.5112\n",
      "\n",
      "Global step: 7744\n",
      "Loss after epoch 10: 0.6865528225898743\n",
      "In-batch accuracy: 0.25\n",
      "Validation accuracy: 0.5156\n",
      "\n",
      "Global step: 8448\n",
      "Loss after epoch 11: 0.6693791151046753\n",
      "In-batch accuracy: 0.75\n",
      "Validation accuracy: 0.504\n",
      "\n",
      "Global step: 9152\n",
      "Loss after epoch 12: 0.6989344954490662\n",
      "In-batch accuracy: 0.25\n",
      "Validation accuracy: 0.5108\n",
      "\n",
      "Global step: 9856\n",
      "Loss after epoch 13: 0.7431229948997498\n",
      "In-batch accuracy: 0.5\n",
      "Validation accuracy: 0.5176\n",
      "\n",
      "Global step: 10560\n",
      "Loss after epoch 14: 0.7222448587417603\n",
      "In-batch accuracy: 0.25\n",
      "Validation accuracy: 0.5172\n",
      "\n",
      "Global step: 11264\n",
      "Loss after epoch 15: 0.7256454825401306\n",
      "In-batch accuracy: 0.25\n",
      "Validation accuracy: 0.5196\n",
      "\n",
      "Global step: 11968\n",
      "Loss after epoch 16: 0.655430793762207\n",
      "In-batch accuracy: 0.75\n",
      "Validation accuracy: 0.5036\n",
      "\n",
      "Global step: 12672\n",
      "Loss after epoch 17: 0.7431749105453491\n",
      "In-batch accuracy: 0.0\n",
      "Validation accuracy: 0.5176\n",
      "\n",
      "Global step: 13376\n",
      "Loss after epoch 18: 0.6873272061347961\n",
      "In-batch accuracy: 0.5\n",
      "Validation accuracy: 0.5236\n",
      "\n",
      "Global step: 14080\n",
      "Loss after epoch 19: 0.6581532955169678\n",
      "In-batch accuracy: 0.75\n",
      "Validation accuracy: 0.5216\n",
      "\n",
      "Global step: 14784\n",
      "Loss after epoch 20: 0.7225463390350342\n",
      "In-batch accuracy: 0.25\n",
      "Validation accuracy: 0.5324\n",
      "\n",
      "Global step: 15488\n",
      "Loss after epoch 21: 0.644294023513794\n",
      "In-batch accuracy: 0.75\n",
      "Validation accuracy: 0.5348\n",
      "\n",
      "Global step: 16192\n",
      "Loss after epoch 22: 0.6488078832626343\n",
      "In-batch accuracy: 0.75\n",
      "Validation accuracy: 0.5288\n",
      "\n",
      "Global step: 16896\n",
      "Loss after epoch 23: 0.6605956554412842\n",
      "In-batch accuracy: 0.75\n",
      "Validation accuracy: 0.5628\n",
      "\n",
      "Global step: 17600\n",
      "Loss after epoch 24: 0.7789527177810669\n",
      "In-batch accuracy: 0.5\n",
      "Validation accuracy: 0.572\n",
      "\n",
      "Global step: 18304\n",
      "Loss after epoch 25: 0.7113422155380249\n",
      "In-batch accuracy: 0.5\n",
      "Validation accuracy: 0.5724\n",
      "\n",
      "Global step: 19008\n",
      "Loss after epoch 26: 0.6265515685081482\n",
      "In-batch accuracy: 0.75\n",
      "Validation accuracy: 0.5788\n",
      "\n",
      "Global step: 19712\n",
      "Loss after epoch 27: 0.7378420829772949\n",
      "In-batch accuracy: 0.25\n",
      "Validation accuracy: 0.5816\n",
      "\n",
      "Global step: 20416\n",
      "Loss after epoch 28: 0.5696713328361511\n",
      "In-batch accuracy: 0.75\n",
      "Validation accuracy: 0.5772\n",
      "\n",
      "Global step: 21120\n",
      "Loss after epoch 29: 0.7239721417427063\n",
      "In-batch accuracy: 0.25\n",
      "Validation accuracy: 0.56\n",
      "\n",
      "Global step: 21824\n",
      "Loss after epoch 30: 0.6748836636543274\n",
      "In-batch accuracy: 0.75\n",
      "Validation accuracy: 0.5824\n",
      "\n",
      "Global step: 22528\n",
      "Loss after epoch 31: 0.5926603078842163\n",
      "In-batch accuracy: 0.75\n",
      "Validation accuracy: 0.582\n",
      "\n",
      "Global step: 23232\n",
      "Loss after epoch 32: 0.8007228374481201\n",
      "In-batch accuracy: 0.25\n",
      "Validation accuracy: 0.584\n",
      "\n",
      "Global step: 23936\n",
      "Loss after epoch 33: 0.5300740003585815\n",
      "In-batch accuracy: 1.0\n",
      "Validation accuracy: 0.5636\n",
      "\n",
      "Global step: 24640\n",
      "Loss after epoch 34: 0.7083131670951843\n",
      "In-batch accuracy: 0.5\n",
      "Validation accuracy: 0.59\n",
      "\n",
      "Global step: 25344\n",
      "Loss after epoch 35: 0.6326422691345215\n",
      "In-batch accuracy: 0.75\n",
      "Validation accuracy: 0.5804\n",
      "\n",
      "Global step: 26048\n",
      "Loss after epoch 36: 0.7212014198303223\n",
      "In-batch accuracy: 0.5\n",
      "Validation accuracy: 0.56\n",
      "\n",
      "Global step: 26752\n",
      "Loss after epoch 37: 0.5954393148422241\n",
      "In-batch accuracy: 0.5\n",
      "Validation accuracy: 0.5912\n",
      "\n",
      "Global step: 27456\n",
      "Loss after epoch 38: 0.7703224420547485\n",
      "In-batch accuracy: 0.25\n",
      "Validation accuracy: 0.5856\n",
      "\n",
      "Global step: 28160\n",
      "Loss after epoch 39: 0.7133157253265381\n",
      "In-batch accuracy: 0.5\n",
      "Validation accuracy: 0.5888\n",
      "\n",
      "Global step: 28864\n",
      "Loss after epoch 40: 0.5758777856826782\n",
      "In-batch accuracy: 0.75\n",
      "Validation accuracy: 0.5748\n",
      "\n",
      "Global step: 29568\n",
      "Loss after epoch 41: 0.7943898439407349\n",
      "In-batch accuracy: 0.25\n",
      "Validation accuracy: 0.5832\n",
      "\n",
      "Global step: 30272\n",
      "Loss after epoch 42: 0.7322217226028442\n",
      "In-batch accuracy: 0.5\n",
      "Validation accuracy: 0.5948\n",
      "\n",
      "Global step: 30976\n",
      "Loss after epoch 43: 0.699298620223999\n",
      "In-batch accuracy: 0.25\n",
      "Validation accuracy: 0.5872\n",
      "\n",
      "Global step: 31680\n",
      "Loss after epoch 44: 0.747400164604187\n",
      "In-batch accuracy: 0.5\n",
      "Validation accuracy: 0.5848\n",
      "\n",
      "Global step: 32384\n",
      "Loss after epoch 45: 0.6879792213439941\n",
      "In-batch accuracy: 0.5\n",
      "Validation accuracy: 0.5796\n",
      "\n",
      "Global step: 33088\n",
      "Loss after epoch 46: 0.7442531585693359\n",
      "In-batch accuracy: 0.75\n",
      "Validation accuracy: 0.5872\n",
      "\n",
      "Global step: 33792\n",
      "Loss after epoch 47: 0.46316587924957275\n",
      "In-batch accuracy: 1.0\n",
      "Validation accuracy: 0.5484\n",
      "\n",
      "Global step: 34496\n",
      "Loss after epoch 48: 0.7851572036743164\n",
      "In-batch accuracy: 0.25\n",
      "Validation accuracy: 0.5756\n",
      "\n",
      "Global step: 35200\n",
      "Loss after epoch 49: 0.4662242829799652\n",
      "In-batch accuracy: 1.0\n",
      "Validation accuracy: 0.5864\n",
      "\n",
      "Global step: 35904\n",
      "Loss after epoch 50: 0.7712576389312744\n",
      "In-batch accuracy: 0.25\n",
      "Validation accuracy: 0.576\n",
      "\n",
      "Global step: 36608\n",
      "Loss after epoch 51: 0.5226591229438782\n",
      "In-batch accuracy: 0.5\n",
      "Validation accuracy: 0.5812\n",
      "\n",
      "Global step: 37312\n",
      "Loss after epoch 52: 0.6306784152984619\n",
      "In-batch accuracy: 0.5\n",
      "Validation accuracy: 0.5912\n",
      "\n",
      "Global step: 38016\n",
      "Loss after epoch 53: 0.6289879679679871\n",
      "In-batch accuracy: 0.5\n",
      "Validation accuracy: 0.5964\n",
      "\n",
      "Global step: 38720\n",
      "Loss after epoch 54: 0.6365715265274048\n",
      "In-batch accuracy: 0.5\n",
      "Validation accuracy: 0.584\n",
      "\n",
      "Global step: 39424\n",
      "Loss after epoch 55: 0.714073121547699\n",
      "In-batch accuracy: 0.75\n",
      "Validation accuracy: 0.5712\n",
      "\n",
      "Global step: 40128\n",
      "Loss after epoch 56: 0.8227112889289856\n",
      "In-batch accuracy: 0.5\n",
      "Validation accuracy: 0.5848\n",
      "\n",
      "Global step: 40832\n",
      "Loss after epoch 57: 0.8048059940338135\n",
      "In-batch accuracy: 0.5\n",
      "Validation accuracy: 0.5932\n",
      "\n",
      "Global step: 41536\n",
      "Loss after epoch 58: 0.696605920791626\n",
      "In-batch accuracy: 0.5\n",
      "Validation accuracy: 0.5828\n",
      "\n",
      "Global step: 42240\n",
      "Loss after epoch 59: 0.47740623354911804\n",
      "In-batch accuracy: 1.0\n",
      "Validation accuracy: 0.5892\n",
      "\n",
      "Global step: 42944\n",
      "Loss after epoch 60: 0.7132588624954224\n",
      "In-batch accuracy: 0.5\n",
      "Validation accuracy: 0.5952\n",
      "\n",
      "Global step: 43648\n",
      "Loss after epoch 61: 0.6535913944244385\n",
      "In-batch accuracy: 0.75\n",
      "Validation accuracy: 0.5872\n",
      "\n",
      "Global step: 44352\n",
      "Loss after epoch 62: 0.6725362539291382\n",
      "In-batch accuracy: 0.75\n",
      "Validation accuracy: 0.5756\n",
      "\n",
      "Global step: 45056\n",
      "Loss after epoch 63: 0.5778068900108337\n",
      "In-batch accuracy: 1.0\n",
      "Validation accuracy: 0.584\n",
      "\n",
      "Global step: 45760\n",
      "Loss after epoch 64: 0.6747623682022095\n",
      "In-batch accuracy: 0.5\n",
      "Validation accuracy: 0.58\n",
      "\n",
      "Global step: 46464\n",
      "Loss after epoch 65: 0.7159287929534912\n",
      "In-batch accuracy: 0.5\n",
      "Validation accuracy: 0.5636\n",
      "\n",
      "Global step: 47168\n",
      "Loss after epoch 66: 0.7432101964950562\n",
      "In-batch accuracy: 0.75\n",
      "Validation accuracy: 0.5908\n",
      "\n",
      "Global step: 47872\n",
      "Loss after epoch 67: 0.5742340087890625\n",
      "In-batch accuracy: 0.75\n",
      "Validation accuracy: 0.586\n",
      "\n",
      "Global step: 48576\n",
      "Loss after epoch 68: 0.6224431991577148\n",
      "In-batch accuracy: 0.5\n",
      "Validation accuracy: 0.5916\n",
      "\n",
      "Global step: 49280\n",
      "Loss after epoch 69: 0.5950433611869812\n",
      "In-batch accuracy: 0.75\n",
      "Validation accuracy: 0.5728\n",
      "\n",
      "Global step: 49984\n",
      "Loss after epoch 70: 0.585405707359314\n",
      "In-batch accuracy: 0.5\n",
      "Validation accuracy: 0.5872\n",
      "\n",
      "Global step: 50688\n",
      "Loss after epoch 71: 0.7169253826141357\n",
      "In-batch accuracy: 0.75\n",
      "Validation accuracy: 0.5828\n",
      "\n",
      "Global step: 51392\n",
      "Loss after epoch 72: 0.5969681739807129\n",
      "In-batch accuracy: 0.5\n",
      "Validation accuracy: 0.5896\n",
      "\n",
      "Global step: 52096\n",
      "Loss after epoch 73: 0.5244110822677612\n",
      "In-batch accuracy: 1.0\n",
      "Validation accuracy: 0.5952\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global step: 52800\n",
      "Loss after epoch 74: 0.5699837803840637\n",
      "In-batch accuracy: 0.75\n",
      "Validation accuracy: 0.5792\n",
      "\n",
      "Global step: 53504\n",
      "Loss after epoch 75: 0.5464096665382385\n",
      "In-batch accuracy: 0.75\n",
      "Validation accuracy: 0.5852\n",
      "\n",
      "Global step: 54208\n",
      "Loss after epoch 76: 0.5671347379684448\n",
      "In-batch accuracy: 0.75\n",
      "Validation accuracy: 0.6012\n",
      "\n",
      "Global step: 54912\n",
      "Loss after epoch 77: 0.6364257335662842\n",
      "In-batch accuracy: 0.5\n",
      "Validation accuracy: 0.5824\n",
      "\n",
      "Global step: 55616\n",
      "Loss after epoch 78: 0.4664842486381531\n",
      "In-batch accuracy: 1.0\n",
      "Validation accuracy: 0.5872\n",
      "\n",
      "Global step: 56320\n",
      "Loss after epoch 79: 0.7226962447166443\n",
      "In-batch accuracy: 0.5\n",
      "Validation accuracy: 0.5896\n",
      "\n",
      "Global step: 57024\n",
      "Loss after epoch 80: 0.5735809803009033\n",
      "In-batch accuracy: 0.75\n",
      "Validation accuracy: 0.5892\n",
      "\n",
      "Global step: 57728\n",
      "Loss after epoch 81: 0.8028784990310669\n",
      "In-batch accuracy: 0.25\n",
      "Validation accuracy: 0.6004\n",
      "\n",
      "Global step: 58432\n",
      "Loss after epoch 82: 0.9222941994667053\n",
      "In-batch accuracy: 0.25\n",
      "Validation accuracy: 0.5812\n",
      "\n",
      "Global step: 59136\n",
      "Loss after epoch 83: 0.47657111287117004\n",
      "In-batch accuracy: 1.0\n",
      "Validation accuracy: 0.5928\n",
      "\n",
      "Global step: 59840\n",
      "Loss after epoch 84: 0.6367892622947693\n",
      "In-batch accuracy: 0.75\n",
      "Validation accuracy: 0.59\n",
      "\n",
      "Global step: 60544\n",
      "Loss after epoch 85: 0.6898384690284729\n",
      "In-batch accuracy: 0.5\n",
      "Validation accuracy: 0.58\n",
      "\n",
      "Global step: 61248\n",
      "Loss after epoch 86: 0.7011841535568237\n",
      "In-batch accuracy: 0.75\n",
      "Validation accuracy: 0.58\n",
      "\n",
      "Global step: 61952\n",
      "Loss after epoch 87: 0.6542638540267944\n",
      "In-batch accuracy: 0.5\n",
      "Validation accuracy: 0.5884\n",
      "\n",
      "Global step: 62656\n",
      "Loss after epoch 88: 0.6759254336357117\n",
      "In-batch accuracy: 0.5\n",
      "Validation accuracy: 0.5596\n",
      "\n",
      "Global step: 63360\n",
      "Loss after epoch 89: 0.6154249310493469\n",
      "In-batch accuracy: 0.5\n",
      "Validation accuracy: 0.5764\n",
      "\n",
      "Global step: 64064\n",
      "Loss after epoch 90: 0.9357709884643555\n",
      "In-batch accuracy: 0.25\n",
      "Validation accuracy: 0.5828\n",
      "\n",
      "Global step: 64768\n",
      "Loss after epoch 91: 0.525493323802948\n",
      "In-batch accuracy: 0.75\n",
      "Validation accuracy: 0.5844\n",
      "\n",
      "Global step: 65472\n",
      "Loss after epoch 92: 0.8595097064971924\n",
      "In-batch accuracy: 0.0\n",
      "Validation accuracy: 0.5864\n",
      "\n",
      "Global step: 66176\n",
      "Loss after epoch 93: 0.7183809280395508\n",
      "In-batch accuracy: 0.75\n",
      "Validation accuracy: 0.5912\n",
      "\n",
      "Global step: 66880\n",
      "Loss after epoch 94: 0.6057461500167847\n",
      "In-batch accuracy: 0.75\n",
      "Validation accuracy: 0.5928\n",
      "\n",
      "Global step: 67584\n",
      "Loss after epoch 95: 0.6345586776733398\n",
      "In-batch accuracy: 0.5\n",
      "Validation accuracy: 0.5932\n",
      "\n",
      "Global step: 68288\n",
      "Loss after epoch 96: 0.9481221437454224\n",
      "In-batch accuracy: 0.0\n",
      "Validation accuracy: 0.5828\n",
      "\n",
      "Global step: 68992\n",
      "Loss after epoch 97: 0.6558783054351807\n",
      "In-batch accuracy: 0.75\n",
      "Validation accuracy: 0.592\n",
      "\n",
      "Global step: 69696\n",
      "Loss after epoch 98: 0.6332458257675171\n",
      "In-batch accuracy: 0.75\n",
      "Validation accuracy: 0.596\n",
      "\n",
      "Global step: 70400\n",
      "Loss after epoch 99: 0.7045444846153259\n",
      "In-batch accuracy: 0.75\n",
      "Validation accuracy: 0.5944\n",
      "\n",
      "Final test accuracy: 0.5754\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = run_model_with(noise_level=0, hidden=256, epochs=100, layers=1, lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global step: 704\n",
      "Loss after epoch 0: 0.6701354384422302\n",
      "In-batch accuracy: 0.75\n",
      "Validation accuracy: 0.5116\n",
      "\n",
      "Global step: 1408\n",
      "Loss after epoch 1: 0.7083399295806885\n",
      "In-batch accuracy: 0.25\n",
      "Validation accuracy: 0.5012\n",
      "\n",
      "Global step: 2112\n",
      "Loss after epoch 2: 0.6849496364593506\n",
      "In-batch accuracy: 0.5\n",
      "Validation accuracy: 0.5028\n",
      "\n",
      "Global step: 2816\n",
      "Loss after epoch 3: 0.7296500205993652\n",
      "In-batch accuracy: 0.5\n",
      "Validation accuracy: 0.5204\n",
      "\n",
      "Global step: 3520\n",
      "Loss after epoch 4: 0.6584969162940979\n",
      "In-batch accuracy: 0.75\n",
      "Validation accuracy: 0.514\n",
      "\n",
      "Global step: 4224\n",
      "Loss after epoch 5: 0.70598965883255\n",
      "In-batch accuracy: 0.5\n",
      "Validation accuracy: 0.5136\n",
      "\n",
      "Global step: 4928\n",
      "Loss after epoch 6: 0.6887087821960449\n",
      "In-batch accuracy: 0.5\n",
      "Validation accuracy: 0.5124\n",
      "\n",
      "Global step: 5632\n",
      "Loss after epoch 7: 0.7495694160461426\n",
      "In-batch accuracy: 0.25\n",
      "Validation accuracy: 0.5488\n",
      "\n",
      "Global step: 6336\n",
      "Loss after epoch 8: 0.5603877902030945\n",
      "In-batch accuracy: 0.75\n",
      "Validation accuracy: 0.6056\n",
      "\n",
      "Global step: 7040\n",
      "Loss after epoch 9: 0.5462902188301086\n",
      "In-batch accuracy: 0.75\n",
      "Validation accuracy: 0.6496\n",
      "\n",
      "Global step: 7744\n",
      "Loss after epoch 10: 0.744918167591095\n",
      "In-batch accuracy: 0.5\n",
      "Validation accuracy: 0.6456\n",
      "\n",
      "Global step: 8448\n",
      "Loss after epoch 11: 0.7226830124855042\n",
      "In-batch accuracy: 0.5\n",
      "Validation accuracy: 0.6908\n",
      "\n",
      "Global step: 9152\n",
      "Loss after epoch 12: 0.7363699674606323\n",
      "In-batch accuracy: 0.5\n",
      "Validation accuracy: 0.6728\n",
      "\n",
      "Global step: 9856\n",
      "Loss after epoch 13: 0.7529700398445129\n",
      "In-batch accuracy: 0.5\n",
      "Validation accuracy: 0.7024\n",
      "\n",
      "Global step: 10560\n",
      "Loss after epoch 14: 0.3863769471645355\n",
      "In-batch accuracy: 0.75\n",
      "Validation accuracy: 0.7144\n",
      "\n",
      "Global step: 11264\n",
      "Loss after epoch 15: 0.6466313600540161\n",
      "In-batch accuracy: 0.75\n",
      "Validation accuracy: 0.722\n",
      "\n",
      "Global step: 11968\n",
      "Loss after epoch 16: 0.14602619409561157\n",
      "In-batch accuracy: 1.0\n",
      "Validation accuracy: 0.726\n",
      "\n",
      "Global step: 12672\n",
      "Loss after epoch 17: 0.6005991697311401\n",
      "In-batch accuracy: 0.5\n",
      "Validation accuracy: 0.7372\n",
      "\n",
      "Global step: 13376\n",
      "Loss after epoch 18: 1.2577643394470215\n",
      "In-batch accuracy: 0.25\n",
      "Validation accuracy: 0.7228\n",
      "\n",
      "Global step: 14080\n",
      "Loss after epoch 19: 0.580107569694519\n",
      "In-batch accuracy: 0.75\n",
      "Validation accuracy: 0.728\n",
      "\n",
      "Global step: 14784\n",
      "Loss after epoch 20: 0.5992403030395508\n",
      "In-batch accuracy: 0.75\n",
      "Validation accuracy: 0.7176\n",
      "\n",
      "Global step: 15488\n",
      "Loss after epoch 21: 0.6347543001174927\n",
      "In-batch accuracy: 0.75\n",
      "Validation accuracy: 0.7284\n",
      "\n",
      "Global step: 16192\n",
      "Loss after epoch 22: 0.5572739243507385\n",
      "In-batch accuracy: 0.5\n",
      "Validation accuracy: 0.7216\n",
      "\n",
      "Global step: 16896\n",
      "Loss after epoch 23: 0.8550562858581543\n",
      "In-batch accuracy: 0.75\n",
      "Validation accuracy: 0.7348\n",
      "\n",
      "Global step: 17600\n",
      "Loss after epoch 24: 0.48999691009521484\n",
      "In-batch accuracy: 0.75\n",
      "Validation accuracy: 0.7348\n",
      "\n",
      "Global step: 18304\n",
      "Loss after epoch 25: 0.6373368501663208\n",
      "In-batch accuracy: 0.75\n",
      "Validation accuracy: 0.7332\n",
      "\n",
      "Global step: 19008\n",
      "Loss after epoch 26: 0.28712886571884155\n",
      "In-batch accuracy: 0.75\n",
      "Validation accuracy: 0.7264\n",
      "\n",
      "Global step: 19712\n",
      "Loss after epoch 27: 0.22509141266345978\n",
      "In-batch accuracy: 0.75\n",
      "Validation accuracy: 0.74\n",
      "\n",
      "Global step: 20416\n",
      "Loss after epoch 28: 0.6829990148544312\n",
      "In-batch accuracy: 0.75\n",
      "Validation accuracy: 0.7144\n",
      "\n",
      "Global step: 21120\n",
      "Loss after epoch 29: 0.35125574469566345\n",
      "In-batch accuracy: 0.75\n",
      "Validation accuracy: 0.7312\n",
      "\n",
      "Global step: 21824\n",
      "Loss after epoch 30: 0.5144132971763611\n",
      "In-batch accuracy: 0.5\n",
      "Validation accuracy: 0.7324\n",
      "\n",
      "Global step: 22528\n",
      "Loss after epoch 31: 0.255066454410553\n",
      "In-batch accuracy: 0.75\n",
      "Validation accuracy: 0.7324\n",
      "\n",
      "Global step: 23232\n",
      "Loss after epoch 32: 0.04479143023490906\n",
      "In-batch accuracy: 1.0\n",
      "Validation accuracy: 0.7404\n",
      "\n",
      "Global step: 23936\n",
      "Loss after epoch 33: 0.02524733543395996\n",
      "In-batch accuracy: 1.0\n",
      "Validation accuracy: 0.7184\n",
      "\n",
      "Global step: 24640\n",
      "Loss after epoch 34: 0.5448048114776611\n",
      "In-batch accuracy: 0.75\n",
      "Validation accuracy: 0.7168\n",
      "\n",
      "Global step: 25344\n",
      "Loss after epoch 35: 0.2880546450614929\n",
      "In-batch accuracy: 0.75\n",
      "Validation accuracy: 0.722\n",
      "\n",
      "Global step: 26048\n",
      "Loss after epoch 36: 0.29170483350753784\n",
      "In-batch accuracy: 0.75\n",
      "Validation accuracy: 0.7312\n",
      "\n",
      "Global step: 26752\n",
      "Loss after epoch 37: 0.013348788022994995\n",
      "In-batch accuracy: 1.0\n",
      "Validation accuracy: 0.724\n",
      "\n",
      "Global step: 27456\n",
      "Loss after epoch 38: 0.4642255902290344\n",
      "In-batch accuracy: 0.75\n",
      "Validation accuracy: 0.7104\n",
      "\n",
      "Global step: 28160\n",
      "Loss after epoch 39: 0.10951679944992065\n",
      "In-batch accuracy: 1.0\n",
      "Validation accuracy: 0.718\n",
      "\n",
      "Global step: 28864\n",
      "Loss after epoch 40: 0.3255539536476135\n",
      "In-batch accuracy: 0.75\n",
      "Validation accuracy: 0.7048\n",
      "\n",
      "Global step: 29568\n",
      "Loss after epoch 41: 0.013571977615356445\n",
      "In-batch accuracy: 1.0\n",
      "Validation accuracy: 0.7176\n",
      "\n",
      "Global step: 30272\n",
      "Loss after epoch 42: 0.05867643654346466\n",
      "In-batch accuracy: 1.0\n",
      "Validation accuracy: 0.7196\n",
      "\n",
      "Global step: 30976\n",
      "Loss after epoch 43: 0.08116734027862549\n",
      "In-batch accuracy: 1.0\n",
      "Validation accuracy: 0.7204\n",
      "\n",
      "Global step: 31680\n",
      "Loss after epoch 44: 0.032684847712516785\n",
      "In-batch accuracy: 1.0\n",
      "Validation accuracy: 0.7104\n",
      "\n",
      "Global step: 32384\n",
      "Loss after epoch 45: 0.041045770049095154\n",
      "In-batch accuracy: 1.0\n",
      "Validation accuracy: 0.7204\n",
      "\n",
      "Global step: 33088\n",
      "Loss after epoch 46: 0.5955413579940796\n",
      "In-batch accuracy: 0.75\n",
      "Validation accuracy: 0.7224\n",
      "\n",
      "Global step: 33792\n",
      "Loss after epoch 47: 1.0731990337371826\n",
      "In-batch accuracy: 0.75\n",
      "Validation accuracy: 0.7156\n",
      "\n",
      "Global step: 34496\n",
      "Loss after epoch 48: 0.22376857697963715\n",
      "In-batch accuracy: 0.75\n",
      "Validation accuracy: 0.7128\n",
      "\n",
      "Global step: 35200\n",
      "Loss after epoch 49: 0.0003133416175842285\n",
      "In-batch accuracy: 1.0\n",
      "Validation accuracy: 0.7204\n",
      "\n",
      "Global step: 35904\n",
      "Loss after epoch 50: 0.023821353912353516\n",
      "In-batch accuracy: 1.0\n",
      "Validation accuracy: 0.7196\n",
      "\n",
      "Global step: 36608\n",
      "Loss after epoch 51: 0.23217400908470154\n",
      "In-batch accuracy: 0.75\n",
      "Validation accuracy: 0.7224\n",
      "\n",
      "Global step: 37312\n",
      "Loss after epoch 52: 0.0027141571044921875\n",
      "In-batch accuracy: 1.0\n",
      "Validation accuracy: 0.7232\n",
      "\n",
      "Global step: 38016\n",
      "Loss after epoch 53: 0.0361294150352478\n",
      "In-batch accuracy: 1.0\n",
      "Validation accuracy: 0.716\n",
      "\n",
      "Global step: 38720\n",
      "Loss after epoch 54: 0.00212937593460083\n",
      "In-batch accuracy: 1.0\n",
      "Validation accuracy: 0.7144\n",
      "\n",
      "Global step: 39424\n",
      "Loss after epoch 55: 0.1556071937084198\n",
      "In-batch accuracy: 1.0\n",
      "Validation accuracy: 0.7164\n",
      "\n",
      "Global step: 40128\n",
      "Loss after epoch 56: 0.01267331838607788\n",
      "In-batch accuracy: 1.0\n",
      "Validation accuracy: 0.714\n",
      "\n",
      "Global step: 40832\n",
      "Loss after epoch 57: 0.02719104290008545\n",
      "In-batch accuracy: 1.0\n",
      "Validation accuracy: 0.7192\n",
      "\n",
      "Global step: 41536\n",
      "Loss after epoch 58: 0.8020529747009277\n",
      "In-batch accuracy: 0.75\n",
      "Validation accuracy: 0.7168\n",
      "\n",
      "Global step: 42240\n",
      "Loss after epoch 59: 0.1157984733581543\n",
      "In-batch accuracy: 1.0\n",
      "Validation accuracy: 0.7176\n",
      "\n",
      "Global step: 42944\n",
      "Loss after epoch 60: 0.8464627265930176\n",
      "In-batch accuracy: 0.75\n",
      "Validation accuracy: 0.71\n",
      "\n",
      "Global step: 43648\n",
      "Loss after epoch 61: 0.2703622281551361\n",
      "In-batch accuracy: 0.75\n",
      "Validation accuracy: 0.7088\n",
      "\n",
      "Global step: 44352\n",
      "Loss after epoch 62: 0.0024493932723999023\n",
      "In-batch accuracy: 1.0\n",
      "Validation accuracy: 0.7212\n",
      "\n",
      "Global step: 45056\n",
      "Loss after epoch 63: 0.06424468755722046\n",
      "In-batch accuracy: 1.0\n",
      "Validation accuracy: 0.7196\n",
      "\n",
      "Global step: 45760\n",
      "Loss after epoch 64: 0.009541749954223633\n",
      "In-batch accuracy: 1.0\n",
      "Validation accuracy: 0.7096\n",
      "\n",
      "Global step: 46464\n",
      "Loss after epoch 65: 0.16064393520355225\n",
      "In-batch accuracy: 1.0\n",
      "Validation accuracy: 0.7084\n",
      "\n",
      "Global step: 47168\n",
      "Loss after epoch 66: 0.05534198135137558\n",
      "In-batch accuracy: 1.0\n",
      "Validation accuracy: 0.7152\n",
      "\n",
      "Global step: 47872\n",
      "Loss after epoch 67: 0.017612308263778687\n",
      "In-batch accuracy: 1.0\n",
      "Validation accuracy: 0.722\n",
      "\n",
      "Global step: 48576\n",
      "Loss after epoch 68: 0.0019603967666625977\n",
      "In-batch accuracy: 1.0\n",
      "Validation accuracy: 0.7216\n",
      "\n",
      "Global step: 49280\n",
      "Loss after epoch 69: 0.1069171205163002\n",
      "In-batch accuracy: 1.0\n",
      "Validation accuracy: 0.706\n",
      "\n",
      "Global step: 49984\n",
      "Loss after epoch 70: 0.03160026669502258\n",
      "In-batch accuracy: 1.0\n",
      "Validation accuracy: 0.7156\n",
      "\n",
      "Global step: 50688\n",
      "Loss after epoch 71: 0.061738863587379456\n",
      "In-batch accuracy: 1.0\n",
      "Validation accuracy: 0.722\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-579:\n",
      "Process Process-577:\n",
      "Process Process-580:\n",
      "Process Process-578:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/phobos_aijun/.virtualenvs/pytorch-env/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 55, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/phobos_aijun/.virtualenvs/pytorch-env/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 50, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/home/phobos_aijun/.virtualenvs/pytorch-env/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 50, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/home/phobos_aijun/.virtualenvs/pytorch-env/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 50, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/home/phobos_aijun/.virtualenvs/pytorch-env/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 55, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 343, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"<ipython-input-4-2304a0eb7684>\", line 52, in __getitem__\n",
      "    text = noise_generator(text, self.noise_level)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"<ipython-input-4-2304a0eb7684>\", line 40, in noise_generator\n",
      "    if random() > noise_level:\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "Exception ignored in: <bound method DataLoaderIter.__del__ of <torch.utils.data.dataloader.DataLoaderIter object at 0x7fec2138f0f0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/phobos_aijun/.virtualenvs/pytorch-env/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 333, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/phobos_aijun/.virtualenvs/pytorch-env/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 319, in _shutdown_workers\n",
      "    self.data_queue.get()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 345, in get\n",
      "    return ForkingPickler.loads(res)\n",
      "  File \"/home/phobos_aijun/.virtualenvs/pytorch-env/lib/python3.5/site-packages/torch/multiprocessing/reductions.py\", line 70, in rebuild_storage_fd\n",
      "    fd = df.detach()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/resource_sharer.py\", line 57, in detach\n",
      "    with _resource_sharer.get_connection(self._id) as conn:\n",
      "  File \"/usr/lib/python3.5/multiprocessing/resource_sharer.py\", line 87, in get_connection\n",
      "    c = Client(address, authkey=process.current_process().authkey)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 487, in Client\n",
      "    c = SocketClient(address)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 614, in SocketClient\n",
      "    s.connect(address)\n",
      "FileNotFoundError: [Errno 2] No such file or directory\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-975f94992fa8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_model_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoise_level\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-de58c92069ca>\u001b[0m in \u001b[0;36mrun_model_with\u001b[0;34m(noise_level, hidden, lr, dropout, layers, epochs)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m             \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/pytorch-env/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-74ad0c550f25>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/pytorch-env/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/pytorch-env/lib/python3.5/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    202\u001b[0m             \u001b[0mflat_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflat_weight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         )\n\u001b[0;32m--> 204\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_packed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPackedSequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/pytorch-env/lib/python3.5/site-packages/torch/nn/_functions/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(input, *fargs, **fkwargs)\u001b[0m\n\u001b[1;32m    383\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mhack_onnx_rnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mfargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/pytorch-env/lib/python3.5/site-packages/torch/autograd/function.py\u001b[0m in \u001b[0;36m_do_forward\u001b[0;34m(self, *input)\u001b[0m\n\u001b[1;32m    326\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nested_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m         \u001b[0mflat_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_iter_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m         \u001b[0mflat_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNestedIOFunction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mflat_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    329\u001b[0m         \u001b[0mnested_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nested_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[0mnested_variables\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_unflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nested_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/pytorch-env/lib/python3.5/site-packages/torch/autograd/function.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    348\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mnested_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_map_variable_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nested_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 350\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_extended\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnested_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    351\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nested_input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nested_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/pytorch-env/lib/python3.5/site-packages/torch/nn/_functions/rnn.py\u001b[0m in \u001b[0;36mforward_extended\u001b[0;34m(self, input, weight, hx)\u001b[0m\n\u001b[1;32m    292\u001b[0m             \u001b[0mhy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mh\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m         \u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    295\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_for_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/pytorch-env/lib/python3.5/site-packages/torch/backends/cudnn/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(fn, input, hx, weight, output, hy)\u001b[0m\n\u001b[1;32m    293\u001b[0m                 \u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcy_desc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_void_p\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_ptr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m                 \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_void_p\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mworkspace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_ptr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworkspace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m                 \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_void_p\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreserve\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_ptr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreserve\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m             ))\n\u001b[1;32m    297\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# inference\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = run_model_with(noise_level=0, hidden=256, epochs=100, layers=1, lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-env",
   "language": "python",
   "name": "pytorch-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
