{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "from random import random, choice\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "from tqdm import tqdm as tqdm\n",
    "\n",
    "CUDA = torch.cuda.is_available()\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import torchtext\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAXLEN = 256\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "VALID_SIZE = 0.1\n",
    "\n",
    "NOISE_LEVEL = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_field = torchtext.data.Field(\n",
    "    lower=True, include_lengths=False, fix_length=2048, tensor_type=torch.FloatTensor, batch_first=True,\n",
    "    tokenize=lambda x: x, use_vocab=False, sequential=False\n",
    ")\n",
    "label_field = torchtext.data.Field(sequential=False, use_vocab=False)\n",
    "\n",
    "ALPHABET = [' ', 'e', 't', 'a', 'i', 'o', 's', 'n', 'r', 'h', 'l', 'd', 'c', 'm', 'u', 'f', 'g', 'y', 'b', 'w', 'p',\\\n",
    "            '.', 'v', ',', 'k', \"'\", '/', '>', '<', '-', '\"', 'j', 'x', ')', '(', '!', 'z', 'q', '0', '1', '?', ':',\\\n",
    "            '9', '2', '*', ';', '3', '5', '8', '4', '7', '&', '6', 'Ã©', '\\x96', '`', '$', '\\x85', '_', '%', '=', '#',\\\n",
    "            'UNK', 'PAD']\n",
    "\n",
    "ALPHABET_LEN = len(ALPHABET)\n",
    "\n",
    "char2int = {s: i for s, i in zip(ALPHABET, range(ALPHABET_LEN))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(char):\n",
    "    zeros = np.zeros(ALPHABET_LEN)\n",
    "    if char in char2int:\n",
    "        zeros[char2int[char]] = 1.\n",
    "    else:\n",
    "        zeros[char2int['UNK']] = 1.\n",
    "\n",
    "def preprocess_text_nobatch(text, maxlen=MAXLEN):\n",
    "    one_hotted_text = np.zeros((maxlen, ALPHABET_LEN))\n",
    "    for i, char in enumerate(text):\n",
    "        if i >= MAXLEN:\n",
    "            break\n",
    "        one_hotted_text[i, char2int.get(char, char2int['UNK'])] = 1.\n",
    "    if i < MAXLEN:\n",
    "        for j in range(i+1, MAXLEN):\n",
    "            one_hotted_text[j, char2int['PAD']] = 1.\n",
    "\n",
    "    return torch.FloatTensor(one_hotted_text)\n",
    "\n",
    "def onehot2text(one_hotted_text, batch_size=None):\n",
    "    if batch_size is None:\n",
    "        text = ''\n",
    "        _, idx = torch.max(one_hotted_text, 1)\n",
    "        for i in idx:\n",
    "            symb = ALPHABET[i]\n",
    "            if symb == 'PAD':\n",
    "                break\n",
    "            else:\n",
    "                text += symb\n",
    "        return text\n",
    "    else:\n",
    "        texts = []\n",
    "        for text in one_hotted_text:\n",
    "            texts.append(onehot2text(one_hotted_text, batch_size=None))\n",
    "        return texts\n",
    "\n",
    "def noise_generator(string, noise_level, chars=ALPHABET+['']):\n",
    "    noised = \"\"\n",
    "    for c in string:\n",
    "        if random() > noise_level:\n",
    "            noised += c\n",
    "        if random() < noise_level:\n",
    "            noised += choice(chars)\n",
    "    return noised\n",
    "\n",
    "class CharIMDB(torchtext.datasets.imdb.IMDB):\n",
    "    noise_level = 0\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = super(CharIMDB, self).__getitem__(idx)\n",
    "        text = item.text\n",
    "        text = noise_generator(text, self.noise_level)\n",
    "        label = int(item.label == 'pos')\n",
    "        return preprocess_text_nobatch(text), label\n",
    "\n",
    "def get_train_valid_loader(dataset, valid_size, batch_size, random_seed=42, shuffle=True, num_workers=4):\n",
    "\n",
    "    len_dataset = len(dataset)\n",
    "    indices = list(range(len_dataset))\n",
    "\n",
    "    if shuffle:\n",
    "        np.random.seed(random_seed)\n",
    "        np.random.shuffle(indices)\n",
    "\n",
    "    val_actual_size = int(len_dataset * valid_size)\n",
    "\n",
    "    train_idx, valid_idx = indices[:-val_actual_size], indices[-val_actual_size:]\n",
    "    \n",
    "    train_sampler = SubsetRandomSampler(train_idx)\n",
    "    valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        dataset, batch_size=batch_size, sampler=train_sampler, num_workers=4\n",
    "    )\n",
    "    valid_loader = torch.utils.data.DataLoader(\n",
    "        dataset, batch_size=batch_size, sampler=valid_sampler, num_workers=4\n",
    "    )\n",
    "\n",
    "    return train_loader, valid_loader\n",
    "\n",
    "def get_accuracy(model, test_dataset):\n",
    "    \"\"\"\n",
    "    Moder will be in TRAIN mode after that\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    predictions = []\n",
    "    lables = []\n",
    "\n",
    "    for text, label in test_dataset:\n",
    "        if CUDA:\n",
    "            text = Variable(text.cuda())\n",
    "        else:\n",
    "            text = Variable(text)\n",
    "\n",
    "        text = text.permute(1, 0, 2)\n",
    "        prediction = model(text)\n",
    "\n",
    "        _, idx = torch.max(prediction, 1)\n",
    "        predictions += idx.data.tolist()\n",
    "        lables += label.tolist()\n",
    "\n",
    "    acc = accuracy_score(lables, predictions)\n",
    "    model.train()\n",
    "    return acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "CharIMDB.noise_level = NOISE_LEVEL\n",
    "train, test = CharIMDB.splits(text_field, label_field)\n",
    "\n",
    "dataloader, val_dataloader = get_train_valid_loader(train, valid_size=VALID_SIZE, batch_size=BATCH_SIZE)\n",
    "\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "    test, batch_size=BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sru import SRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharRNN(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden, dropout=0.5, num_layers=2):\n",
    "        super(CharRNN, self).__init__()\n",
    "\n",
    "        self.embed = nn.Linear(ALPHABET_LEN, ALPHABET_LEN)\n",
    "        self.embed.weight = init.xavier_normal(self.embed.weight)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.rnn = SRU(ALPHABET_LEN, hidden, dropout=dropout, num_layers=num_layers)\n",
    "        self.fc = nn.Linear(hidden, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: (seq_len, batch_size, signal_dim)\n",
    "        \"\"\"\n",
    "        x = self.embed(x)\n",
    "        x = self.dropout(x)\n",
    "        x, c_states = self.rnn(x)\n",
    "        x = self.fc(x[-1])\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model_with(noise_level, hidden, lr=1e-4, dropout=0.5, layers=1, epochs=30):\n",
    "    CharIMDB.noise_level = noise_level\n",
    "\n",
    "    model = CharRNN(hidden, num_layers=layers)\n",
    "    if CUDA:\n",
    "        model.cuda()\n",
    "    model.train()\n",
    "\n",
    "    writer = SummaryWriter(comment='_charSRU_dropout%s_embed_layers%s' % (dropout, layers))\n",
    "    \n",
    "    optimizer = optim.Adam(params=model.parameters(), lr=lr)\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    global_step = 0\n",
    "\n",
    "    loss_f = F.cross_entropy\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        for batch_idx, (text, label) in enumerate(dataloader):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            if CUDA:\n",
    "                text = Variable(text.cuda())\n",
    "                label = Variable(torch.LongTensor(label).cuda())\n",
    "            else:\n",
    "                text = Variable(text)\n",
    "                label = Variable(torch.LongTensor(label))\n",
    "\n",
    "            text = text.permute(1, 0, 2)\n",
    "            prediction = model(text)\n",
    "\n",
    "            loss = loss_f(prediction, label)\n",
    "\n",
    "            writer.add_scalar('loss', loss.data[0], global_step=global_step)\n",
    "\n",
    "            loss.backward()        \n",
    "            torch.nn.utils.clip_grad_norm(model.parameters(), 1e-1)\n",
    "            optimizer.step()\n",
    "\n",
    "            if CUDA:\n",
    "                torch.cuda.synchronize()\n",
    "            global_step += 1\n",
    "\n",
    "        # evaluation\n",
    "        print('Global step: %s' % global_step)\n",
    "        print('Loss after epoch %s: %s' % (epoch, loss.data[0]))\n",
    "\n",
    "        _, idx = torch.max(prediction, 1)\n",
    "        acc = accuracy_score(label.data.tolist(), idx.data.tolist())\n",
    "        writer.add_scalar('accuracy_train', acc, global_step=global_step)\n",
    "        print('In-batch accuracy:', acc)\n",
    "\n",
    "        acc = get_accuracy(model, val_dataloader)\n",
    "        print('Validation accuracy:', acc)\n",
    "        writer.add_scalar('accuracy_val', acc, global_step=global_step)\n",
    "        print()\n",
    "\n",
    "    # Test\n",
    "\n",
    "    acc = get_accuracy(model, test_dataloader)\n",
    "    print('Final test accuracy:', acc)\n",
    "    writer.add_scalar('accuracy_test_final', acc, global_step=global_step)\n",
    "    print()\n",
    "    model.eval()\n",
    "    # model is in EVAL mode!\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SRU loaded for gpu 0\n",
      "Global step: 704\n",
      "Loss after epoch 0: 0.6822301745414734\n",
      "In-batch accuracy: 0.75\n",
      "Validation accuracy: 0.4972\n",
      "\n",
      "Global step: 1408\n",
      "Loss after epoch 1: 0.6573439240455627\n",
      "In-batch accuracy: 1.0\n",
      "Validation accuracy: 0.4988\n",
      "\n",
      "Global step: 2112\n",
      "Loss after epoch 2: 0.6799705028533936\n",
      "In-batch accuracy: 0.75\n",
      "Validation accuracy: 0.4936\n",
      "\n",
      "Global step: 2816\n",
      "Loss after epoch 3: 0.7229750156402588\n",
      "In-batch accuracy: 0.25\n",
      "Validation accuracy: 0.4856\n",
      "\n",
      "Global step: 3520\n",
      "Loss after epoch 4: 0.7095352411270142\n",
      "In-batch accuracy: 0.25\n",
      "Validation accuracy: 0.4916\n",
      "\n",
      "Global step: 4224\n",
      "Loss after epoch 5: 0.7096357345581055\n",
      "In-batch accuracy: 0.25\n",
      "Validation accuracy: 0.4948\n",
      "\n",
      "Global step: 4928\n",
      "Loss after epoch 6: 0.643151581287384\n",
      "In-batch accuracy: 1.0\n",
      "Validation accuracy: 0.494\n",
      "\n",
      "Global step: 5632\n",
      "Loss after epoch 7: 0.7027839422225952\n",
      "In-batch accuracy: 0.5\n",
      "Validation accuracy: 0.492\n",
      "\n",
      "Global step: 6336\n",
      "Loss after epoch 8: 0.6687096953392029\n",
      "In-batch accuracy: 1.0\n",
      "Validation accuracy: 0.4912\n",
      "\n",
      "Global step: 7040\n",
      "Loss after epoch 9: 0.7099725604057312\n",
      "In-batch accuracy: 0.25\n",
      "Validation accuracy: 0.4856\n",
      "\n",
      "Global step: 7744\n",
      "Loss after epoch 10: 0.719476580619812\n",
      "In-batch accuracy: 0.25\n",
      "Validation accuracy: 0.4868\n",
      "\n",
      "Global step: 8448\n",
      "Loss after epoch 11: 0.6901521682739258\n",
      "In-batch accuracy: 0.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-93:\n",
      "Process Process-96:\n",
      "Process Process-94:\n",
      "Process Process-95:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/phobos_aijun/.virtualenvs/pytorch-env/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 55, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/phobos_aijun/.virtualenvs/pytorch-env/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 55, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"<ipython-input-4-cd72a0080c8b>\", line 54, in __getitem__\n",
      "    return preprocess_text_nobatch(text), label\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/phobos_aijun/.virtualenvs/pytorch-env/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 55, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/phobos_aijun/.virtualenvs/pytorch-env/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 50, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"<ipython-input-4-cd72a0080c8b>\", line 13, in preprocess_text_nobatch\n",
      "    one_hotted_text[i, char2int.get(char, char2int['UNK'])] = 1.\n",
      "  File \"/home/phobos_aijun/.virtualenvs/pytorch-env/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 50, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/home/phobos_aijun/.virtualenvs/pytorch-env/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 135, in default_collate\n",
      "    return [default_collate(samples) for samples in transposed]\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 343, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/home/phobos_aijun/.virtualenvs/pytorch-env/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 135, in <listcomp>\n",
      "    return [default_collate(samples) for samples in transposed]\n",
      "  File \"/usr/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/home/phobos_aijun/.virtualenvs/pytorch-env/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 112, in default_collate\n",
      "    return torch.stack(batch, 0, out=out)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/phobos_aijun/.virtualenvs/pytorch-env/lib/python3.5/site-packages/torch/functional.py\", line 66, in stack\n",
      "    return torch.cat(inputs, dim, out=out)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "Exception ignored in: <bound method DataLoaderIter.__del__ of <torch.utils.data.dataloader.DataLoaderIter object at 0x7fafe7d1e828>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/phobos_aijun/.virtualenvs/pytorch-env/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 333, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/phobos_aijun/.virtualenvs/pytorch-env/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 319, in _shutdown_workers\n",
      "    self.data_queue.get()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 345, in get\n",
      "    return ForkingPickler.loads(res)\n",
      "  File \"/home/phobos_aijun/.virtualenvs/pytorch-env/lib/python3.5/site-packages/torch/multiprocessing/reductions.py\", line 70, in rebuild_storage_fd\n",
      "    fd = df.detach()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/resource_sharer.py\", line 57, in detach\n",
      "    with _resource_sharer.get_connection(self._id) as conn:\n",
      "  File \"/usr/lib/python3.5/multiprocessing/resource_sharer.py\", line 87, in get_connection\n",
      "    c = Client(address, authkey=process.current_process().authkey)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 487, in Client\n",
      "    c = SocketClient(address)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 614, in SocketClient\n",
      "    s.connect(address)\n",
      "FileNotFoundError: [Errno 2] No such file or directory\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-cdf26e98ab9f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrun_model_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoise_level\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-3405dbdf3f2f>\u001b[0m in \u001b[0;36mrun_model_with\u001b[0;34m(noise_level, hidden, lr, dropout, epochs)\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'In-batch accuracy:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Validation accuracy:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'accuracy_val'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-cd72a0080c8b>\u001b[0m in \u001b[0;36mget_accuracy\u001b[0;34m(model, test_dataset)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_dataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mCUDA\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m             \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/pytorch-env/lib/python3.5/site-packages/torch/_utils.py\u001b[0m in \u001b[0;36m_cuda\u001b[0;34m(self, device, async)\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0mnew_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnew_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0masync\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "run_model_with(noise_level=0, hidden=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global step: 704\n",
      "Loss after epoch 0: 0.6877833604812622\n",
      "In-batch accuracy: 0.5\n",
      "Validation accuracy: 0.5012\n",
      "\n",
      "Global step: 1408\n",
      "Loss after epoch 1: 0.6759294271469116\n",
      "In-batch accuracy: 0.5\n",
      "Validation accuracy: 0.5224\n",
      "\n",
      "Global step: 2112\n",
      "Loss after epoch 2: 0.6749019026756287\n",
      "In-batch accuracy: 0.5\n",
      "Validation accuracy: 0.526\n",
      "\n",
      "Global step: 2816\n",
      "Loss after epoch 3: 0.74471116065979\n",
      "In-batch accuracy: 0.0\n",
      "Validation accuracy: 0.5172\n",
      "\n",
      "Global step: 3520\n",
      "Loss after epoch 4: 0.6857150793075562\n",
      "In-batch accuracy: 0.5\n",
      "Validation accuracy: 0.5076\n",
      "\n",
      "Global step: 4224\n",
      "Loss after epoch 5: 0.6910227537155151\n",
      "In-batch accuracy: 0.75\n",
      "Validation accuracy: 0.514\n",
      "\n",
      "Global step: 4928\n",
      "Loss after epoch 6: 0.696495532989502\n",
      "In-batch accuracy: 0.75\n",
      "Validation accuracy: 0.512\n",
      "\n",
      "Global step: 5632\n",
      "Loss after epoch 7: 0.7819569110870361\n",
      "In-batch accuracy: 0.0\n",
      "Validation accuracy: 0.5048\n",
      "\n",
      "Global step: 6336\n",
      "Loss after epoch 8: 0.629566490650177\n",
      "In-batch accuracy: 0.75\n",
      "Validation accuracy: 0.5008\n",
      "\n",
      "Global step: 7040\n",
      "Loss after epoch 9: 0.660133957862854\n",
      "In-batch accuracy: 0.5\n",
      "Validation accuracy: 0.5112\n",
      "\n",
      "Global step: 7744\n",
      "Loss after epoch 10: 0.6865528225898743\n",
      "In-batch accuracy: 0.25\n",
      "Validation accuracy: 0.5156\n",
      "\n",
      "Global step: 8448\n",
      "Loss after epoch 11: 0.6693791151046753\n",
      "In-batch accuracy: 0.75\n",
      "Validation accuracy: 0.504\n",
      "\n",
      "Global step: 9152\n",
      "Loss after epoch 12: 0.6989344954490662\n",
      "In-batch accuracy: 0.25\n",
      "Validation accuracy: 0.5108\n",
      "\n",
      "Global step: 9856\n",
      "Loss after epoch 13: 0.7431229948997498\n",
      "In-batch accuracy: 0.5\n",
      "Validation accuracy: 0.5176\n",
      "\n",
      "Global step: 10560\n",
      "Loss after epoch 14: 0.7222448587417603\n",
      "In-batch accuracy: 0.25\n",
      "Validation accuracy: 0.5172\n",
      "\n",
      "Global step: 11264\n",
      "Loss after epoch 15: 0.7256454825401306\n",
      "In-batch accuracy: 0.25\n",
      "Validation accuracy: 0.5196\n",
      "\n",
      "Global step: 11968\n",
      "Loss after epoch 16: 0.655430793762207\n",
      "In-batch accuracy: 0.75\n",
      "Validation accuracy: 0.5036\n",
      "\n",
      "Global step: 12672\n",
      "Loss after epoch 17: 0.7431749105453491\n",
      "In-batch accuracy: 0.0\n",
      "Validation accuracy: 0.5176\n",
      "\n",
      "Global step: 13376\n",
      "Loss after epoch 18: 0.6873272061347961\n",
      "In-batch accuracy: 0.5\n",
      "Validation accuracy: 0.5236\n",
      "\n",
      "Global step: 14080\n",
      "Loss after epoch 19: 0.6581532955169678\n",
      "In-batch accuracy: 0.75\n",
      "Validation accuracy: 0.5216\n",
      "\n",
      "Global step: 14784\n",
      "Loss after epoch 20: 0.7225463390350342\n",
      "In-batch accuracy: 0.25\n",
      "Validation accuracy: 0.5324\n",
      "\n",
      "Global step: 15488\n",
      "Loss after epoch 21: 0.644294023513794\n",
      "In-batch accuracy: 0.75\n",
      "Validation accuracy: 0.5348\n",
      "\n",
      "Global step: 16192\n",
      "Loss after epoch 22: 0.6488078832626343\n",
      "In-batch accuracy: 0.75\n",
      "Validation accuracy: 0.5288\n",
      "\n",
      "Global step: 16896\n",
      "Loss after epoch 23: 0.6605956554412842\n",
      "In-batch accuracy: 0.75\n",
      "Validation accuracy: 0.5628\n",
      "\n",
      "Global step: 17600\n",
      "Loss after epoch 24: 0.7789527177810669\n",
      "In-batch accuracy: 0.5\n",
      "Validation accuracy: 0.572\n",
      "\n",
      "Global step: 18304\n",
      "Loss after epoch 25: 0.7113422155380249\n",
      "In-batch accuracy: 0.5\n",
      "Validation accuracy: 0.5724\n",
      "\n",
      "Global step: 19008\n",
      "Loss after epoch 26: 0.6265515685081482\n",
      "In-batch accuracy: 0.75\n",
      "Validation accuracy: 0.5788\n",
      "\n",
      "Global step: 19712\n",
      "Loss after epoch 27: 0.7378420829772949\n",
      "In-batch accuracy: 0.25\n",
      "Validation accuracy: 0.5816\n",
      "\n",
      "Global step: 20416\n",
      "Loss after epoch 28: 0.5696713328361511\n",
      "In-batch accuracy: 0.75\n",
      "Validation accuracy: 0.5772\n",
      "\n",
      "Global step: 21120\n",
      "Loss after epoch 29: 0.7239721417427063\n",
      "In-batch accuracy: 0.25\n",
      "Validation accuracy: 0.56\n",
      "\n",
      "Global step: 21824\n",
      "Loss after epoch 30: 0.6748836636543274\n",
      "In-batch accuracy: 0.75\n",
      "Validation accuracy: 0.5824\n",
      "\n",
      "Global step: 22528\n",
      "Loss after epoch 31: 0.5926603078842163\n",
      "In-batch accuracy: 0.75\n",
      "Validation accuracy: 0.582\n",
      "\n",
      "Global step: 23232\n",
      "Loss after epoch 32: 0.8007228374481201\n",
      "In-batch accuracy: 0.25\n",
      "Validation accuracy: 0.584\n",
      "\n",
      "Global step: 23936\n",
      "Loss after epoch 33: 0.5300740003585815\n",
      "In-batch accuracy: 1.0\n",
      "Validation accuracy: 0.5636\n",
      "\n",
      "Global step: 24640\n",
      "Loss after epoch 34: 0.7083131670951843\n",
      "In-batch accuracy: 0.5\n",
      "Validation accuracy: 0.59\n",
      "\n",
      "Global step: 25344\n",
      "Loss after epoch 35: 0.6326422691345215\n",
      "In-batch accuracy: 0.75\n",
      "Validation accuracy: 0.5804\n",
      "\n",
      "Global step: 26048\n",
      "Loss after epoch 36: 0.7212014198303223\n",
      "In-batch accuracy: 0.5\n",
      "Validation accuracy: 0.56\n",
      "\n",
      "Global step: 26752\n",
      "Loss after epoch 37: 0.5954393148422241\n",
      "In-batch accuracy: 0.5\n",
      "Validation accuracy: 0.5912\n",
      "\n",
      "Global step: 27456\n",
      "Loss after epoch 38: 0.7703224420547485\n",
      "In-batch accuracy: 0.25\n",
      "Validation accuracy: 0.5856\n",
      "\n",
      "Global step: 28160\n",
      "Loss after epoch 39: 0.7133157253265381\n",
      "In-batch accuracy: 0.5\n",
      "Validation accuracy: 0.5888\n",
      "\n",
      "Global step: 28864\n",
      "Loss after epoch 40: 0.5758777856826782\n",
      "In-batch accuracy: 0.75\n",
      "Validation accuracy: 0.5748\n",
      "\n",
      "Global step: 29568\n",
      "Loss after epoch 41: 0.7943898439407349\n",
      "In-batch accuracy: 0.25\n",
      "Validation accuracy: 0.5832\n",
      "\n",
      "Global step: 30272\n",
      "Loss after epoch 42: 0.7322217226028442\n",
      "In-batch accuracy: 0.5\n",
      "Validation accuracy: 0.5948\n",
      "\n",
      "Global step: 30976\n",
      "Loss after epoch 43: 0.699298620223999\n",
      "In-batch accuracy: 0.25\n",
      "Validation accuracy: 0.5872\n",
      "\n",
      "Global step: 31680\n",
      "Loss after epoch 44: 0.747400164604187\n",
      "In-batch accuracy: 0.5\n",
      "Validation accuracy: 0.5848\n",
      "\n",
      "Global step: 32384\n",
      "Loss after epoch 45: 0.6879792213439941\n",
      "In-batch accuracy: 0.5\n",
      "Validation accuracy: 0.5796\n",
      "\n",
      "Global step: 33088\n",
      "Loss after epoch 46: 0.7442531585693359\n",
      "In-batch accuracy: 0.75\n",
      "Validation accuracy: 0.5872\n",
      "\n",
      "Global step: 33792\n",
      "Loss after epoch 47: 0.46316587924957275\n",
      "In-batch accuracy: 1.0\n",
      "Validation accuracy: 0.5484\n",
      "\n",
      "Global step: 34496\n",
      "Loss after epoch 48: 0.7851572036743164\n",
      "In-batch accuracy: 0.25\n",
      "Validation accuracy: 0.5756\n",
      "\n",
      "Global step: 35200\n",
      "Loss after epoch 49: 0.4662242829799652\n",
      "In-batch accuracy: 1.0\n",
      "Validation accuracy: 0.5864\n",
      "\n",
      "Global step: 35904\n",
      "Loss after epoch 50: 0.7712576389312744\n",
      "In-batch accuracy: 0.25\n",
      "Validation accuracy: 0.576\n",
      "\n",
      "Global step: 36608\n",
      "Loss after epoch 51: 0.5226591229438782\n",
      "In-batch accuracy: 0.5\n",
      "Validation accuracy: 0.5812\n",
      "\n",
      "Global step: 37312\n",
      "Loss after epoch 52: 0.6306784152984619\n",
      "In-batch accuracy: 0.5\n",
      "Validation accuracy: 0.5912\n",
      "\n",
      "Global step: 38016\n",
      "Loss after epoch 53: 0.6289879679679871\n",
      "In-batch accuracy: 0.5\n",
      "Validation accuracy: 0.5964\n",
      "\n",
      "Global step: 38720\n",
      "Loss after epoch 54: 0.6365715265274048\n",
      "In-batch accuracy: 0.5\n",
      "Validation accuracy: 0.584\n",
      "\n",
      "Global step: 39424\n",
      "Loss after epoch 55: 0.714073121547699\n",
      "In-batch accuracy: 0.75\n",
      "Validation accuracy: 0.5712\n",
      "\n",
      "Global step: 40128\n",
      "Loss after epoch 56: 0.8227112889289856\n",
      "In-batch accuracy: 0.5\n",
      "Validation accuracy: 0.5848\n",
      "\n",
      "Global step: 40832\n",
      "Loss after epoch 57: 0.8048059940338135\n",
      "In-batch accuracy: 0.5\n",
      "Validation accuracy: 0.5932\n",
      "\n",
      "Global step: 41536\n",
      "Loss after epoch 58: 0.696605920791626\n",
      "In-batch accuracy: 0.5\n",
      "Validation accuracy: 0.5828\n",
      "\n",
      "Global step: 42240\n",
      "Loss after epoch 59: 0.47740623354911804\n",
      "In-batch accuracy: 1.0\n",
      "Validation accuracy: 0.5892\n",
      "\n",
      "Global step: 42944\n",
      "Loss after epoch 60: 0.7132588624954224\n",
      "In-batch accuracy: 0.5\n",
      "Validation accuracy: 0.5952\n",
      "\n",
      "Global step: 43648\n",
      "Loss after epoch 61: 0.6535913944244385\n",
      "In-batch accuracy: 0.75\n",
      "Validation accuracy: 0.5872\n",
      "\n",
      "Global step: 44352\n",
      "Loss after epoch 62: 0.6725362539291382\n",
      "In-batch accuracy: 0.75\n",
      "Validation accuracy: 0.5756\n",
      "\n",
      "Global step: 45056\n",
      "Loss after epoch 63: 0.5778068900108337\n",
      "In-batch accuracy: 1.0\n",
      "Validation accuracy: 0.584\n",
      "\n",
      "Global step: 45760\n",
      "Loss after epoch 64: 0.6747623682022095\n",
      "In-batch accuracy: 0.5\n",
      "Validation accuracy: 0.58\n",
      "\n",
      "Global step: 46464\n",
      "Loss after epoch 65: 0.7159287929534912\n",
      "In-batch accuracy: 0.5\n",
      "Validation accuracy: 0.5636\n",
      "\n",
      "Global step: 47168\n",
      "Loss after epoch 66: 0.7432101964950562\n",
      "In-batch accuracy: 0.75\n",
      "Validation accuracy: 0.5908\n",
      "\n",
      "Global step: 47872\n",
      "Loss after epoch 67: 0.5742340087890625\n",
      "In-batch accuracy: 0.75\n",
      "Validation accuracy: 0.586\n",
      "\n",
      "Global step: 48576\n",
      "Loss after epoch 68: 0.6224431991577148\n",
      "In-batch accuracy: 0.5\n",
      "Validation accuracy: 0.5916\n",
      "\n",
      "Global step: 49280\n",
      "Loss after epoch 69: 0.5950433611869812\n",
      "In-batch accuracy: 0.75\n",
      "Validation accuracy: 0.5728\n",
      "\n",
      "Global step: 49984\n",
      "Loss after epoch 70: 0.585405707359314\n",
      "In-batch accuracy: 0.5\n",
      "Validation accuracy: 0.5872\n",
      "\n",
      "Global step: 50688\n",
      "Loss after epoch 71: 0.7169253826141357\n",
      "In-batch accuracy: 0.75\n",
      "Validation accuracy: 0.5828\n",
      "\n",
      "Global step: 51392\n",
      "Loss after epoch 72: 0.5969681739807129\n",
      "In-batch accuracy: 0.5\n",
      "Validation accuracy: 0.5896\n",
      "\n",
      "Global step: 52096\n",
      "Loss after epoch 73: 0.5244110822677612\n",
      "In-batch accuracy: 1.0\n",
      "Validation accuracy: 0.5952\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global step: 52800\n",
      "Loss after epoch 74: 0.5699837803840637\n",
      "In-batch accuracy: 0.75\n",
      "Validation accuracy: 0.5792\n",
      "\n",
      "Global step: 53504\n",
      "Loss after epoch 75: 0.5464096665382385\n",
      "In-batch accuracy: 0.75\n",
      "Validation accuracy: 0.5852\n",
      "\n",
      "Global step: 54208\n",
      "Loss after epoch 76: 0.5671347379684448\n",
      "In-batch accuracy: 0.75\n",
      "Validation accuracy: 0.6012\n",
      "\n",
      "Global step: 54912\n",
      "Loss after epoch 77: 0.6364257335662842\n",
      "In-batch accuracy: 0.5\n",
      "Validation accuracy: 0.5824\n",
      "\n",
      "Global step: 55616\n",
      "Loss after epoch 78: 0.4664842486381531\n",
      "In-batch accuracy: 1.0\n",
      "Validation accuracy: 0.5872\n",
      "\n",
      "Global step: 56320\n",
      "Loss after epoch 79: 0.7226962447166443\n",
      "In-batch accuracy: 0.5\n",
      "Validation accuracy: 0.5896\n",
      "\n",
      "Global step: 57024\n",
      "Loss after epoch 80: 0.5735809803009033\n",
      "In-batch accuracy: 0.75\n",
      "Validation accuracy: 0.5892\n",
      "\n",
      "Global step: 57728\n",
      "Loss after epoch 81: 0.8028784990310669\n",
      "In-batch accuracy: 0.25\n",
      "Validation accuracy: 0.6004\n",
      "\n",
      "Global step: 58432\n",
      "Loss after epoch 82: 0.9222941994667053\n",
      "In-batch accuracy: 0.25\n",
      "Validation accuracy: 0.5812\n",
      "\n",
      "Global step: 59136\n",
      "Loss after epoch 83: 0.47657111287117004\n",
      "In-batch accuracy: 1.0\n",
      "Validation accuracy: 0.5928\n",
      "\n",
      "Global step: 59840\n",
      "Loss after epoch 84: 0.6367892622947693\n",
      "In-batch accuracy: 0.75\n",
      "Validation accuracy: 0.59\n",
      "\n",
      "Global step: 60544\n",
      "Loss after epoch 85: 0.6898384690284729\n",
      "In-batch accuracy: 0.5\n",
      "Validation accuracy: 0.58\n",
      "\n",
      "Global step: 61248\n",
      "Loss after epoch 86: 0.7011841535568237\n",
      "In-batch accuracy: 0.75\n",
      "Validation accuracy: 0.58\n",
      "\n",
      "Global step: 61952\n",
      "Loss after epoch 87: 0.6542638540267944\n",
      "In-batch accuracy: 0.5\n",
      "Validation accuracy: 0.5884\n",
      "\n",
      "Global step: 62656\n",
      "Loss after epoch 88: 0.6759254336357117\n",
      "In-batch accuracy: 0.5\n",
      "Validation accuracy: 0.5596\n",
      "\n",
      "Global step: 63360\n",
      "Loss after epoch 89: 0.6154249310493469\n",
      "In-batch accuracy: 0.5\n",
      "Validation accuracy: 0.5764\n",
      "\n",
      "Global step: 64064\n",
      "Loss after epoch 90: 0.9357709884643555\n",
      "In-batch accuracy: 0.25\n",
      "Validation accuracy: 0.5828\n",
      "\n",
      "Global step: 64768\n",
      "Loss after epoch 91: 0.525493323802948\n",
      "In-batch accuracy: 0.75\n",
      "Validation accuracy: 0.5844\n",
      "\n",
      "Global step: 65472\n",
      "Loss after epoch 92: 0.8595097064971924\n",
      "In-batch accuracy: 0.0\n",
      "Validation accuracy: 0.5864\n",
      "\n",
      "Global step: 66176\n",
      "Loss after epoch 93: 0.7183809280395508\n",
      "In-batch accuracy: 0.75\n",
      "Validation accuracy: 0.5912\n",
      "\n",
      "Global step: 66880\n",
      "Loss after epoch 94: 0.6057461500167847\n",
      "In-batch accuracy: 0.75\n",
      "Validation accuracy: 0.5928\n",
      "\n",
      "Global step: 67584\n",
      "Loss after epoch 95: 0.6345586776733398\n",
      "In-batch accuracy: 0.5\n",
      "Validation accuracy: 0.5932\n",
      "\n",
      "Global step: 68288\n",
      "Loss after epoch 96: 0.9481221437454224\n",
      "In-batch accuracy: 0.0\n",
      "Validation accuracy: 0.5828\n",
      "\n",
      "Global step: 68992\n",
      "Loss after epoch 97: 0.6558783054351807\n",
      "In-batch accuracy: 0.75\n",
      "Validation accuracy: 0.592\n",
      "\n",
      "Global step: 69696\n",
      "Loss after epoch 98: 0.6332458257675171\n",
      "In-batch accuracy: 0.75\n",
      "Validation accuracy: 0.596\n",
      "\n",
      "Global step: 70400\n",
      "Loss after epoch 99: 0.7045444846153259\n",
      "In-batch accuracy: 0.75\n",
      "Validation accuracy: 0.5944\n",
      "\n",
      "Final test accuracy: 0.5754\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = run_model_with(noise_level=0, hidden=256, epochs=100, layers=1, lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-env",
   "language": "python",
   "name": "pytorch-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
